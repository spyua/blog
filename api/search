[{"id":"content:0.code:1.Refactoring00_Quick Review Notes.md","path":"/code/refactoring00_quick-review-notes","dir":"code","title":"Refactoring 00 : Quick Review Notes","description":"Reorganizing study notes for better code understanding","keywords":["Bad Code Smell Item List"],"body":"  Refactoring 00 : Quick Review Notes  Reorganizing study notes for better code understanding  Bad Code Smell Item List    Duplicated Code  : Identical or similar code appears in multiple places, increasing the burden of maintenance and updates.  Scenario: Two methods in different classes perform the same calculation.       // Class A\n   public     class     Order   {\n         public     double     calculateTotalPrice  (List<  Item  >   items  ) {\n             double   total   =     0  ;\n             for   (Item item   :   items) {\n               total   +=   item.  getPrice  ()   *   item.  getQuantity  ();\n           }\n             return   total;\n       }\n   }\n     // Class B\n   public     class     Invoice   {\n         public     double     calculateTotalPrice  (List<  Item  >   items  ) {\n             double   total   =     0  ;\n             for   (Item item   :   items) {\n               total   +=   item.  getPrice  ()   *   item.  getQuantity  ();\n           }\n             return   total;\n       }\n   }    Long Method  : A method (function) is overly long, making it difficult to understand and maintain; it should be broken down into smaller functional units.  Scenario: A method handles multiple responsibilities and is too lengthy.     public     void     processOrder  (Order order) {\n         // Validate order\n         // Calculate totals\n         // Apply discounts\n         // Update inventory\n         // Send notifications\n         // Generate invoice\n         // ... (method continues)\n   }    Large Class  : A class contains too many responsibilities and functionalities, violating the Single Responsibility Principle; it should be split into smaller, more focused classes.  Scenario: A class manages orders, customers, inventory, and reporting.     public     class     StoreManager   {\n         // Order management methods\n         // Customer management methods\n         // Inventory management methods\n         // Reporting methods\n         // ... (class continues)\n   }    Long Parameter List  : A single module needs frequent modifications for different reasons, leading to maintenance difficulties and violating the Single Responsibility Principle.  Scenario: A method requires numerous parameters to perform its task.     public     void     createUser  (String firstName, String lastName, String email, String phone,\n                          String address, String city, String state, String zipCode) {\n         // Method implementation\n   }    Divergent Change  : A single module needs frequent modifications for different reasons, leading to maintenance difficulties and violating the Single Responsibility Principle.  Scenario: A single class changes for multiple, unrelated reasons.     public     class     Employee   {\n         // Fields and methods related to employee data\n           // Methods related to payroll calculations\n         // Methods related to employee scheduling\n         // Methods related to performance reviews\n   }    Shotgun Surgery  : A small change requires modifications in multiple classes or modules, increasing the risk of errors and maintenance costs.  Scenario: Changing a database field name requires updates in many classes.     // Changing \"userName\" to \"username\" affects multiple classes\n   public     class     UserDAO   {\n         public   User   findByUserName  (String   userName  ) {   /* ... */   }\n   }\n     public     class     AuthenticationService   {\n         public     boolean     authenticate  (String   userName  , String   password  ) {   /* ... */   }\n   }\n     public     class     AuditService   {\n         public     void     logLogin  (String   userName  ) {   /* ... */   }\n   }    Feature Envy  : A method excessively uses data or functions from other classes; it should be moved to the related class.  Scenario: A method accesses the data of another class excessively.     public     class     SalaryCalculator   {\n         public     double     calculateSalary  (Employee   employee  ) {\n             double   base   =   employee.  getBaseSalary  ();\n             double   bonus   =   employee.  getPerformanceBonus  ();\n             double   tax   =   employee.  getTaxRate  ();\n             return   (base   +   bonus)   *   (  1     -   tax);\n       }\n   }    Data Clumps  : A group of related data frequently appears together; it should be encapsulated into a new class or structure.  Scenario: Multiple methods use the same group of parameters.     public     void     printAddress  (String street, String city, String state, String zip) {   /* ... */   }\n     public     void     validateAddress  (String street, String city, String state, String zip) {   /* ... */   }\n     public     void     saveAddress  (String street, String city, String state, String zip) {   /* ... */   }    Primitive Obsession  : Overusing primitive types to represent complex concepts; custom classes should be created to encapsulate these concepts.  Scenario: Using primitive types instead of small objects for simple tasks.     public     class     Rectangle   {\n         private     double   width;\n         private     double   height;\n           public     double     calculateArea  () {\n             return   width   *   height;\n       }\n   }\n     // Instead of using a Dimension class like as below\n   public     class     Dimension   {\n         private     double   width;\n         private     double   height;\n           public     Dimension  (  double     width  ,   double     height  ) {\n             this  .width   =   width;\n             this  .height   =   height;\n       }\n           // Getters, setters, and other relevant methods\n   }\n     public     class     Rectangle   {\n         private   Dimension dimension;\n           public     Rectangle  (Dimension   dimension  ) {\n             this  .dimension   =   dimension;\n       }\n           public     double     calculateArea  () {\n             return   dimension.  getWidth  ()   *   dimension.  getHeight  ();\n       }\n   }    Switch Statements  : Overreliance on switch or if-else structures; consider using polymorphism or design patterns as alternatives.  Scenario: Using a switch to determine behavior based on type.     public     double     calculateDiscount  (Customer customer) {\n         switch   (customer.  getType  ()) {\n             case     \"Regular\"  :\n                 return     0.05  ;\n             case     \"Premium\"  :\n                 return     0.10  ;\n             case     \"VIP\"  :\n                 return     0.20  ;\n             default:\n                 return     0  ;\n       }\n   }    Parallel Inheritance Hierarchies  : Adding a subclass in one hierarchy necessitates adding a corresponding subclass in another hierarchy.  Scenario: Adding a new shape requires adding new classes in different hierarchies.     // Shape hierarchy\n   public     abstract     class     Shape   {   /* ... */   }\n   public     class     Circle     extends     Shape   {   /* ... */   }\n   public     class     Square     extends     Shape   {   /* ... */   }\n     // Renderer hierarchy\n   public     abstract     class     ShapeRenderer   {   /* ... */   }\n   public     class     CircleRenderer     extends     ShapeRenderer   {   /* ... */   }\n   public     class     SquareRenderer     extends     ShapeRenderer   {   /* ... */   }    Lazy Class  : A class does too little to justify its existence; it should be merged with another class or removed.  Scenario: A class that doesn't do enough to justify its existence.     public     class     HelperUtils   {\n        public     static   String   toUpperCase  (String   input  ) {\n            return   input.  toUpperCase  ();\n      }\n   }    Speculative Generality  : Adding complexity for possible future needs, making the code harder to understand and maintain.  Scenario: Adding hooks and abstractions for future use cases that aren't needed yet.     public     interface     DataProcessor   {\n        void     processData  (Map<  String  ,   Object  >   data  );\n   }\n     public     class     JsonDataProcessor     implements     DataProcessor   {\n      @  Override\n        public     void     processData  (Map<  String  ,   Object  >   data  ) {\n            // Process JSON data\n      }\n   }\n     // Only JSON processing is needed, but the abstraction is added prematurely.    Temporary Field  : Certain fields in an object have values only under specific conditions and are empty otherwise, increasing the difficulty of understanding the code.  Scenario: An object has fields that are only set under certain conditions.     public     class     Order   {\n        private   Discount couponDiscount;   // Only used if a coupon is applied\n        private   Discount seasonalDiscount;   // Only used during seasonal sales\n          public     double     calculateTotal  () {\n            // Implementation\n      }\n   }    Message Chains  : Chaining multiple method calls across objects, like a.b().c().d(), leading to high coupling and fragility.  Scenario: Code that navigates through multiple objects to get data.     public     class     OrderService   {\n        public   String   getCustomerCity  (Order   order  ) {\n            return   order.  getCustomer  ().  getAddress  ().  getCity  ();\n      }\n   }    Middle Man  : A class where most methods simply delegate to methods in other classes, lacking substantial functionality.  Scenario: A class delegates most of its work to another class.     public     class     CustomerManager   {\n        private   CustomerDAO customerDAO   =     new     CustomerDAO  ();\n          public     void     addCustomer  (Customer   customer  ) {\n          customerDAO.  addCustomer  (customer);\n      }\n          public   Customer   getCustomer  (  int     id  ) {\n            return   customerDAO.  getCustomer  (id);\n      }\n          // Most methods just delegate to CustomerDAO\n   }    Inappropriate Intimacy  : Classes overly depend on each other's internal implementations, violating the principle of encapsulation.  Scenario: Two classes rely too heavily on each other's internal details.     public     class     ClassA   {\n        private     int   secretValue;\n          public     int     getSecretValue  () {\n            return   secretValue;\n      }\n   }\n     public     class     ClassB   {\n        public     void     doSomething  (ClassA   a  ) {\n            int   value   =   a.  getSecretValue  ();   // Accessing internal details\n            // Manipulate value\n      }\n   }    Alternative Classes with Different Interfaces  : Classes perform similar functions but have different interfaces; interfaces should be unified or classes should be combined.  Scenario: Classes with similar functionality but different method signatures.     public     class     XmlParser   {\n        public   Document   parseXml  (String   xml  ) {   /* ... */   }\n   }\n     public     class     JsonParser   {\n        public   JSONObject   parse  (String   json  ) {   /* ... */   }\n   }\n     // Interfaces are different despite similar purposes    Incomplete Library Class  : The library class in use doesn't fully meet the needs; it requires extension or wrapping.  Scenario: A library class doesn't provide necessary functionality.     // Suppose the library's List doesn't have a sort method\n   List<  Integer  > numbers   =     new   ArrayList<>();\n   // Need to write a custom sort\n   Collections.  sort  (numbers);   // Library method is insufficient    Data Class  : A class contains only fields without behavior; it may need methods added or a redesign.  Scenario: A class with only fields and no methods (behavior).     public     class     Point   {\n        public     double   x;\n        public     double   y;\n   }\n     // No methods to manipulate or utilize the data    Refused Bequest  : A subclass inherits unwanted functionality from its parent class, violating the Liskov Substitution Principle.  Scenario: A subclass doesn't use inherited methods or overrides them improperly.     public     class     Animal   {\n        public     void     eat  () {   /* ... */   }\n        public     void     sleep  () {   /* ... */   }\n   }\n     public     class     RobotDog     extends     Animal   {\n      @  Override\n        public     void     eat  () {\n            // Doesn't need to eat; method is irrelevant\n      }\n   }    Comments  : Excessive or unnecessary comments may indicate that the code is hard to understand; the code itself should be improved.  Scenario: Excessive comments that may indicate confusing code.     public     int     calculate  (  int   a,   int   b) {\n        // Check if a is greater than b\n        if   (a   >   b) {\n            // Subtract b from a\n            return   a   -   b;\n      }   else   {\n            // Add a and b\n            return   a   +   b;\n      }\n   }  Bad Smell Problem Classification  Issues caused by code smells:   Understandability : Refers to issues that make the code harder to read and understand.  Modifiability :  Makes it more difficult to change or maintain the code.  Testability : Hinders testing or lowers test coverage.  Performance : Can lead to slower system performance.  Reusability : Limits the ability to reuse code across different projects or situations.  Scalability :  Affects the system’s ability to grow and handle increased demand.  Maintainability : Increases the long-term effort needed to maintain the code.  Complexity : Adds unnecessary complexity to the codebase.  Design Quality : Violates fundamental design principles.  Code Style : Deviates from coding standards or best practices.  This article aims to provide a quick overview of the key points in refactoring. By summarizing common code smells and presenting practical code examples, it helps readers rapidly understand what typical code smells are and the issues they can cause. By recognizing these smells, developers can more easily identify areas that need improvement in their daily programming, take appropriate refactoring actions, and enhance the quality and maintainability of their code.  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-086898{color:#6A737D;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-157101{color:#E36209;}.dark .ct-157101{color:#FFAB70;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}"},{"id":"content:0.code:1.Refactoring01.md","path":"/code/refactoring01","dir":"code","title":"Refactoring 01 : Detail","description":"","keywords":["Lazy Class"],"body":"  Refactoring 01 : Detail  Lazy Class  If a class has very few functionalities and hardly implements any methods or responsibilities, it might be considered a Lazy Class. For example:         class     Address   {\n         private   String street;\n         private   String city;\n           // getter 和 setter\n   }  The Address class might only be used within the Customer class, serving a simple and uncomplicated purpose. In this case, you might consider merging Address into Customer:     class     Customer   {\n       private   String name;\n       private   String street;\n       private   String city;\n         // getter 和 setter\n   }  Generally, you can handle this situation using several techniques:  Inline Class  Integrate simple classes into the classes that use them, reducing unnecessary layers of abstraction and making the code more concise.     // Before\n   class     Address   {\n         private   String fullAddress;\n         public   String   getFullAddress  () {   return   fullAddress; }\n   }\n     class     Customer   {\n         private   Address address;\n         public   String   getAddress  () {   return   address.  getFullAddress  (); }\n   }\n     // After\n   class     Customer   {\n         private   String address;\n         public   String   getAddress  () {   return   address; }\n   }  Move Method  If a class has only one method, consider turning it into a method of another related class. This reduces the number of classes and centralizes related logic.     // Before\n   class     TaxCalculator   {\n         public     double     calculateTax  (  double     amount  ) {   return   amount   *     0.05  ; }\n   }\n     // After\n   class     OrderProcessor   {\n         private     double     calculateTax  (  double     amount  ) {   return   amount   *     0.05  ; }\n   }  Extract Interface  When a class's functionality is simple but important, you can convert it into an interface. This increases flexibility and allows for different implementations.     // Before\n   class     SimpleLogger   {\n         public     void     log  (String   message  ) { System.out.  println  (message); }\n   }\n     // After\n   interface     Logger   {\n         void     log  (String   message  );\n   }\n     class     ConsoleLogger     implements     Logger   {\n         public     void     log  (String   message  ) { System.out.  println  (message); }\n   }  Merge Classes  If two classes have similar or related functionalities, consider merging them. This consolidates related logic and reduces duplicate code.     // Before\n   class     Circle   {\n         private     double   radius;\n         public     double     area  () {   return   Math.PI   *   radius   *   radius; }\n   }\n     class     Circumference   {\n         private     double   radius;\n         public     double     length  () {   return     2     *   Math.PI   *   radius; }\n   }\n     // After\n   class     Circle   {\n         private     double   radius;\n         public     double     area  () {   return   Math.PI   *   radius   *   radius; }\n         public     double     circumference  () {   return     2     *   Math.PI   *   radius; }\n   }  Convert to Attribute  This is a composite refactoring technique that often involves the following specific refactoring methods:   Inline Class: Merge the functionality of the EmployeeId class into the Employee class.  Remove Middle Man: Eliminate unnecessary delegation methods.  Encapsulate Field: Ensure that the id field still has proper encapsulation.     // Before\n   class     EmployeeId   {\n         private   String value;\n       \n         public   String   getValue  () {\n             return   value;\n       }\n       \n         public     void     setValue  (String   value  ) {\n             this  .value   =   value;\n       }\n   }\n     class     Employee   {\n         private   EmployeeId id;\n       \n         public   String   getId  () {\n             return   id.  getValue  ();\n       }\n       \n         public     void     setId  (String   id  ) {\n             this  .id.  setValue  (id);\n       }\n   }\n     // After\n   class     Employee   {\n         private   String id;\n       \n         public   String   getId  () {\n             return   id;\n       }\n       \n         public     void     setId  (String   id  ) {\n             this  .id   =   id;\n       }\n   }  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-086898{color:#6A737D;}.ct-157101{color:#E36209;}.dark .ct-157101{color:#FFAB70;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}"},{"id":"content:0.code:2.Dependency Injection.md","path":"/code/dependency-injection","dir":"code","title":"Dependency Injection","description":"Dependency Injection (DI) is an essential design pattern in many software architectures today. This chapter will explain the concept of DI and introduce its usage in .NET.","keywords":["A. Dependency Injection","B. Container Concept","C. Ioc(Inversion of Control)","D. C# Dependency Injection Framework [Example]"],"body":"  Dependency Injection (DI) is an essential design pattern in many software architectures today. This chapter will explain the concept of DI and introduce its usage in .NET.  A. Dependency Injection  Let's start by discussing the concepts of dependency and injection.  a. Dependency  As the term suggests, 'dependency' means that objects in software often rely on each other. For example, imagine a calculator that needs to print the results. This calculator 'depends on' a printer. So, when you create a calculator, you also need to create a printer and give it to the calculator       public     class     Calculator\n   {\n         public     Printer     Printer   {  get  ;  private     set  }\n         public     Add  (  int     a  ,   int     b  ){\n             var     sum     =   a  +  b;\n           Printer.  ConsoleOut  (sum);\n       }\n         public     Calculator  (  Printer     printer  ){\n           Printer   =   printer;\n       }\n   }\n   public     class     Printer\n   {\n         public     void     ConsoleOut  (  string     txt  ) \n       {\n           Console.  WriteLine  (txt);\n       }\n   }\n   static     class     Program\n   {\n         static     void     Main  ()\n       {\n             var     printer     =     new     Printer  ();\n             var     calculator     =     new     Calculator  (printer);\n           calculator.  Add  (  1  +  1  ); \n       }\n   }  Because the Calculator relies on the Printer in this example, the control is not with the Printer. In other words, modifications to the Printer can have implications for the Calculator.  I found a fitting analogy online to illustrate object dependency. Imagine a teenager (let's call them '8+9') addicted to drugs. The '8+9' might believe they're in control and using drugs for their own pleasure. However, in reality, the '8+9's behavior is now controlled by the drug, making it impossible to break free.    b. Dependency With Interface  How can we reduce dependency? A common approach is to make objects dependent on interfaces. By doing this, objects become decoupled, as illustrated in the diagram.       public     class     Calculator\n   {\n         public     IPrinter     Printer   {  get  ;  private     set  }\n         public     Add  (  int     a  ,   int     b  ){\n             var     sum     =   a  +  b;\n           Printer.  ConsoleOut  (sum);\n       }\n         public     Calculator  (  IPrinter     printer  ){\n           Printer   =   printer;\n       }\n   }\n   public     class     Printer  :  IPrinter\n   {\n         public     void     ConsoleOut  (  string     txt  ) \n       {\n           Console.  WriteLine  (txt);\n       }\n   }\n     public     interface     IPrinter  {\n         void     ConsoleOut  (  string     txt  );\n   }\n     static     class     Program\n   {\n         static     void     Main  ()\n       {\n             IPrinter     printer     =     new     Printer  ();\n             var     calculator     =     new     Calculator  (printer);\n           calculator.  Add  (  1  +  1  ); \n       }\n   }  The Dependency Inversion Principle (DIP) enables us to achieve loose coupling between modules. In this example, by depending on an interface rather than a concrete class, we can easily swap out the printer implementation for a text file output without affecting the higher-level component. This promotes code reusability and maintainability.   The high-level module in this scenario is the Calculator, which is the caller, while the low-level module is the Printer, which is the callee.  Let's go back to the example of the \"8+9\". If we want to help them turn their life around and become a good young person, we can start by replacing their direct dependency on drugs with a dependency on medication, which acts as an interface.    Let's say, hypothetically, we successfully replace the \"8+9\"'s drug dependency with a dependence on medication. Then, we'll create a \"health food\" category and secretly swap out the medication with actual health food. The \"8+9\" will think they're still taking their drugs, but in reality, they'll be consuming daily doses of nutritious food, leading to improved cognitive function, better posture, and even perfect test scores, transforming them into pillars of society.  c. Injection  Let's talk about injection. In the Calculator example, we used constructor injection. Actually, injection is something we use all the time, but we might not notice it. There are three common injection patterns:   Constructor Injection  Property Injection  Method Injection  Constructor Injection     // Car interface\n   public     interface     ICar   {\n         // Method to start the car\n         void     run  ();\n   }\n     // Driver interface\n   public     interface     IDriver   {\n         // Method for the driver to drive a car\n         void     drive  ();\n   }\n     // Concrete implementation of the Driver interface\n   public     class     Driver     implements     IDriver   {\n         private   ICar car;\n           // Constructor injection: Injects the car object during object creation\n         public     Driver  (ICar   car  ) {\n             this  .car   =   car;\n       }\n         @  Override\n         public     void     drive  () {\n           car.  run  ();\n       }\n   }  Property Injection     // Car interface\n   public     interface     ICar   {\n         // Method to start the car\n         void     run  ();\n   }\n     // Driver interface\n   public     interface     IDriver   {\n         // Method for the driver to drive a car\n         void     drive  ();\n   }\n     // Driver class with property injection\n   public     class     Driver     implements     IDriver   {\n         private   ICar car;\n           // Property injection: Injects the car object using a setter method\n         public     void     setCar  (ICar   car  ) {\n             this  .car   =   car;\n       }\n         @  Override\n         public     void     drive  () {\n           car.  run  ();\n       }\n   }  Property Injection     // Car interface\n   public     interface     ICar   {\n         // Method to start the car\n         void     run  ();\n   }\n     // Driver interface\n   public     interface     IDriver   {\n         // Method for the driver to drive a car\n         void     drive  ();\n   }\n       // Driver class with method injection\n   public     interface     IDriver   {\n         // Method for the driver to drive a car, taking the car object as a parameter\n         void     drive  (ICar   car  );\n   }\n     public     class     Driver     implements     IDriver   {\n       @  Override\n         public     void     drive  (ICar   car  ) {\n           car.  run  ();\n       }\n   }  After understanding Dependency and Injection, it should be clear that the dependency injection process involves placing the corresponding object into the instance object.  B. Container Concept  After discussing the DI concept, let's move on to the container concept. In Section A, we mentioned the issue of dependencies. Although we can decouple objects using interfaces, in large-scale projects, the complexity and quantity of objects become more intricate. This is where the container mechanism comes into play.    The concept of a container is to store all objects in a \"box\". When you need to use an object, you simply retrieve it from the box. It's like a golfer who carries a golf bag to store their clubs and has a caddy to assist them. Whenever the golfer needs a specific club, they just tell the caddy.  a. Comparing \"poor\" and \"rich\" DI  After understanding dependency injection and containers, let's talk about the two main approaches: \"poor man's DI\" and \"rich man's DI\".   Poor man's DI means creating objects manually using the new keyword. This can be tedious and error-prone, especially when dealing with complex object graphs.  Rich man's DI involves using a DI container. This is like having a caddy in golf - the container manages the creation and lifetime of objects for you. You simply configure the container and it handles the rest.  C. Ioc(Inversion of Control)  We'd all rather be rich than poor, right? That's why we use DI containers! When we use a DI container to manage our objects, we're essentially implementing inversion of control.  Simply put, if object A needs to use object B, traditionally A would create a new instance of B. But with a DI container, we hand over this responsibility to the container. Instead of A actively controlling the creation of B, the container takes care of it.  This is a great example of inversion of control. In web frameworks, for instance, the framework handles tasks like listening for HTTP requests and parsing them. As developers, we don't need to worry about these low-level details; the framework takes care of them for us.   Inversion of Control (IoC) is a design pattern that shifts the control of object creation and lifecycle management to an external container or framework. Instead of a component directly controlling its dependencies, it receives them from an outside source.  D. C# Dependency Injection Framework   [Example]  After discussing DI, containers, DIP, and IOC, we should now move on to the practical implementation. In .NET, there are two commonly used DI frameworks:   Microsoft.Extensions.DependencyInjection  Autofac  As mentioned above, there are three common injection methods (constructor, property, and method), which Autofac supports. However, ASP.NET DI currently only supports constructor injection. While most online examples focus on web applications, we will be using both DI tools for desktop Windows Forms in this context.  Microsoft.Extensions.DependencyInjection  DI configuration is set in DIServiceConfigure.cs.  The lifespan of injected services  Service lifetime specifies whether a new instance of a component is created on each request via dependency injection, or if a shared instance is used throughout the application. This behavior is controlled by the service's lifetime scope.   1.AddSingleton  2.AddTransient  3.AddScoped  1. AddSingleton  Only one instance of this object is created for the entire application and is shared among all parts of the application. In simpler terms, whenever different parts of the program need to use this object, they are always using the same exact object. It's similar to a static variable, but with the advantage of allowing object-oriented design and dependency injection.  The diagram below illustrates the lifetime of objects in ASP.NET DI, with object dependencies flowing from left to right.    In our simulation scenario, we can map the objects as follows:   Request: Button_Singleton_Click event  First circle: Call LogController  Second circle: Call LogRepository  Instance: SingletonLogRepository  For the Repository injection, we use AddSingleton. To observe if the instances are indeed the same, we will generate a GUID when the object is instantiated.         // Use a singleton pattern for system-wide unique objects or when there are no concurrency or high-load request issues.\n   collection.  AddSingleton  <  ISingletonLogRepository  ,   LogRepository  >();  When the Button_Singleton_Click button is pressed, both LogController and LogController2 will call the SingletonLogRepository object and print its GUID.     private     void     Button_Singleton_Click  (  object     sender  ,   EventArgs     e  )\n   {\n         //var provider = DIServiceConfigure.GetProvider();\n         //var logController = provider.GetRequiredService<ILogController>();\n         var     log     =     \"Log1(Singleton):\"  +  \"UUID-\"  +  LogController.  OperationId  (  \"Singleton\"  );\n         var     log2     =     \"Log2(Singleton):\"     +     \"UUID-\"     +   LogController2.  OperationId  (  \"Singleton\"  );\n       richTextBox_Info.  AppendText  (log  +  \"  \\n  \"  );\n       richTextBox_Info.  AppendText  (log2   +     \"  \\n  \"  );\n       richTextBox_Info.  AppendText  (LogController.  QueryLogCount  ());\n   }  We can observe that the two printed UUIDs are identical, indicating that the SingletonLogRepository instance is shared between the LogController and LogController2.    2. AddTransient  By definition, a new instance of the object, along with any of its dependent objects, is created every time a request for it is made within the application.  The diagram below depicts the lifecycle of objects within the ASP.NET Dependency Injection system. The dependencies between objects are represented from left to right.    In our simulation model, the objects can be mapped as follows:   Request: Triggered by the Button_Transient_Click event.  First node: Invokes the LogController.  Second node: Invokes the LogController2.  Instance: Utilizes a TransientLogRepository.  To implement the Repository injection, we employ AddTransient. In order to verify that distinct instances are created, we will generate a unique identifier (GUID) upon object instantiation.     collection.  AddTransient  <  ITransientLogRepository  ,   LogRepository  >();  For scenarios requiring asynchronous operations and a high volume of requests, using the Transient lifetime is recommended. However, simulating this in a simple Windows Forms application can be challenging.  When the Button_Transient_Click button is pressed, both LogController and LogController2 will call the SingletonLogRepository object and print its GUID.     private     void     Button_Transient_Click  (  object     sender  ,   EventArgs     e  )\n   {\n        // Simulate multiple requests\n        //var provider = DIServiceConfigure.GetProvider();\n        //var logController = provider.GetRequiredService<LogController>();\n        //var log = \"Log1(Transient):\" + \"UUID-\" + logController.GUID.ToString();\n        //richTextBox_Info.AppendText(log + \"\\n\");\n              // Simulate a single requests\n        var     log     =     \"Log1(Transient):\"     +     \"UUID-\"     +   LogController.  OperationId  (  \"Transient\"  );\n        var     log2     =     \"Log2(Transient):\"     +     \"UUID-\"     +   LogController2.  OperationId  (  \"Transient\"  );\n      richTextBox_Info.  AppendText  (log   +     \"  \\n  \"  );\n      richTextBox_Info.  AppendText  (log2   +     \"  \\n  \"  );\n          //richTextBox_Info.AppendText(LogController.QueryLogCount());\n   }  We can see that the two printed UUIDs are different, indicating that the TransientLogRepository instances in Controller and Controller2 are distinct objects.    The above simulation example only demonstrates a single Request1 scenario. According to the definition, a new object is created every time a component is requested in the program. To simulate this, we have repeatedly clicked the button. To simulate multiple requests, we will uncomment the \"multiple requests\" block and comment out the \"single request\" code.     private     void     Button_Transient_Click  (  object     sender  ,   EventArgs     e  )\n   {\n         // Simulate multiple requests\n         var     provider     =   DIServiceConfigure.  GetProvider  ();\n         var     logController     =   provider.  GetRequiredService  <  LogController  >();\n         var     log     =     \"Log1(Transient):\"     +     \"UUID-\"     +   logController.GUID.  ToString  ();\n       richTextBox_Info.  AppendText  (log   +     \"  \\n  \"  );\n           // Simulate a single requests\n         //var log = \"Log1(Transient):\" + \"UUID-\" + LogController.OperationId(\"Transient\");\n         //var log2 = \"Log2(Transient):\" + \"UUID-\" + LogController2.OperationId(\"Transient\");\n         //richTextBox_Info.AppendText(log + \"\\n\");\n         //richTextBox_Info.AppendText(log2 + \"\\n\");\n           //richTextBox_Info.AppendText(LogController.QueryLogCount());\n   }  The fact that the UUIDs are unique for every request proves that the Transient lifetime of the Controller is working as expected. Each time the container is asked to provide a Controller instance, a new one is created, ensuring that there are no shared state issues between different requests. This behavior is also observed for other dependencies registered with a transient lifetime.    3. AddScope  The Scoped lifetime is arguably the most complex to understand. When visualizing the ASP.NET DI lifecycle, object dependencies are typically represented as a directed graph from left to right. A key characteristic of the Scoped lifetime is that within the boundaries of a single request, all components and their dependencies are resolved from the same scope, ensuring that they reference the same instances. Nevertheless, a new scope is initiated for each incoming request, guaranteeing that subsequent requests will receive fresh instances of these components.    In order to examine the behavior of multiple requests, we implemented a simulation by calling the Dialog method. The diagram below provides a visual representation of the experiment's outcomes. A notable observation is that the Instance property of each object...   SingletonLogRepository  TransientLogRepository  ScopedLogRepository  We can observe that within the same request flow, the ScopedLogRepository instances in LogController and LogService share the same UUID. However, when the dialog is opened a second time, the UUID becomes different from the first one. This behavior is distinct from that of a singleton.  Additionally, we can see that the Transaction instances have unique UUIDs for each object and every request.    Differentiating between Scoped and Transient lifetimes can be tricky,  here's a simpler way to grasp their differences.  The Scoped lifetime is tied to a specific scope, usually a single request in web applications. Within that scope, the first instance of a component is created and reused for all dependencies as long as the scope is active. This ensures that all components within the same request share the same instance. Once the request is complete and the scope ends, the instance is disposed. For each new request, a new instance is created, ensuring no shared state between different requests.  On the other hand, the Transient lifetime creates a new instance of a component every time it's requested, regardless of scope. This means that even within the same request or operation, multiple calls to resolve the same component will result in entirely new instances, mimicking the behavior of traditional object creation using the new keyword. As a result, Transient components are more lightweight but don't share state across different parts of the application, even within a single request.  Summary of Dependency Injection Lifetimes  The concepts of Scoped, Singleton, and Transient lifetimes are crucial in understanding how dependency injection functions across different application architectures. In web applications, where the HTTP request lifecycle is relatively short, the Scoped lifetime is commonly used. This is because each HTTP request typically represents a distinct scope, allowing objects related to the request to be instantiated once and then disposed of at the end of the request. This helps ensure efficient resource management during the short-lived nature of web requests.  In contrast, Windows Forms applications often favor the Singleton lifetime. Due to the long-running nature of desktop applications, Singletons provide a convenient way to share state across the entire application without repeatedly creating new instances. The Scoped lifetime is less frequently applied in this context since there is no natural \"request boundary\" as seen in web applications. However, it can still be useful in specific cases where scoping is required for certain operations.  The Transient lifetime, which creates a new instance of a component each time it's requested, is commonly used for short-lived objects that don't need to be shared or reused. In many cases, such objects are disposed of immediately after use. While Transient lifetimes are useful, many developers prefer manual object creation using the \"new\" keyword, especially in scenarios where the overhead of dependency injection might not be necessary. This manual approach can sometimes offer more control over object instantiation and disposal.  Dynamic injection and swapping of dependencies  After discussing object lifetimes, let's move on to the concept of swapping concrete objects. In our example, we use the IPrinter interface to inject and swap between PrinterMethodA and PrinterMethodB.  Initially, PrinterMethodA is injected into IPrinter. When the button_Printer_Click button is pressed, the console outputs \"MethodA Print: Printer Out\".    Then, we replace PrinterMethodA with PrinterMethodB.     collection.  AddTransient  <  IPrinter  ,   PrinterMethodB  >();  Upon clicking the button_Printer_Click button, the console will display the message \"MethodB Print: Printer Out\".    Additionally, we can inject a list of IPrinter instances by injecting multiple concrete implementations.     collection.  AddTransient  <  IPrinter  ,   PrinterMethodA  >();\n   collection.  AddTransient  <  IPrinter  ,   PrinterMethodB  >();  We can inject multiple PrinterMethod methods for flexibility.     private     LogController     LogController  ;\n   private     LogController2     LogController2  ;\n     // Multiple injection\n   private     IEnumerable  <  IPrinter  >   Printer  ;\n     public     Form1  (  IEnumerable  <  IPrinter  >   printer  ,\n                  LogController     logController  , \n                  LogController2     logController2  )\n   {\n         InitializeComponent  ();\n       LogController   =   logController;\n       LogController2   =   logController2;\n         // Multiple injection\n       Printer   =   printer;\n   }    Through altering the container configuration, we can dynamically replace all components that rely on the IPrinter interface, while keeping the main program's codebase unchanged. This demonstrates the power of dependency injection.  AutoFac  The sample DI configuration is specified in AutofacConfig.cs.  Autofac and ASP.NET DI share a similar concept of lifetime scope, as illustrated in the table below   Source    As indicated in the mapping table, to transition from the sample program to an Autofac implementation, you merely need to comment out the .NET DI code within Bootstrapper and Form.cs and uncomment the associated Autofac code. This straightforward modification enables you to examine the distinct object lifetime behaviors governed by each dependency injection framework.  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}.ct-086898{color:#6A737D;}.ct-157101{color:#E36209;}.dark .ct-157101{color:#FFAB70;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}"},{"id":"content:0.code:3.Pattern-Strategy.md","path":"/code/pattern-strategy","dir":"code","title":"Pattern-Strategy","description":"","keywords":["Introduction","Usage Context"],"body":"  Pattern-Strategy  Introduction  Recently, I've begun to organize the design patterns I've previously utilized. Among these patterns, the Strategy pattern is a common one, categorized under Behavioral Patterns.   Behavioral design patterns are concerned with algorithms and the assignment of responsibilities between objects.  The Strategy pattern is generally applied in scenarios where methods vary in their implementation. In such cases, an interface is declared, and different objects implement this interface. When there's a need to change the implementation method, it can be directly swapped within the Context, or the change can be made directly through Dependency Injection (DI). This approach facilitates flexible and interchangeable behavior within software components.  Usage Context  Recently, while working on national instrument communication, I encountered a piece of code that initially used GPIB for communication. Later on, USB communication was added. GPIB communication utilized the MessageBasedSession Class from NI's VISA DLL files, whereas USB communication employed the Device Class from NI's N4882 DLL files.  In the past, when writing structured programs, my instinct was to directly use a variable to switch between communication methods, as shown in the 5th line of the program below. This resulted in having to perform a check in every method (lines 29, 43). If a new communication method, such as TCP, were to be added at this point, it would require significant adjustments to the NormalBaseDevice (every function would need to be modified).     public     class     NormalBaseDevice\n   {\n           private     bool     IsGpib  ;\n           // Gpib Com\n         MessageBasedSession     GpibCom  ;\n       \n         // Usb Com\n         Device     UsbCom  ;\n       \n         public     NormalBaseDevice  (  bool     isGPIB  )\n       {\n             IsGpib   =   isGPIB;\n                   if   (isGPIB)\n               {\n                   GpibCom   =     new     MessageBasedSession  ();\n               }\n                 else\n               {\n                   UsbCom   =     new     Device  ();\n               }\n             }\n               public     string     Write  (  string     cmd  )\n           {\n                   if   (IsGpib)\n               {\n                     return   GpibCom.  Write  (cmd);\n               }\n                 else\n               {\n                     return   UsbCom.  Write  (cmd);\n               }\n             }\n               public     int     Read  ()\n           {\n                   if   (IsGpib)\n               {\n                     return   GpibCom.  Read  ();\n               }\n                 else\n               {\n                     return   UsbCom.  Read  ();\n               }\n             }\n         }  This approach results in all communication methods being tightly coupled together, making any adjustments inevitably impactful. At this point, we can use an interface to achieve separation, allowing each communication method to implement the interface. Adjustments can then be made directly in the Context.  First, we declare an IStrategy interface, with the interface methods for Write and Read as follows:     public     interface     IStrategy\n   {\n         string     Write  (  string     cmd  );\n           int     Read  ();\n   }  Next, we write the GpibCom and UsbCom concrete classes and implement the IStrategy methods.     public     class     GpibCom   :   IStrategy\n   {\n         private     MessageBasedSession     Device  ;\n           public     GpibCom  ()\n       {\n           Device   =     new     MessageBasedSession  ();\n       }\n           public     string     Write  (  string     cmd  )\n       {\n             return   Device.  Write  (cmd);\n            \n       }\n           public     int     Read  ()\n       {\n             return   Device.  Read  ();\n       }\n   }        public     class     UsbCom   :   IStrategy\n   {\n         private     Device     Device  ;\n           public     UsbCom  ()\n       {\n           Device   =     new     Device  ();\n       }\n           public     string     Write  (  string     cmd  )\n       {\n             return   Device.  Write  (cmd);\n       }\n           public     int     Read  ()\n       {\n             return   Device.  Read  ();\n       }\n   }  At this point, within the Context, we can freely swap to the method we need, as shown below:     class     Program\n   {\n          static     void     Main  (  string  []   args  )\n       {\n             IStrategy     com  ;\n           \n           com   =     new     GpibCom  ();\n             var     cmd     =   com.  Write  (Action);\n           \n           com   =     new     UsbCom  ();\n             var     cmd     =   com.  Write  (Action);\n           \n       }\n   }  Additionally, it's common to pair the Strategy pattern with a simple factory to encapsulate the switch for selecting which object to instantiate, as shown below:     public     class     ComFacotry\n   {\n         public     enum     ComType   {   Gpib  ,   Usb  }\n           public     static     IStrategy     CreateCom  (  ComType     type  )\n       {\n             IStrategy     com  ;\n               switch   (type)\n           {\n                 case   ComType.Gpib:\n                   com   =     new     GpibCom  ();\n                     break  ;\n                   \n                 default  :\n                   com   =     new     UsbCom  ();\n                     break  ;\n             }\n               return   com;\n       }\n   }  The use of Context then becomes:     class     Program\n   {\n          static     void     Main  (  string  []   args  )\n       {\n             var     usbCom     =   ComFacotry.  CreateCom  (ComType.Usb);\n             var     cmd     =   usbCom.  Write  (Action);\n       }\n   }  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-086898{color:#6A737D;}"},{"id":"content:0.code:4.Pattern-Singleton.md","path":"/code/pattern-singleton","dir":"code","title":"Pattern-Singleton","description":"","keywords":["Introduction","Context Usage"],"body":"  Pattern-Singleton  Introduction  Recently, I began organizing and documenting the design patterns I've used before. The Singleton pattern is one of the more frequently used patterns, categorized under Creational Patterns.   Creational Patterns: Creational patterns provide various object creation mechanisms, which increase flexibility and reuse of existing code.  The Singleton pattern is among the commonly used design patterns, falling under the category of Creational Patterns. Creational patterns provide various mechanisms for creating objects, enhancing flexibility and reuse of existing code.  A Singleton object is instantiated automatically in memory upon the program's startup, similar to the commonly seen Static Method. This method can be used without needing to be created explicitly.  Context Usage   Static Method  Static Object Definition  For instance, if we're looking to implement a certain static method, we typically use the static keyword. For example, it's common in systems to declare shared configuration variables that are read from a config file. This approach allows these variables to be accessed globally without creating an instance of the class. The Singleton pattern can be applied to ensure that only one instance of the configuration settings is created and shared across the system, enhancing consistency and preventing potential issues from multiple instantiations.     //Static Method\n      public     class     StaticAppSetting\n   {\n         public     static     readonly     string     AppName     =     GetConfigValue  (  \"AppName\"  );\n           public     static     string     GetConfigValue  (  string     value  )\n       {\n             return   ConfigurationManager.AppSettings[value];\n       }\n   }\n     // Context\n   class     Program\n   {\n         static     void     Main  (  string  []   args  )\n       {\n           Console.  WriteLine  (StaticAppSetting.AppName);\n       }\n   }  Using static methods allows us to easily implement shared static functionality, which is more commonly utilized in structured programming. However, if we want to incorporate object-oriented concepts, such as inheritance and interfaces, relying solely on static methods won't meet our needs for object-oriented design.  When implementing the Singleton pattern, typically two key actions are undertaken:   Creating a static private field (INSTANCE): This field holds the singleton instance. You implement logic to check whether this field is null. If it is, a new instance of the class is created. This ensures that only one instance of the class exists within the application at any time. Additionally, the constructor of the class is made private or inaccessible to prevent external instantiation.  Creating a public static method or property to access the instance: This method or property provides a global access point to the singleton instance. It checks if the INSTANCE field is null; if so, it initializes the instance. If not, it returns the existing instance. This method ensures that the class adheres to the singleton property, providing a single, globally accessible instance throughout the application.  Online content can be presented in various styles, and currently, the method my friend shared is considered the most elegant as described below.     //Singleton Pattern\n   public     class     SingletonAppSetting\n   {\n         //Instance Method\n         private     static     class     SingletonHolder\n       {\n             static     SingletonHolder  () { }\n             internal     static     readonly     SingletonAppSetting     INSTANCE     =     new     SingletonAppSetting  ();\n       }\n       \n         // Instance Field\n         public     static     SingletonAppSetting     Instance   {   get   {   return   SingletonHolder.INSTANCE; } }\n           public     readonly     string     AppName  ;\n    \n         // Construct\n         protected     SingletonAppSetting  ()\n       {\n           AppName   =     GetConfigValue  (  \"AppName\"  );\n       }\n           public     string     GetConfigValue  (  string     value  )\n       {\n             return   ConfigurationManager.AppSettings[value];\n       }\n   }\n     // Context\n   class     Program\n   {\n         static     void     Main  (  string  []   args  )\n       {\n            Console.  WriteLine  (SingletonAppSetting.Instance.AppName);\n       }\n   }  If we have another system that we want to adapt, which involves network-related settings, we can revise it by using a static approach. One method is to add a new static class, and the other is to modify the existing static class as follows.        public     class     StaticAppSetting\n   {\n         public     static     readonly     string     AppName     =     GetConfigValue  (  \"AppName\"  );\n           public     static     readonly     string     LocalIp     =     GetConfigValue  (  \"LocalIp\"  );\n           \n         public     static     readonly     string     LocalPort     =     GetConfigValue  (  \"LocalPort\"  );\n           public     static     string     GetConfigValue  (  string     value  )\n       {\n                 return   ConfigurationManager.AppSettings[value];\n       }\n   }  If using the Singleton pattern, you can declare a new InternetAppSetting object that inherits from the original AppSetting as follows.     public     class     SingletonInternetAppSetting   :   SingletonAppSetting\n   {\n         private     static     class     SingletonHolder\n       {\n             static     SingletonHolder  () { }\n             internal     static     readonly     SingletonInternetAppSetting     INSTANCE     =     new     SingletonInternetAppSetting  ();\n       }\n         public     static     new     SingletonInternetAppSetting     Instance   {   get   {   return   SingletonHolder.INSTANCE; } }\n           public     readonly     string     LocalIp  ;\n         public     readonly     string     LocalPort  ;\n           protected     SingletonInternetAppSetting  ()\n       {\n           LocalIp   =     GetConfigValue  (  \"LocalIp\"  );\n           LocalPort   =     GetConfigValue  (  \"LocalPort\"  );\n       }\n   }\n     //Context\n   class     Program\n   {\n         static     void     Main  (  string  []   args  )\n       {\n           Console.  WriteLine  (SongletonInternetAppSetting.Instance.AppName);\n           Console.  WriteLine  (SongletonInternetAppSetting.Instance.LocalIp);\n           Console.  WriteLine  (SongletonInternetAppSetting.Instance.LocalPort);\n       }\n   }  Additionally, we can utilize an interface to work with the Singleton method. For instance, we can declare an ISetting interface.     public     interface     ISetting\n   {\n         string     GetConfigValue  (  string     value  );\n   }  Have the object implement the interface.   public class SingletonAppSetting : ISetting\npublic class SingletonInternetAppSetting : ISetting  This would then allow for direct substitution of the implementation in the context of use.     class     Program\n   {\n         static     void     Main  (  string  []   args  )\n       {\n             ISetting     appSetting     =   SingletonAppSetting.Instance;\n             var     valueFromConfig     =   appSetting.  GetConfigValue  (  \"Value\"  );\n             appSetting   =   SongletonInternetAppSetting.Instance;\n             var     valueFromIni     =   appSetting.  GetConfigValue  (  \"Value\"  );\n       }\n   }  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-086898{color:#6A737D;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}"},{"id":"content:0.code:5.Aspect-Oriented Programming.md","path":"/code/aspect-oriented-programming","dir":"code","title":"AOP (Aspect-Oriented Programming)","description":"","keywords":["Introduction","Dynamic proxies","AspectCore","Summary"],"body":"  AOP (Aspect-Oriented Programming)  Introduction  Recently, during a discussion with a friend about mechanisms for logging, I learned about Aspect-Oriented Programming (AOP) and decided to delve a little into its usage and implementation. Although King's use of AspCore proved to be a quick way to implement AOP, I still wanted to spend some time understanding how to achieve this without relying on a framework.  AOP employs a cross-cutting technique to insert new methods into the original class without altering or disrupting the original class methods. This technique is a skillful way to introduce new functionalities into a program, enhancing its modularization and flexibility without compromising the integrity of the original code structure.   Separate cross-cutting concerns from the core business logic to further enhance the modularity of the code!  Let me give you an example. Suppose you received the following service code that has already been written:     public     interface     IXXXService  {\n         void     QueryData  ()\n   }\n     public class XXXService:IXXXService{\n       \n         public     IXXXService  (){\n       \n       }\n       \n       public   void     QueryData  (){\n           ....\n       }\n   }  This service method does not provide a logging mechanism. If you want to add logging, what should you do?  The first method you might consider is directly modifying the source code.     public     interface     IXXXService  {\n         void     QueryData  ()\n   }\n     public class XXXService:IXXXService{\n       \n       private readonly ILog _log;\n       \n         public     IXXXService  (  ILog     log  ){\n           _log   =   log;\n       }\n       \n       public   void     QueryData  (){\n             try  {\n              _log.  I  (  \"QueryData\"  );\n              ....\n           }\n             catch  (  Exception     e  )\n           {\n             _log.  E  (  $\"Imp QueryData \"     +   {e.  ToString  ()});\n           }\n           ....\n       }\n   }  We modify the service directly by injecting your own logging function and incorporating try-catch mechanisms within the method. However, this approach clearly alters the original source code. Moreover, logging and the original logic are not directly related, so combining them isn't an ideal method.  What alternatives exist that allow us to add logging without modifying the internal code, ensuring the service retains its sole responsibility of handling business logic? One solution is to employ a simple static proxy method to achieve this.       // 原程式碼\n   public     interface     IXXXService  {\n         void     QueryData  ()\n   }\n     public class XXXService:IXXXService{\n       \n         public     IXXXService  (){\n       \n       }\n       \n       public   void     QueryData  (){\n           ....\n       }\n   }\n     //使用Proxy\n     public     class     ProxyXXX   :   IXXXService  {\n           private     readonly     IXXXService     _xxxService  ;\n         private     readonly     ILog     _log  ;\n       \n         public     ProxyXXX  (  IXXXService     xxxService  ,   ILog     log  ){\n           _xxxService   =   xxxService;\n           _log   =   log;\n       }\n       \n         public     void     QueryData  (){\n             try  {\n               _log.  I  (  \"QueryData\"  );\n               _xxxService.  QueryData  ();\n               \n           }  catch  (  Exception     e  ){\n               _log.  E  (  $\"Imp QueryData \"     +   {e.  ToString  ()});\n           }\n       }\n     }  Declare a Proxy Class to implement the IXXXService method. Since this is through a proxy, there's an opportunity to intervene before and after execution actions (try-catch, log). This allows for the addition of logging functionality without altering the original code.  This concept is essentially a form of aspect-oriented programming (AOP), which separates concerns (e.g., logging) from the business core, further decoupling them.  In more detail, many program requirements, such as logging, are not directly related to the program's logic but need to be interjected into functions at appropriate times. Such requirements are typically addressed using aspects or cross-cutting concerns. This approach separates these unrelated needs from the original logical functions, enhancing the design's modifiability and reducing coupling.  The diagram conceptually illustrates that in programming, we often need to handle functions like error logging, authentication, and potentially adding user activity tracking. By applying the AOP concept, we don't have to individually add these functionalities to each method. Instead, as indicated by the grey, yellow, and red arrows, all methods must go through authentication, data handling, and error processing. This represents a type of middleware design concept.    After discussing the above, if the service you're working with has hundreds of methods, it's impractical to implement a static proxy for each method to introduce additional functionalities. In such cases, we would use dynamic proxies to achieve this goal.  Dynamic proxies  In C#, dynamic proxies can be implemented using two classes: RealProxy and DispatchProxy. The former is available for use in the standard .NET Framework, while the latter is intended for use with .NET Core.  Practical Scenario  Based on the Dynamic Proxy tutorial scenario from   MSDN , let's assume we have a Customer Model scenario, where we interact with the data through a Repository.  Therefore, we first focus on designing the foundational infrastructure for the Context, including the implementation related to the Customer Model and Repository. The Repository will handle the usual CRUD (Create, Read, Update, Delete) operations.    Customer Model        public     class     Customer\n   {\n       public     int     Id   {   get  ;   set  ; }\n       public     string     Name   {   get  ;   set  ; }\n       public     string     Address   {   get  ;   set  ; }\n   }  IRepository        public     interface     IRepository  <  T  >\n   {\n         void     Add  (  T     entity  );\n         void     Delete  (  T     entity  );\n         void     Update  (  T     entity  );\n         IEnumerable  <  T  >   GetAll  ();\n         T     GetById  (  int     id  );\n   }  Repository     public     class     Repository  <  T  > :   IRepository  <  T  >\n   {\n             public     void     Add  (  T     entity  )\n           {\n               Console.  WriteLine  (  \"Adding {0}\"  , entity);\n           }\n             public     void     Delete  (  T     entity  )\n           {\n               Console.  WriteLine  (  \"Deleting {0}\"  , entity);\n           }\n             public     void     Update  (  T     entity  )\n           {\n               Console.  WriteLine  (  \"Updating {0}\"  , entity);\n           }\n             public     IEnumerable  <  T  >   GetAll  ()\n           {\n               Console.  WriteLine  (  \"Getting entities\"  );\n                 return     null  ;\n           }\n             public     T     GetById  (  int     id  )\n           {\n               Console.  WriteLine  (  \"Getting entity {0}\"  , id);\n                 return     default  (  T  );\n           }\n   }  After implementing the foundational infrastructure, in a scenario without using a Proxy, you can directly create a Repository instance to perform CRUD operations.  Main        class     Program\n       {\n             static     void     Main  (  string  []   args  )\n           {\n                 //Simple Use - No Logger\n               Console.  WriteLine  (  \"***  \\r\\n   Begin program - no logging  \\r\\n  \"  );\n                 IRepository  <  Customer  >   customerRepository     =\n                   new     Repository  <  Customer  >();\n                 var     customer     =     new     Customer\n               {\n                   Id   =     1  ,\n                   Name   =     \"Customer 1\"  ,\n                   Address   =     \"Address 1\"\n               };\n               customerRepository.  Add  (customer);\n               customerRepository.  Update  (customer);\n               customerRepository.  Delete  (customer);\n               Console.  WriteLine  (  \"  \\r\\n  End program - no logging  \\r\\n  ***\"  );\n             }\n       }  The output is as follows    Implementing Dynamic Proxy using RealProxy    Add logging to the Repository layer.  Now, we implement logging and Try-Catch in every CRUD operation at the Repository layer through RealProxy. Implementing Dynamic Proxy with RealProxy is quite straightforward, requiring only the implementation of the Invoke method. The underlying principle uses C# reflection to implement the methods of the entity being proxied. For those interested in a deeper exploration, consider reviewing the article on   Aspect-oriented programming . It discusses the principles of AOP, which are largely based on Reflection, Metaobject Protocols, and Composition Filters.     public     class     DynamicProxy  <  T  > :   RealProxy\n       {\n             private     readonly     T     _decorated  ;\n             public     DynamicProxy  (  T     decorated  )\n             :   base  (  typeof  (  T  ))\n           {\n               _decorated   =   decorated;\n           }\n             // Log Fun\n             private     void     Log  (  string     msg  ,   object     arg     =     null  )\n           {\n               Console.ForegroundColor   =   ConsoleColor.Red;\n               Console.  WriteLine  (msg, arg);\n               Console.  ResetColor  ();\n           }\n             // Impleation Invoke\n             public     override     IMessage     Invoke  (  IMessage     msg  )\n           {\n                 var     methodCall     =   msg   as     IMethodCallMessage  ;\n                 var     methodInfo     =   methodCall.MethodBase   as     MethodInfo  ;\n                   Log  (  \"In Dynamic Proxy - Before executing '{0}'\"  , methodCall.MethodName);\n                   try\n               {\n                     var     result     =   methodInfo.  Invoke  (_decorated, methodCall.InArgs);\n                     Log  (  \"In Dynamic Proxy - After executing '{0}' \"  , methodCall.MethodName);\n                     return     new     ReturnMessage  (result,   null  ,   0  , methodCall.LogicalCallContext, methodCall);\n               }\n                 catch   (  Exception     e  )\n               {\n                     Log  (string.  Format  (  \"In Dynamic Proxy- Exception {0} executing '{1}'\"  , e),methodCall.MethodName);\n                     return     new     ReturnMessage  (e, methodCall);\n               }\n           }\n       }  After implementing the dynamic proxy, the client-side usage is as follows.     var     repository     =     new     Repository  <  Customer  >();                      \n   var     customerRepoProxy     =  (  IRepository  <  Customer  >)  new     DynamicProxy  <  IRepository  <  Customer  >>(repository);\n     var     newcustomer     =     new     Customer\n    {\n       Id   =     1  ,\n       Name   =     \"New Customer \"  ,\n       Address   =     \"New Address\"\n    };\n   customerRepoProxy.  Add  (newcustomer);\n   customerRepoProxy.  Update  (newcustomer);\n   customerRepoProxy.  Delete  (newcustomer);  We can create a Repository factory that can flexibly generate or assemble different proxies.     public     class     RepositoryFactory\n   {\n             public     static     IRepository  <  T  >   Create  <  T  >()\n           {\n                 var     repository     =     new     Repository  <  T  >(); \n                 var     decoratedRepository     =  (  IRepository  <  T  >)  new     DynamicProxy  <  IRepository  <  T  >>(repository).  GetTransparentProxy  ();\n                 return   decoratedRepository;\n           }\n   }  Add authentication to the Repository  By utilizing Dynamic Proxy, after adding logging and try-catch to the original Repository CRUD operations, we then attempt to construct a Dynamic Proxy to simulate method-level permission verification.  Implement an AuthenticationProxy.     public     class     AuthenticationProxy  <  T  > :   RealProxy\n   {\n             private     readonly     T     _decorated  ;\n             public     AuthenticationProxy  (  T     decorated  )\n             :   base  (  typeof  (  T  ))\n           {\n               _decorated   =   decorated;\n           }\n             private     void     Log  (  string     msg  ,   object     arg     =     null  )\n           {\n               Console.ForegroundColor   =   ConsoleColor.Green;\n               Console.  WriteLine  (msg, arg);\n               Console.  ResetColor  ();\n           }\n             public     override     IMessage     Invoke  (  IMessage     msg  )\n           {\n                 var     methodCall     =   msg   as     IMethodCallMessage  ;\n                 var     methodInfo     =   methodCall.MethodBase   as     MethodInfo  ;\n                   try\n               {\n                     Log  (  \"User authenticated - You can execute '{0}' \"  ,methodCall.MethodName);\n                     var     result     =   methodInfo.  Invoke  (_decorated, methodCall.InArgs);\n                     return     new     ReturnMessage  (result,   null  ,   0  ,\n                     methodCall.LogicalCallContext, methodCall);\n               }\n                 catch   (  Exception     e  )\n               {\n                     Log  (string.  Format  (\n                       \"User authenticated - Exception {0} executing '{1}'\"  , e),methodCall.MethodName);\n                     return     new     ReturnMessage  (e, methodCall);\n               }\n                   Log  (  \"User not authenticated - You can't execute '{0}' \"  ,methodCall.MethodName);\n                 return     new     ReturnMessage  (  null  ,   null  ,   0  , methodCall.LogicalCallContext, methodCall);\n             }\n   }  After implementing the AuthenticationProxy, we modify the original Repository Factory accordingly.        public     class     RepositoryFactory\n   {\n             public     static     IRepository  <  T  >   Create  <  T  >()\n           {\n                 var     repository     =     new     Repository  <  T  >();\n               \n               \n                 var     decoratedRepository     =  (  IRepository  <  T  >)  new     DynamicProxy  <  IRepository  <  T  >>(repository).  GetTransparentProxy  ();\n                   // Create a dynamic proxy for the class already decorated\n               decoratedRepository   =  (  IRepository  <  T  >)  new     AuthenticationProxy  <  IRepository  <  T  >>(decoratedRepository).  GetTransparentProxy  ();\n                     return   decoratedRepository;\n           }\n   }  The client-side is as follows.     //Use Dynamic Proxy \n   Console.  WriteLine  (  \"***  \\r\\n   Begin program - logging with dynamic proxy  \\r\\n  \"  );\n   IRepository  <  Customer  >   customerRepoProxy     =   RepositoryFactory.  Create  <  Customer  >();\n   var     newcustomer     =     new     Customer\n   {\n      Id   =     1  ,\n      Name   =     \"New Customer \"  ,\n      Address   =     \"New Address\"\n   };\n   customerRepoProxy.  Add  (newcustomer);\n   customerRepoProxy.  Update  (newcustomer);\n   customerRepoProxy.  Delete  (newcustomer);\n   Console.  WriteLine  (  \"  \\r\\n  End program - logging with dynamic proxy  \\r\\n  ***\"  );\n   Console.  ReadLine  ();    Implement a Dynamic Proxy using DispatchProxy    Add logging to the Repository  Based on the above example, we implement it again using DispatchProxy. The operation of DispatchProxy is similar, but in addition to implementing Invoke, we also need to specifically implement (decorate) the part about creating class instances.     public     class     DynamicProxy  <  T  > :   DispatchProxy     where     T   :   class\n   {\n               public     T     Target   {   get  ;   private     set  ; }\n           \n             public     DynamicProxy  () :   base  ()\n           {\n             }\n               public     static     T     Decorate  (  T     target     =     null  )\n           {\n                 var     proxy     =     Create  <  T  ,   DynamicProxy  <  T  >>()   as     DynamicProxy  <  T  >;\n                 proxy.Target   =   target   ??   Activator.  CreateInstance  <  T  >();\n                   return   proxy   as     T  ;\n           }\n                 private     void     Log  (  string     msg  ,   object     arg     =     null  )\n           {\n               Console.ForegroundColor   =   ConsoleColor.Red;\n               Console.  WriteLine  (msg, arg);\n               Console.  ResetColor  ();\n           }\n               protected     override     object     Invoke  (  MethodInfo     targetMethod  ,   object  []   args  )\n           {\n                 Log  (  \"In Dynamic Proxy - Before executing '{0}'\"  , targetMethod.Name);\n                   try\n               {\n                     // 使用Class Method\n                     var     result     =   targetMethod.  Invoke  (Target, args);\n                     Log  (  \"In Dynamic Proxy - After executing '{0}' \"  , targetMethod.Name);\n                     return   result;\n                 }\n                 catch  (  Exception     e  )\n               {\n                     Log  (string.  Format  (  \"In Dynamic Proxy- Exception {0} executing '{1}'\"  , e), targetMethod.Name);\n                     return     null  ;\n               }\n           }\n   }  After implementation, the method of use on the client side is as follows.        var     repository     =     new     Repository  <  Customer  >();\n      var     messageDispatchProxy     =   DynamicProxy<  IRepository  <  Customer  >>.  Decorate  (repository);   Implement a RepositoryFactory to flexibly generate or assemble different proxies.     public     class     RepositoryFactory\n   {\n             public     static     IRepository  <  T  >   Create  <  T  >()\n           {\n                 var     repository     =     new     Repository  <  T  >();\n                 var     proxyRepo     =   DynamicProxy<  IRepository  <  T  >>.  Decorate  (repository);\n                 return   proxyRepo;\n           }\n   }  The client-side usage is as follows.     var     messageDispatchProxy     =   RepositoryFactory.  Create  <  Customer  >();\n   var     customer     =     new     Customer\n   {\n     Id   =     1  ,\n     Name   =     \"Customer 1\"  ,\n     Address   =     \"Address 1\"\n   };\n   messageDispatchProxy.  Add  (customer);\n   messageDispatchProxy.  Update  (customer);\n   messageDispatchProxy.  Delete  (customer);    AspectCore  For AOP (Aspect-Oriented Programming), .NET Core already has ready-to-use tools, one of which is called AspectCore. Regarding   AspectCore , I find Neil Tsai's article   AspectCore | .Net Core Lightweight AOP Implementation  quite clear in its explanation. However, I will continue to use the context described in the Microsoft MSDN tutorial.  As mentioned above, we talked about using a dynamic proxy to fetch Customer data from a repository. Next, we will use AspectCore on the web to achieve this. In terms of web architecture, we design using a common centralized architecture, writing a Customer Service to use the repository. And with AspCore, when calling the service from a controller, we add logging to display.  Step1:Add a new Web MVC project.  We use Visual Studio to add a new MVC project. There are no special settings during the process, so there's no need for detailed description of the initial project creation.  Step2:Install AspCore  Directly install AspCore using the CLI, or use the NuGet Package Manager to install AspectCore.Extensions.DependencyInjection.   dotnet add package AspectCore.Extensions.DependencyInjection\n  Step3:Implement the CustomerService  Next, we start implementing the service. First, add the Customer Service Interface, implementing only AddCustomer.     public     interface     ICustomerService\n   {\n       void     AddCustmoer  (  Customer     customer  );\n   }  Then proceed with the implementation.     public     class     CustmoerService   :   ICustomerService\n   {\n           private     readonly     IRepository  <  Customer  >   _repo  ;\n           public     CustmoerService  (  IRepository  <  Customer  >   repo  )\n       {\n               _repo   =   repo;\n       }\n           public     void     AddCustmoer  (  Customer     customer  )\n       {\n               _repo.  Add  (customer);\n       }\n     }  Step4:Design a service interceptor using the AbstractInterceptorAttribute.  After completing the service implementation, begin writing the interceptor with AspCore (utilizing the service as a proxy). This will intercept service calls and insert log displays both before and after the calls.     public     class     ServiceInterceptor   :   AbstractInterceptorAttribute\n   {\n       [  FromServiceContext  ]\n         public     ILogger  <  ServiceInterceptor  >   Logger   {   get  ;   set  ; }\n           public     async     override     Task     Invoke  (  AspectContext     context  ,   AspectDelegate     next  )\n       {\n             try\n           {\n               Logger.  LogInformation  (  \"In Dynamic Proxy - Before executing '{0}'\"  , context.ServiceMethod.Name); \n                 await     next  (context);    // 進入 Service 前會於此處被攔截（如果符合被攔截的規則）...\n               Logger.  LogInformation  (  \"In Dynamic Proxy - After executing '{0}'\"  , context.ServiceMethod.Name);\n           }\n             catch   (  Exception     ex  )\n           {\n               Logger.  LogError  (ex.  ToString  ());    // 記錄例外錯誤...\n                 throw  ;\n           }\n       }\n   }  Step5:Configure the DI (Dependency Injection) and AspCore proxy in Startup  A. In Configure, set up Services and Repository  Set up Service and Repository DI configurations in Startup     services.  AddTransient  <  IRepository  <  Customer  >,   Repository  <  Customer  >>();\n   services.  AddTransient  <  ICustomerService  ,   CustmoerService  >();  B.Set up the dynamic proxy  Set up DynamicProxy DI configurations in Startup.     services.  ConfigureDynamicProxy  (  config     =>   { config.Interceptors.  AddTyped  <  ServiceInterceptor  >(Predicates.  ForService  (  \"*Service\"  )); });  Common proxy rule configurations   Everything will be proxied:     config.Interceptors.  AddTyped  <  ServiceInterceptor  >();   Services with a suffix of \"Service\" will be proxied:     config.Interceptors.  AddTyped  <  ServiceInterceptor  >(Predicates.  ForService  (  \"*Service\"  )   Methods with a prefix of \"Execute\" will be proxied:     config.Interceptors.  AddTyped  <  ServiceInterceptor  >(Predicates.  ForMethod  (  \"Execute*\"  ));   Services under the namespace \"App1\" will not be proxied:     config.NonAspectPredicates.  AddNamespace  (  \"App1\"  );   Services in namespaces that have \"App1\" as the last segment will not be proxied:     config.NonAspectPredicates.  AddNamespace  (  \".App1\"  );   \"ICustomService\" will not be proxied:     config.NonAspectPredicates.  AddService  (  \"ICustomService\"  );   Services with a suffix of \"Service\" will not be proxied:     config.NonAspectPredicates.  AddService  (  \"Service\"  );   Methods named \"Query\" will not be proxied:     config.NonAspectPredicates.  AddMethod  (  \"Query\"  );   Methods with a suffix of \"Query\" will not be proxied:     config.NonAspectPredicates.  AddMethod  (  \"*Query\"  );  AspectCore also provides the NonAspectAttribute to prevent services or methods from being proxied. Simply add   NonAspect  to the method on the interface, and this method of the service will be ignored and not proxied.     public     interface     IXXXService\n   {\n     [  NonAspect  ]\n     void XXXMethod;\n   }  C. In program.cs, add UseServiceProviderFactory at the CreateHostBuilder location.  In Program.cs, at the CreateHostBuilder section, add UseServiceProviderFactory(new DynamicProxyServiceProviderFactory()) to delegate the default DI (Dependency Injection) handling to AspectCore.     public     static     IHostBuilder     CreateHostBuilder  (  string  []   args  )   =>\n      Host.  CreateDefaultBuilder  (args)\n          .  ConfigureWebHostDefaults  (  webBuilder     =>\n          {\n              webBuilder.  UseStartup  <  Startup  >();\n          })\n          .  UseServiceProviderFactory  (  newDynamicProxyServiceProviderFactory  ());  Step 6: Inject ICustomerService into HomeController, and add a service action in the Privacy API.  To easily demonstrate the AspCore service interception functionality, we add the AddCustomer service action to the Privacy API of the HomeController in the .Net Core Web initial project. When a user clicks on the Privacy tab, it will call the CustomerService function. The interceptor will intercept this call, print a log first, and then execute the service method.     private     readonly     ILogger  <  HomeController  >   _logger  ;\n   private     readonly     ICustomerService     _repoService  ;\n     public     HomeController  (  ILogger  <  HomeController  >   logger  ,   ICustomerService     repoService  )\n   {\n      _logger   =   logger;\n      _repoService   =   repoService;\n   }\n     public     IActionResult     Privacy  ()\n   {\n                 var     customer     =     new     Customer\n               {\n                   Id   =     1  ,\n                   Name   =     \"Customer 1\"  ,\n                   Address   =     \"Address 1\"\n               };\n                 _repoService.  AddCustmoer  (customer);\n                   return     View  ();\n   }  Step 7: Execute the test.      Summary  This article roughly organizes the use cases and methods of AOP and also provides a simple demonstration of how to use AspCore. It is hoped that those who have never heard of or used AOP can quickly gain an understanding of the concept.   Code Demo  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}.ct-086898{color:#6A737D;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}"},{"id":"content:0.code:6.FP(Functional programming)簡易釐清 (含Currying 與 Closure 理解).md","path":"/code/fp(functional-programming)-(currying-closure-)","dir":"code","title":"FP(Functional programming)簡易釐清 (含Currying 與 Closure 理解)","description":"","keywords":["OOP與FP在純數據處理應用場景差異","FP中的Closure與Currying","小記"],"body":"  FP(Functional programming)簡易釐清 (含Currying 與 Closure 理解)  tags:   Code Sense    Design-Pattern  前幾天在看Youtube講  Currying 時，看到留言處有人回關於在FP的世界closure觀念有很重要。於是好奇看了一下兩者的差異，對於OOP派的自己，已經很久沒寫FP設計了，有的話也是在很多年前對於軟工還處於碼農對OO完全不會時期的自己。特別寫一篇記錄Currying與Closure的認知紀錄，記錄過程順便釐清OOP與FP差異之處。  OOP與FP在純數據處理應用場景差異  OOP在物件、設計模式面去設計實際架構非常好用，但在FP世界哩，感覺大多是用於處裡數據面的應用，大多會以API形式去呈現。對於一開始的認知是這樣，針對這部分做一個研讀  數學和科學計算  FP稍微查文獻，理念和概念來源於一種稱為 λ 演算（Lambda Calculus）的數學邏輯系統。在λ演算中，所有東西都是函數，並且只有一種操作，那就是應用（apply）。這種簡單、統一的架構與數學中函數的概念有很強的關聯，並且有助於構建簡潔、可預測和易於推理的程序。   純粹性：數學中，函數的值僅取決於它的輸入，並不取決於任何外部狀態。這與λ演算中函數的概念相一致，也是函數式編程中\"純函數\"的概念的基礎。  函數的一等性：數學中，函數可以作為輸入給其他函數，也可以作為其他函數的輸出。這與λ演算中函數的概念相一致，並且是函數式編程中\"函數作為一等公民\"的概念的基礎。  所以，可以說FP在設計之初就深受數學的影響。然而，這並不意味著函數式編程只適合於數學問題。事實上，函數式編程的概念，如  不可變性 、  純函數 和  函數組合 ，在許多不同的應用情境和領域中都是有用的。  對於OOP較熟看到此的人大致會有點小感覺，會覺得對於這些特性的應用，其實OOP概念上也是可實現的。但在於數據需要進行大量並行處理和異步操作的場景中，FP的不可變性和狀態無關性可以帶來很大的優勢。  另外更簡單數據應用差異的例子如下，假設我們有一個任務，需要對一個數據集（比如一個數字列表）中的每個元素都進行平方運算。在一種物件導向的實現方式中，我們可能會創建一個數據集類別（DataSet），並為該類別提供一個方法來進行平方運算：     class     DataSet  :\n         def     __init__  (self, data):\n             self  .data   =   data\n           def     square  (self):\n             for   i   in     range  (  len  (  self  .data)):\n                 self  .data[i]   =     self  .data[i]   **     2  Context如下     data   =   DataSet([  1  ,   2  ,   3  ,   4  ,   5  ])\n   data.square()\n   print  (data.data)    # Outputs: [1, 4, 9, 16, 25]  這種實現方式的問題在於，每當我們需要對數據進行新的操作時（比如開根號，取對數等），我們都需要在 DataSet 類別中添加一個新的方法。如果操作的種類很多，這樣會使得 DataSet 類別變得非常臃腫。  在FP設計中，我們將這些操作抽象為函數，並且可以很容易地將這些函數應用到數據集中的每一個元素如下     def     square  (x):\n         return   x   **     2\n     data   =   [  1  ,   2  ,   3  ,   4  ,   5  ]\n   data   =     list  (  map  (square, data))\n     print  (data)    # Outputs: [1, 4, 9, 16, 25]  在這種情況下，當我們需要添加新的操作時，只需添加一個新的函數即可，無需修改數據結構。而且，這些函數可以重用於任何類型的數據集，而不僅僅是我們定義的 DataSet 類別。因此，對於這種需要對大量數據進行同樣操作的情境，函數式程式設計的風格通常能提供更高的靈活性和可重用性。  其實也許這時針對這例子你可能會思考，這例子有點爛，誰會針對這應用情境去設計物件。沒錯，你的觀察非常準確! FP傾向於表達計算過程為數學函數的求值，更多的關注在數據操作和轉換上，而不是像物件導向程式設計那樣，重視封裝資料和行為到物件中，並使用物件的互動來描述計算過程。  這並不是說 FP 不適合處理需要物件設計的場景，而是它們解決問題的角度和方法有所不同。事實上，在很多現代的語言中，FP 和 OOP 都被視為是語言的重要組成部分，並且在很多場景中可以一起使用。  以上述的例子，如果在一個更複雜的系統中，這些數據被封裝在物件中，並且物件有自己的行為和狀態，那麼使用 OOP 可能會更合適。但是，當我們需要處理大量數據，並且對這些數據進行各種轉換和處理時，FP 提供的 map, reduce, filter 等高階函數可以讓我們更直接地描述這些操作。  講到此，應該對於數學和科學計算的應用場景FP與OOP的差異性就會有較清楚的認知。  大數據處理和分析  FP與不可變數據結構的結合，在大數據處理和分析的應用中非常有效。比如，Apache Spark就是一個使用Scala（一種支持FP的語言）編寫的大數據處理框架，並利用了許多FP的概念，如不可變數據集和高階函數。  接著具體來談談 Apache Spark 以及函數式編程在大數據處理中的應用  Apache Spark 是一個用於大規模數據處理的開源集群運算系統，提供了一個非常高級的（抽象層次高的）數據操作接口，稱為 Resilient Distributed Dataset（RDD）。RDD是一種不可變的、分布式的、元素類型相同的數據集合。  在 Spark 中，RDD 是主要的數據結構，所有的操作（如 map, filter, reduce 等）都是在 RDD 上進行的。由於 RDD 是不可變的，所以一旦創建就不能被修改，這使得 Spark 能夠在大規模並行計算中有效地追蹤數據的來源和變化。  以下是一個用 Spark 的 Python API (pyspark) 進行簡單 map-reduce 操作的例子：     from   pyspark   import   SparkContext\n     sc   =   SparkContext(  \"local\"  ,   \"count app\"  )\n   nums   =   sc.parallelize([  1  ,   2  ,   3  ,   4  ,   5  ])\n     # 使用 map 操作將所有數字加一\n   add_one   =   nums.map(  lambda   x: x   +     1  )\n     # 使用 reduce 操作將所有數字相加\n   sum     =   add_one.reduce(  lambda   a, b: a   +   b)\n     print  (  sum  )    # 輸出： 20  這個例子中，首先創建了一個 RDD（nums），然後使用 map 操作將 RDD 中的每個數字加一，接著使用 reduce 操作將所有數字相加。這些操作都是以函數的形式（lambda 函數）進行的，並且可以自動地在多個節點上並行執行。  這就是函數式編程在大數據處理中的一個應用例子。通過使用不可變數據結構（RDD）和高階函數（map, reduce 等），Spark 可以將大規模數據處理問題抽象化，並提供一種有效、簡潔和可伸縮的解決方案。  如果這例子太難，在舉一個更簡單的例子來理解FP在大數據處理中的優勢。假設需要處理一個包含數百萬條記錄的數據集，每條記錄都是一個人的個人信息（例如，名字，年齡，地址等）。任務是找出年齡在 18 歲以上的所有人。  在物件導向的方法中，可能會像這樣來處理：       class     Person  :\n         def     __init__  (self, name, age, address):\n             self  .name   =   name\n             self  .age   =   age\n             self  .address   =   address\n     people   =   [  ...  ]    # 假設這是一個包含數百萬個 Person 物件的列表\n     adults   =   []\n   for   person   in   people:\n         if   person.age   >=     18  :\n           adults.append(person)  而在FP設計中，可以將這個任務抽象為一個過濾操作：       people   =   [  ...  ]    # 假設這是一個包含數百萬個 tuples 的列表，每個 tuple 是一個人的信息\n     adults   =     filter  (  lambda   person: person[  1  ]   >=     18  , people)  在這個例子中，使用了內建的 filter 函數和一個 lambda 函數來達成目標。這種方式更為簡短和直觀，並且可以自動地利用並行處理來加速計算（例如，如果使用的是像 PySpark 這樣的分布式計算框架）。  講到此，再去觀察上述例子，FP設計具有狀態不變性、Pure Function(Output只取決於Input)，因此他非常適合拿來做數據的應用處理。但真實世界不可能完全靠Pure方法設計，光是撈資料庫顯示到前端操作部分就會牽扯到很多資料面的狀態(與數據面不一樣)的變化處裡。此時OOP就會是較好的選擇。  FP中的Closure與Currying  理解Closure  雖然FP特性在於狀態不變性去設計，但有些數據操作流程還是會有數據狀態應用，此時就可以使用Closure較精簡進階的寫法去達成。Closure是一種特殊類型的函數，它可以訪問並操作在它被定義的時候已經存在的變數，即使這個函數在它的定義環境之外被調用。特別是在需要創建一個能夠記住某些特定狀態的函數時。在 Python 中，Closure可以讓一個函數「記住」它被定義時的環境。舉個例子如下     def     make_multiplier  (x):\n         def     multiplier  (n):\n             return   x   *   n\n         return   multiplier\n     times3   =   make_multiplier(  3  )\n   print  (times3(  10  ))    # Outputs: 30  這裡的 multiplier(n) 函數就是一個Closure。為何呢？因為它「記住」了當它被創建時所在的環境。當我們調用 make_multiplier(3) 時，我們實際上是在創建一個新的 multiplier(n) 函數，這個函數「知道」 x 是 3(狀態不再改變)，即使multiplier(n) 函數是在 make_multiplier(x) 函數的內部被定義的，當它被返回並指派給 times3 變數時，它實際上已經離開了 make_multiplier(x) 的「範疇」或「作用域」。換句話說，multiplier(n) 函數已經在 make_multiplier(x) 函數的外部被調用。  這裡再加強補充multiplier(n) 函數已經在 make_multiplier(x) 函數的外部被調用這件事情。  在 Python 中，當你定義一個函數，這個函數會有它自己的作用域，也就是說它只能直接訪問在自己內部定義的變數。 make_multiplier(x) 函數裡，這個函數可以訪問變數 x，因為 x 是作為參數傳進來的。  然後， make_multiplier(x) 函數裡面定義了一個新的函數 multiplier(n)。這個新函數可以訪問 n，因為 n 是作為參數傳進來的，但它也能訪問 x，即使 x 是在外部的 make_multiplier(x) 函數中定義的。這是因為 multiplier(n) 函數是在 make_multiplier(x) 函數的作用域內創建的，所以它能夠訪問 make_multiplier(x) 函數的作用域。  然後將multiplier(n) 函數作為 make_multiplier(x) 函數的返回值，因此當調用 make_multiplier(3)，實際上獲得的是一個新的函數，這個新函數在被調用時會將傳入的值乘以3。  當將 make_multiplier(3) 的返回值（即 multiplier(n) 函數）賦值給 times3 並調用 times3(10) 時，即使 multiplier(n) 函數現在已經在 make_multiplier(x) 函數的外部被調用，它仍然能夠「記住」 x 的值（在這裡是3）。當你調用 times3(10)（實際上就是調用 multiplier(10)）時，它知道要將10乘以3，因為它“記住”了 x 的值為3。   這就是所謂的Closure：一個函數記住並能夠訪問其外部作用域的變數(上述例子的x)，即使它現在在其創建時的作用域之外被調用。  換句話說multiplier(n)記住了當它被創建時的環境，它依舊有能力記住並訪問它所在的外部函數的變數(make_multiplier(x))。  理解Currying  因為FP大多是做function的組合(無物件特性)，要讓程式碼比較自然且容易閱讀。此時我們就會談到Currying。它是一種將接收多個參數的函數轉換為一系列使用一個參數的函數的技術。例如，一個接收兩個參數的函數 f(x, y) 可以被 Curry 化為一個接收一個參數並返回一個函數的函數 g(x)(y)。在這種情況下，可以首先提供第一個參數（例如，g(2)），並得到一個新的函數，該函數接收第二個參數並返回最終結果（例如，g(2)(3) 返回與 f(2, 3) 相同的結果）。簡單來說Currying就是一種將接受多個參數的函數轉換為一系列使用一個參數的函數的技術。  根據上述例子在Currying實現回如下     def     multiply  (x):\n         def     multiply_x  (y):\n             return   x   *   y\n         return   multiply_x\n     double   =   multiply(  2  )\n   print  (double(  5  ))    # Outputs: 10  將一個接受兩個參數的函數 multiply 轉化為一個函數 multiply_x，這個函數接受一個參數並且返回一個函數，這個返回的函數也接受一個參數。我們可以看到，這裡的 multiply_x 同時也是一個Closure，因為它記住了 x 的值。雖然這範例看起來與Closure很像，但其實應用場景上還是有些差異...請往下看~  Currying與Closure情境差異  這邊做個小整理  Closure：Closure的主要用途是「記住」來自外部函數的變數。當內部函數被返回並在其他地方使用時，即使原來的外部函數已經完成執行，這個內部函數仍然可以訪問和操作那些變數。這可以用來創造有狀態的函數，也就是說，這些函數的行為會被他們的「環境」影響。在上述例子中，multiplier(n) 就是一個Closure。  Currying：Currying的主要用途是將一個接收多個參數的函數轉化成一連串接收單一參數的函數。這可以使得我們能夠以更靈活的方式使用函數，尤其是在涉及函數作為參數的場合。在你的第二個例子中，multiply(x) 就進行了Currying的過程。  假設我們在開發一款遊戲，並且我們想要計數玩家取得的分數。我們可以使用Closure來實現這個需求：     def     create_score_counter  ():\n       score   =     0\n         def     add_score  (points):\n             nonlocal   score\n           score   +=   points\n             return   score\n         return   add_score\n     counter   =   create_score_counter()\n   print  (counter(  10  ))    # Outputs: 10\n   print  (counter(  20  ))    # Outputs: 30  create_score_counter 函數返回一個add_score，這個Closure會記住並修改它的外部環境中的 score 變數。即使 create_score_counter 函數的執行已經結束，add_score 函數仍然可以訪問和修改 score 變數。  在此例子中，Closure是更直觀且更簡單的方法來達到需求。我們要記錄和更新一個「狀態」（score），並且這個狀態需要在函數被連續調用的過程中保留。Closure讓我們能夠將狀態（這裡是 score）與用來操作狀態的函數（這裡是 add_score）捆綁在一起，並且這個狀態會在連續的函數調用之間被「記住」。  （Currying）主要用來將一個接收多個參數的函數轉換成一系列接收單一參數的函數。當我們需要讓一個函數的一部分參數（或預設參數）被「固定」，而另一部分參數在之後被提供時，Currying就很有用。然而在此例子中，我們主要的需求是「記錄和更新狀態」，而不僅僅是「固定一部分參數」。  另一個情境，假設我們正在處理一個列表，我們想要對列表中的每個元素應用一個函數。我們可以使用Currying來創建一個函數，這個函數接收一個函數和一個列表作為參數，然後返回一個新的函數，這個新的函數接收一個元素並應用我們之前傳入的函數：     def     map_function  (func):\n         def     apply_func_to_list  (lst):\n             return   [func(x)   for   x   in   lst]\n         return   apply_func_to_list\n     double   =     lambda   x: x   *     2\n   map_double   =   map_function(double)\n     print  (map_double([  1  ,   2  ,   3  ,   4  ,   5  ]))    # Outputs: [2, 4, 6, 8, 10]  map_function 函數實際上就是在進行Currying：它接收一個函數作為參數，然後返回一個新的函數 apply_func_to_list。這個新的函數可以接收一個列表作為參數，並且對列表中的每個元素應用我們傳入 map_function 的那個函數。  此例沒有需要維護和更新的\"狀態\"。這個例子中的 map_function 可以看作一個被Currying的函數：它首先接收一個函數 func，然後返回一個新的函數 apply_func_to_list，這個新的函數會接收一個列表 lst，並將 func 應用到 lst 的每個元素上。  這種將函數的參數分階段接收的特性，使得你可以提前固定部分參數（在這裡就是固定了 func），並生成一個新的函數用於處理之後的參數（在這裡就是處理 lst）。這種特性在一些情況下可以使代碼更簡潔，更有彈性。而Closure則是針對狀態維護與更新的簡潔寫法。  看完後  應該對於FP設計概念及Closure與Currying有較清楚的認知 但實際還是要更具Context的親自下手設計才會更有感覺。  小記  其實Currying在C#也是可以用Delgate實現，如下     public     static     void     Main  ()\n   {\n         Func  <  int  ,   Func  <  int  ,   int  >>   curriedMultiply     =     MultiplyCurried  ();\n         Func  <  int  ,   int  >   multiplyBy2     =     curriedMultiply  (  2  );\n           int     result     =     multiplyBy2  (  3  );   // result will be 6\n       Console.  WriteLine  (result);\n   }\n     static     Func  <  int  ,   Func  <  int  ,   int  >>   MultiplyCurried  ()\n   {\n         return     a     =>     b     =>   a   *   b;\n   }  雖說可以實現，但Delgate的設計主要用於事件處理、異步調用與回調函數場景，跟數據世界Closure的應用就不太相同。在數據分析和處理的領域，Closure的主要用途是「記住」來自外部函數的變數，並且在閉包函數內部操作這些變數。這特別適用於需要維護和更新內部狀態的情況。例如，在統計數據或者計數的時候，Closure可以使我們的代碼變得更簡潔，而且容易理解。  而Delegate應用於通用的軟件設計領域，例如事件處理、異步調用和回調函數等場景。一個Delgate實際上就是一個包含有指向其他函數或方法的指針的對象。當一個Delgate被調用時，它可以調用它所引用的函數或方法。這可以動態地改變函數或方法的行為，並讓程式碼在執行時更加靈活。  感覺自己可以在對FP做更完整的認知，每當聽到有人說分兩派信仰，我覺得應該是對於OOP與FP沒有完整的認知或是適當應用場景經驗的不足才會有偏差偏於用哪一種。  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}.ct-086898{color:#6A737D;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}"},{"id":"content:0.code:7.var使用灌輸.md","path":"/code/var","dir":"code","title":"var使用灌輸","description":"以前寫C#，對var的使用非常習慣。但蠻多人對於var使用非常感冒，因為對於型別不明確這件事情會大大降低閱讀性。我的思維覺得如果var真的這麼母湯，java與Net這兩種強行別語言應該就不會納入var的使用…所以特別對這部分去做一個較深入的整理。","keywords":["一、簡述var","二、探討使用情境","三、簡易總結條例"],"body":"  var使用灌輸  以前寫C#，對var的使用非常習慣。但蠻多人對於var使用非常感冒，因為對於型別不明確這件事情會大大降低閱讀性。我的思維覺得如果var真的這麼母湯，java與Net這兩種強行別語言應該就不會納入var的使用…所以特別對這部分去做一個較深入的整理。  一、簡述var  var 關鍵字用於隱式型別聲明。簡單來說，當你使用 var，你告訴編譯器去自動判斷變數的型別。舉例來說：     var     i     =     10  ;    // 自動判斷為 int\n   var     s     =     \"hello\"  ;    // 自動判斷為 string  為什麼我們不寫明確的Type，要使用var?  var在一開始的語言設計主要為了簡化代碼和提高可讀性，另外在C#一些語法糖上，可以讓開發者在撰寫語法上可以更簡便。這樣描述還是會太含糊，下述會針對幾個情境去探討使用var的適用性。  二、探討使用情境  此章節會探討var的適用與不適用情境去  a. 簡潔性  假設你正在使用一個很長、很複雜的型別名稱，像是 Dictionary<int, List  >。如果每次宣告這個型別都需要完全寫出來，那會讓代碼看起來很雜亂，也浪費你的時間。又或是很長的物件宣告，例如Google Cloud的KMS Package KeyManagementServiceClient 物件。  I.好的使用情境(Linq)  關於複雜型別，舉個例子如下，我們Net Core Linq為例子會很好使看出他的優勢，假設你有一個名為 users 的列表，每個元素都是一個 User 類別的實例，你想要選出所有年齡大於 20 的用戶。  如果不使用 var，你需要這樣寫：     IEnumerable  <  User  >   usersAbove20     =   users.  Where  (  u     =>   u.Age   >     20  );  但假如你的查詢更複雜，回傳的型別也會更複雜。例如，你想從這些用戶中取出他們的姓名和年齡，然後排序。這時，你可能需要寫：     IOrderedEnumerable  <  AnonymousType1  >   result     =   users.  Where  (  u     =>   u.Age   >     20  )\n                                                    .  Select  (  u     =>     new   { u.Name, u.Age })\n                                                    .  OrderBy  (  u     =>   u.Age);  相對地，使用 var 會讓這一切變得簡單許多：     var     result     =   users.  Where  (  u     =>   u.Age   >     20  )\n                     .  Select  (  u     =>     new   { u.Name, u.Age })\n                     .  OrderBy  (  u     =>   u.Age);  在這串程式碼，他會是一個匿名型別如下    匿名型別是一種沒有顯示名稱的型別。這種型別主要用於儲存一些臨時的、結構簡單的資料。Linq的世界，當你需要從一個複雜的資料結構中提取特定的資訊，但又不想定義一個全新的類別。通常就會使用var，操作起來會相對於簡單很多。這段重點在於資料過渡階段的操作，尤其是當你不需要長期存儲或多次使用某個特定結構的資料時，使用 var 和匿名型別是很好的選擇。  再舉一個例子，一個任務是對來自多個來源（資料庫、APIs、用戶輸入等）的資料進行綜合處理。你需要用到多個類型、集合、使用var Lambda 表達式如下     var     query     =   dataContext.Customers\n                 .  Where  (  c     =>   c.Age   >     18  )\n                 .  SelectMany  (  c     =>   c.Orders, (  c  ,   o  )   =>     new   { c.Name, o.OrderId, o.Amount })\n                 .  GroupBy  (  x     =>   x.Name)\n                 .  Select  (  g     =>     new   { Customer   =   g.Key, TotalAmount   =   g.  Sum  (  x     =>   x.Amount) });  如果不使用var會如下     IEnumerable  <  IGrouping  <  string  ,   anonymousType  <  string  ,   int  ,   decimal  >>>   query     =   \n         dataContext.Customers\n                    .  Where  (  c     =>   c.Age   >     18  )\n                    .  SelectMany  (  c     =>   c.Orders, (  c  ,   o  )   =>     new   { c.Name, o.OrderId, o.Amount })\n                    .  GroupBy  (  x     =>   x.Name)\n                    .  Select  (  g     =>     new   { Customer   =   g.Key, TotalAmount   =   g.  Sum  (  x     =>   x.Amount) });  在這個例子中，不使用 var 使得類型非常明確，但代價是代碼變得過於冗長和難以閱讀。  II.好的使用情境(過長的Object命名)  Lambda語法糖在很多語言其實都有，另外一個var是用情境在撰寫flow過程中，蠻常遇到一些很長的Object命名，舉一個Google Cloud KMS封裝非對稱解密方法如下  過程中，可以看到物件的命名規則，我們可以很清楚理解整個流程為   建立keyManagementServiceClient  產生cryptoKeyVersionName  decode cryptoKeyVersionName  為bytes  開始作非對稱解密  回傳結果     @Override\n       public     String     asymmetricDecrypt  (  KeyInfoDto     keyInfo  ,   String     ciphertext  ) throws IOException {\n         try   (KeyManagementServiceClient keyManagementServiceClient = KeyManagementServiceClient.create()) {\n             // 調用CreateCryptoKeyVersionName方法，根據keyInfo生成密鑰版本名稱\n           CryptoKeyVersionName     cryptoKeyVersionName      =     generateCryptoKeyVersionName  (keyInfo);\n             // 將加密後的文本（Base64）轉換為字節數組\n           byte  []   decryptedBytes     =     decodeBase64ToBytes  (ciphertext);\n             // 使用密鑰版本名稱進行非對稱解密\n           AsymmetricDecryptResponse     asymmetricDecryptResponse     =   keyManagementServiceClient.  asymmetricDecrypt  (cryptoKeyVersionName, ByteString.  copyFrom  (decryptedBytes));\n             // 將解密後的字節數組轉換為UTF-8字符串並返回\n           return   asymmetricDecryptResponse.  getPlaintext  ().  toStringUtf8  ();\n       }\n     }  如果我們把程式碼改成使用var如下，此時可以問自己，有影響可讀性嗎? 其實沒有，相對來說，對於宣告Type物件，看起來會簡潔一點，但重點在Naming有定義清楚狀況下，閱讀起來不會讓長的物件命名礙到眼，除非你flow某個段落不太清楚想去理解他的型態，此時我們就可以透過IDE協助滑鼠移過去理解。     @Override\n       public     String     asymmetricDecrypt  (  KeyInfoDto     keyInfo  ,   String     ciphertext  ) throws IOException {\n         try   (var keyManagementServiceClient = KeyManagementServiceClient.create()) {\n             // 調用CreateCryptoKeyVersionName方法，根據keyInfo生成密鑰版本名稱\n           var     cryptoKeyVersionName      =     generateCryptoKeyVersionName  (keyInfo);\n             // 將加密後的文本（Base64）轉換為字節數組\n           byte  []   decryptedBytes     =     decodeBase64ToBytes  (ciphertext);\n             // 使用密鑰版本名稱進行非對稱解密\n           var     asymmetricDecryptResponse     =   keyManagementServiceClient.  asymmetricDecrypt  (cryptoKeyVersionName, ByteString.  copyFrom  (decryptedBytes));\n             // 將解密後的字節數組轉換為UTF-8字符串並返回\n           return   asymmetricDecryptResponse.  getPlaintext  ().  toStringUtf8  ();\n       }\n     }  我覺得上述例子感受度還不夠強烈，我在用一段程式碼來比較如下，這段在於對RSA加密，使用指定的DER格式公鑰（derKeyBytes）來完成整個過程，我們直接使用var   讀取公鑰：x509EncodedKeySpec = new X509EncodedKeySpec(derKeyBytes); 這行用於讀取DER格式的公鑰數據。  生成RSA公鑰對象(rsaPublicKey)：接下來，用這些公鑰數據來生成一個實際的RSA公鑰對象。  初始化加密器：選用了一種特殊的RSA加密模式和填充方式，這是由Cipher.getInstance(\"RSA/ECB/OAEPWithSHA-256AndMGF1Padding\");這行確定的。  設置參數：在這個例子中，使用了OAEP（光學端對端加密填充）和SHA-256作為Hash算法。  加密動作：最後，rsaCipher.doFinal(plaintext.getBytes(StandardCharsets.UTF_8));這行將明文轉換為字節數組並進行加密。  返回結果：加密後的數據會被轉換成Base64格式的字串，然後返回。  整個過程可以感受到，在命名有命好狀況下，其實完全不影響整個易讀性。甚至簡化冗長的物件名稱宣告，讓程式碼寬度不至於太長很難閱讀。     /**\n      * 進行RSA加密使用指定的DER格式公鑰。\n      *\n      * @param plaintext    需要加密的明文。\n      * @param derKeyBytes  DER格式的公鑰位元組。\n      * @return             返回加密後的Base64字串。\n      * @throws InvalidKeySpecException  如果加密過程中出現任何安全相關的異常。\n      */\n       private     String     encryptUsingPublicKey  (  String     plaintext  ,   byte  []   derKeyBytes  ) throws InvalidKeySpecException, NoSuchAlgorithmException, NoSuchPaddingException, InvalidKeyException, InvalidAlgorithmParameterException, IllegalBlockSizeException, BadPaddingException {\n         var     x509EncodedKeySpec     =     new     X509EncodedKeySpec  (derKeyBytes);\n           // 生成RSA公鑰對象\n         var     rsaPublicKey      =   KeyFactory.  getInstance  (  \"RSA\"  ).  generatePublic  (x509EncodedKeySpec);\n           // 初始化加密器和相關參數\n         var     rsaCipher     =   Cipher.  getInstance  (  \"RSA/ECB/OAEPWithSHA-256AndMGF1Padding\"  );\n         var     oaepParameters     =     new     OAEPParameterSpec  (  \"SHA-256\"  ,   \"MGF1\"\n               , MGF1ParameterSpec.SHA256, PSource.PSpecified.DEFAULT);\n       rsaCipher.  init  (Cipher.ENCRYPT_MODE, rsaPublicKey , oaepParameters );\n           // 進行加密\n         byte  []   encryptedData      =   rsaCipher.  doFinal  (plaintext.  getBytes  (StandardCharsets.UTF_8));\n         return     encodeBase64ToString  (encryptedData);\n     }  以下為比照組，使用強型別type，我們再來讀一段就會有感覺相對上述段還來的些微不易閱讀     /**\n      * 進行RSA加密使用指定的DER格式公鑰。\n      *\n      * @param plaintext    需要加密的明文。\n      * @param derKeyBytes  DER格式的公鑰位元組。\n      * @return             返回加密後的Base64字串。\n      * @throws InvalidKeySpecException  如果加密過程中出現任何安全相關的異常。\n      */\n       private     String     encryptUsingPublicKey  (  String     plaintext  ,   byte  []   derKeyBytes  ) throws InvalidKeySpecException, NoSuchAlgorithmException, NoSuchPaddingException, InvalidKeyException, InvalidAlgorithmParameterException, IllegalBlockSizeException, BadPaddingException {\n         X509EncodedKeySpec     x509EncodedKeySpec     =     new     X509EncodedKeySpec  (derKeyBytes);\n           // 生成RSA公鑰對象\n         PublicKey     rsaPublicKey      =   KeyFactory.  getInstance  (  \"RSA\"  ).  generatePublic  (x509EncodedKeySpec);\n           // 初始化加密器和相關參數\n         Cipher     rsaCipher     =   Cipher.  getInstance  (  \"RSA/ECB/OAEPWithSHA-256AndMGF1Padding\"  );\n         OAEPParameterSpec     oaepParameters     =     new     OAEPParameterSpec  (  \"SHA-256\"  ,   \"MGF1\"\n               , MGF1ParameterSpec.SHA256, PSource.PSpecified.DEFAULT);\n       rsaCipher.  init  (Cipher.ENCRYPT_MODE, rsaPublicKey , oaepParameters );\n           // 進行加密\n         byte  []   encryptedData      =   rsaCipher.  doFinal  (plaintext.  getBytes  (StandardCharsets.UTF_8));\n         return     encodeBase64ToString  (encryptedData);\n     }  III. 不好的使用情境  聊完對於簡潔性好處外，其實var也有不適當的應用情境。我們直接帶Context情境去了解，請看下述測試段程式碼，下段程式碼為一個對稱與非對稱的API測試，我們要比較encrypt 與 decrypt 。此時如果我們使用var，我們會不清楚們比較的是什麼…String? 或是 byte   ?  在這個很需要資料明確定義型態的情境，就不適合使用var去做宣告     String     testPlaintext     =  \"PAL\"  ;\n   // 設置金鑰訊息\n   var     keyInfoDto     =     new     KeyInfoDto  ();\n   keyInfoDto.  setProjectId  (  \"affable-cacao-389805\"  );\n   keyInfoDto.  setLocationId  (  \"asia-east1\"  );\n   keyInfoDto.  setKeyRingId  (  \"cathy-sample-project\"  );\n   keyInfoDto.  setKeyVersion  (  \"1\"  );\n     // 測試對稱非對稱 (明文加密->解密->比對)\n   keyInfoDto.  setKeyId  (  \"kms-sdk-testing\"  );\n   // 對稱加密\n   var     encrypt     =   kmsService.  symmetricEncrypt  (keyInfoDto,testPlaintext);\n   // 對稱解密\n   var     decrypt     =   kmsService.  symmetricDecrypt  (keyInfoDto,encrypt);\n   // 解密後比對明文\n   Assert.  hasText  (decrypt,testPlaintext);  所以確實在某些狀況，var的宣告不太適當，例如   跨平台或多語言整合   如果你的後端微服務需要與其他平台或用不同語言寫的服務進行交互，明確的型別會更容易讓其他開發者理解代碼。     // 不適合用 var，因為其他開發者或平台需要知道明確的型別\n   HttpResponse  <  String  >   response     =   client.  send  (request, HttpResponse.BodyHandlers.  ofString  ());  容易混淆的型別   當你處理兩個或多個類似但不完全相同的型別時，使用 var 可能會造成混淆。     // 明確型別可以避免將 BigDecimal 與 BigInteger 混淆\n   BigDecimal     bigDecimal     =     new     BigDecimal  (  \"10.0\"  );\n   BigInteger     bigInteger     =     new     BigInteger  (  \"10\"  );  數學運算   當你進行數學運算時，使用 var 也可能會引發問題。例如：     // 是 int 還是 double？\n   var     result     =     10     /     2  ;  初始化值不清晰   當初始化值不提供足夠的型別信息時，使用 var 會讓型別變得不明確。     // 到底是 List<String> 還是 ArrayList<String>，或是其他？\n   var     names     =   Arrays.  asList  (  \"Alice\"  ,   \"Bob\"  );  單元測試與資料驗證(包含上述比較encrypt 與 decrypt例子)   在單元測試中，你經常需要確保變數的型別與預期相符。使用明確的型別可以提供更多的上下文信息。     // 測試情境中，我們想要確保 getResult 返回的是一個 List\n   List  <  String  >   result     =   someObject.  getResult  ();  foreach寫法   如果使用foreach迴圈，並且集合的項目型別不是很明確，那麼使用var可能會降低程式碼的可讀性。     List  <  DetailedOrder  >   orders     =     GetOrders  ();\n     // 使用明確型別\n   foreach   (  DetailedOrder     order     in   orders)\n   {\n         // ...\n   }\n     // 使用 var\n   foreach   (  var     order     in   orders)\n   {\n         // ...\n   }  因此在需要明確資料型態以增加代碼可讀性和維護性的場合，特別是涉及跨平台交互、容易混淆的型別、數學運算、不明確的初始化值，以及單元測試與資料驗證時，使用    var  往往不是最佳選擇。  b. 鼓勵描述性命名(有意義命名)  當你使用 var 來宣告變數時，因為變數的型別不是明確寫在代碼中，所以為了讓讀取代碼的人（或者是你自己在未來能夠更快速地理解這個變數是用來做什麼的，你會被鼓勵給變數一個更具描述性的名字。  假設你不使用 var，而是使用明確的型別名稱來宣告變數：     List  <  Student  >   list     =     new     List  <  Student  >();  在這裡，你可能簡單地命名變數為    list ，因為型別    List<Student>  已經告訴你這是一個學生列表。  但如果你使用    var ：     var     students     =     new     List  <  Student  >();  這時，你可能會選擇一個更具描述性的名稱 students，來清晰地表明這個變數是用來存儲學生的。這樣，即使型別不是直接寫在代碼中，但從變數名稱 students，我們仍然可以很快地了解它的用途。  不過這部分，我覺得實際也很看開發者Clean Code Sense，如果一個連Clean Code都不懂的其實用var就會是一件很災難的事情。  三、簡易總結條例    代碼簡潔性 ：使用    var  可以使代碼更簡潔，尤其是當處理長型別名稱時。   型別推斷 ：對於匿名型別（如 Linq 查詢結果），必須使用    var 。   讀取性 ：在某些情境下，避免重複的型別名稱可以提高代碼的讀取性。  情境  不明確的型別 ：如果情境需求初始化表達式不清楚，使用    var  可能會導致讀者不知道變數的實際型別。   過度使用 ：呼應上一點，不考慮情境過度使用    var  可能會使代碼難以維護和理解。  使用 var 可以提高代碼的可讀性和靈活性，特別是在需要處理匿名或動態型別時。當然，開發者應該根據具體情況來決定是否使用 var，並確保代碼的意圖仍然清晰。  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}.ct-086898{color:#6A737D;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}"},{"id":"content:0.index.md","path":"/","dir":"","title":"Home","description":"","keywords":[],"body":"     Hakuna Matata.   I am a dedicated learner, and together with my friend, we have formed a team called Code Sense in Kaohsiung. We engage in research and study on a weekly basis, with a passion for learning new technologies and skills.Continual learning is the driving force of my life.     8-9 years of software development and maintenance experience  3-4 years of experience in automation domain development  1-2 years of experience in department software training and management  Familiar with integration and connection of various factory systems (  MES ,   WMS ,   PLC )  Proficient in web system front-end and back-end development and database design  Well-versed in measurement instrument and motor PC Base control (  RS232 ,   ModBusTCP ,   EtherCAT )      My Personal Moments.      Reading Blog   Regular Reading and Public Journal of Thoughts and Moods.[  Medium ]    Code Sense Trello   Regular Study and Discussion Sessions with Friends.\n[  Code Sense Trello ]    Slider   My Presentation Slides.[  Slider Link ]    Technical Documentation   Regular document writing and temporary storage.[  Hackmd ]"},{"id":"content:1.architecture:1.DDD實戰-Module.md","path":"/architecture/ddd-module","dir":"architecture","title":"DDD實戰-Module","description":"","keywords":["一、關於Module","二、構成Module考量五要素","三、Package Vs Module","四、一般Module設計方法","五、未使用DDD的一般Service服務設計","六、在DDD世界中的Module設計","補充","參考"],"body":"  DDD實戰-Module  一、關於Module  在闡述Module前，讓我們先來比對非物件化設計與物件化設計的差異性。看到下圖左，在非物件化的傳統設計，資料與方法操作上是整個拆開設計，在資料流複雜外我們可以看相依性高，偶合的狀況也較高。  接著討論物件化設計部分，先不論封裝等物件操作技巧，單純根據資料與方法關係設計出物件(物件特性:資料(Data) 物件行為:方法(Function))，我們可以看到原本互相依賴的Fun關係可各自獨立使用，在資料流上也較為單純。    物件化後，雖然在資料操作面上簡化許多，但隨著專案功能性逐漸增加，物件與功能處理流程上會趨近越複雜，如下圖為一個簡易的系統，此系統根據外部Sensor系統獲取、過濾與計算資料，在沒模組化的設計前提，可以看到Control與Data Flow呈現一個較零散的關係狀態    此時我們根據Sensor功能額外設計一個Sensor模組如下，此時我們就可以針對Sensor相對應的資料模組將之封裝在模組內，此模組對外則就單純開放運算(calculation)與過濾(filtering)的功能。  模組化後可以明顯看出控制與資料流的彼此相依性值，因此降低了修改與擴充狀況的互相影響性值。    模組化指是將  系統功能程序作分離獨立(特定功能class的容器) ，除了功能獨立外，也強調設計上可以根據功能隨意抽換模組。達到高內聚、低耦合的目的，進而提高開發者的生產力(將複雜的功能拆分管理)。並讓程式碼能夠透過引用的方式來重複使用，提升重用性(Reusable)   Modular design allows code to remain agile in the face of ever-changing requirements.   二、構成Module考量五要素   Propose:Module功能目標單一職責，盡量不要與其他模組設計有太多相依關係。  Interface:模組功能API所提供使用方式要簡潔易懂，通常User不需要去了解實際內部的實作方式，只需專注在確定輸入什麼，會輸出什麼可達到什麼功能。  Encapsulation:封裝模組，除了讓不暴露資料結構讓使用者亂使用外，對於在修改細節上也能較不容易直接影響使用端。另外再使用抽象實作上，物件抽象化多少還是難以避免Leaky Abstractions的問題。  Implementation:實作上除了考量功能正確性外還需考慮效能、測試與功能架構程式碼最小化。  Connection(關聯性):呼應Propose功能單一職責，將與其他模組相依性最小化。   三、Package Vs Module  在了解Module構成要素後~隨著專案模組(Module)的增加，將難以管理及問題的追蹤，這時候就能將模組(Module)打包成套件(Package)，利用其階層式的結構來彈性規劃模組(Module)。   Module:單一功能模組  Package:多Module組成  四、一般Module設計方法  在非以領域事件為出發點的設計上，大部分狀況會根據功能設計成物件與介面使用。下圖為Zebra SDK Package的Module列項，我們可以大致分得出，他的Module設計就是根據功能性值去區分(graphics, certificate...)    接著我們點近discovery看提供什麼API功能，可以看到可以使用的介面功能以及相關功能可使用的功能物件。    在點進個別更詳細功能介紹，我們就會看到這物件功能具有什麼資料特質、物件實體化須提供什麼參數以及此物件可使用哪些方法。   Field: address，IP or Mac Adress  Constructor:物件實體化須提供印表機的IP or Mac Adress  Method:可使用的物件功能，在此例看起來需實作getConnection功能    上述為印表機找到印表機裝置功能的模組化介紹   五、未使用DDD的一般Service服務設計  上述一般Module設計概念聊完，在聊DDD Module設計之前先來聊一下在一般未使用DDD領域設計的服務系統會如何設計。  在對大部分的開發者，一開始習慣設計都以數據為考量的集中式設計架構。設計架構上會出現比較常見到的分層式設計，大致分成Controller, Service, Repositories, Models   xxx/Model\n   數據庫Model、Request Model, Response Model  xxx/Controller\n   給Client端的API第一時間接口，提供Get,Post,Delete,Update API。除了此在此層一般都會安插屬性驗證Client第一時間傳過來的資料是否正確。並作實際的DTO(Object 與DB Model Mapping)轉換。  xxx/Service\n   這層基本上就是作商業邏輯的處哩，進到此層的資料基本上都是做完DTO轉換，在此層通常會作實際的資料邏輯處理處理完後再往DB方向送。  xxx/Repositorie\n   DataSource(DBContext)上一層，一般會除了實際面DataSource讀寫變更操作外，Source 資料Join處理也會在此層處理。一般多這一層都是為了隔開DataSource的來源切換，不管置換不同的DB系統，或是Source改成Shared Prefs，都可快速置換資料來源。     六、在DDD世界中的Module設計  上述稍微帶過Module的設計概念後，接著探討在DDD世界裡，Module的設計概念如何~大致分成幾個探討議題  1.DDD設計步驟流程     在探討需求架構DDD設計的第一步，就是根據需求情境列出事件風暴(Event storming)，並在事件風暴中的用戶操作、事件、以及依賴關係根據這些要素設計歸納出領域與實體。  接著第二步在領域實體之間找尋彼此務的關聯性，將具有相關的實體組合成聚合(Aggregate)，同時確定聚合根(Aggregate Root)。在聚合根行程時，基本上第一層邊界(邏輯邊界-虛線)也會跟著產生，他們會在同一個服務器中運行。  當聚合規劃好後~會根據業務及語意邊界等因素，將一個或多個聚合規劃訂製在一個限界上下文內(服務邊界)，形成領域模型。    2.程式碼一級目錄架構     Interface(API Interface)\n   給使用者API介面，使用者透過Restful請求，將資料傳到此層，解析用戶傳送的請求資訊，資料的組裝、資料傳輸格式以及 Facade 介面等代碼都會放在這一層目錄裡。  Application\n   他有點像是原先集中式設計Service的功能，實作所有相依於指定前端之使用案例的地方。 例如，與 Web API 服務相關的實作。若使用的是 CQRS 方法，它便會包含查詢、微服務接受的命令，甚至是微服務之間的事件驅動通訊 (整合事件)。  Domain\n   它主要存放領域層核心業務邏輯相關的代碼。領域層可以包含多個聚合代碼包，它們共同實現領域模型的核心業務邏輯。聚合以及聚合內的實體、方法、領域服務和事件等代碼會放在這一層目錄裡。  Infrastructure\n   它主要存放基礎資源服務相關的代碼，為其它各層提供的通用技術能力、三方套裝軟體、資料庫服務、配置和基礎資源服務的代碼都會放在這一層目錄裡。  3.在DDD Module準則  例子:如何對電商平台上的顧客進行模塊設計  對於顧客來說，一般須要維護顧客的   個人訊息  收穫地址  付款方法  這三個之間的關係是緊密相關，不可獨立存在，我們根據這三點抽象出三個Aggregate   Customer 個人訊息  AddressBook 收穫地址  Wallet 付款方法  那該如何去放置這些Aggregate，是針對每一個Aggregate作資料夾分類還是這三個Aggregate放同一格資料夾?基本上這三個Aggregate就是一個Custer Module，所以都會放到Custer Module資料夾內。    當整理出Aggregate與Module後，接著會開始根據各Module去實作事件應用處理  基本上我們在DDD模塊的設計上有幾個注意要點   Module應該要和Domain概念一致:一般一組聚合集成(領域)，我們會相對應建立一個Module。  根據通用語言來命名:模組命名要一眼就看出這是在做什麼的。  模組設計盡量鬆偶合:盡量與其他模組不要有太多的偶合，若有也許在領域設計上還沒切得很乾淨。  如果有PeerModule或父子Module出現，盡量避免循環相依。  4.關於Module命名    5.Module界線與限界上下文不同  為了對領域模型中進行準確建模，需要將領域模型劃分成多個子域，每個子域對應一個或多個限界區域。 模塊。所以，從子域到限界某些再到模塊，應該是依次包含關係。  補充  補充一   Abstracion:\n   將真實世界物體與事件的大量資訊縮減一個概念或是一個現象的資訊含量來將其廣義化，保存和一特定目的有關的資訊。例如，將一個皮製的足球抽象化成一個球，只保留一般球的屬性(形狀)和行為(滾)等資訊。  Leaky Abstractions\n   所有非不證自明的抽象概念，都有某種程度的疏漏。例如TCP雖簡化(抽象化)網路行為，設計上也保證網路傳送過程中不遺漏資訊，但不保證就真的能完整傳到資訊，例如我們無法避開海底電纜被魚咬斷因此斷訊的狀況。  參考   範例  The 5 Essential Elements of Modular Software Design  The Law of Leaky Abstractions  The Three Principles of Excellent API Design  解析Python模組(Module)和套件(Package)的概念  Module Design   Domain Events vs. Integration Events in Domain-Driven Design and microservices architectures   DDD理论学习系列（13）-- 模块   DDD理论学习系列——案例及目录"},{"id":"content:1.architecture:2.DDD-簡易整理.md","path":"/architecture/ddd","dir":"architecture","title":"DDD實戰-簡易整理","description":"","keywords":["一、回顧","二、何謂Entity,如何定義他","三、跟ValueObject有何不同","四、如何產出 Entity Id?","五、.NET Core 實作微服務領域模型"],"body":"  DDD實戰-簡易整理  一、回顧  (DDD) 重點在於協助您在使用案例相關的商務實際情況下建立模型，然後根據Domain定義後續不同的Context與彼此的對應關係，再與事件驅動方式實際實現。  書中所提例子 : 電商系統，人員瀏覽商並下訂後交易。   定義Domain\n   根據問題空間與解決方法定義出Domain\n   Core Domain :產品最有價值部分 (Ex AI 推薦購買商品需求)  Supporting Subdomain : 未提供核心競爭力，但支援核心所需功能 (Ex 購物需求)  Generic Subdomain : 未提供核心競爭力，但整個系統都可能會用到它 (Ex 身份認證需求、金流串接)      根據語意(Linguistic)與業務能力(Business Capability)定義BoundContext   重點一、通常識別 Bounded Context 會由兩點下手：語意(Linguistic)與業務能力(Business Capability)。  電商例子(語意-業務能力)   登入-帳號管理 => 身分管理Context (Identity)  商品-商品選擇 => 商品目錄Context (Catalog)  下購-購買功能 => 選購Context (Purchase)  重點二、注重業務能力勝過資料分類 (習慣性地用資料表去起始設計系統，甚至把業務邏輯與 ORM 框架綁在一起。這麼一來容易造成物件乘載太多的責任，比如說「顧客」是屬於「會員管理系統」還是屬於「購物系統」？)  Context定義出後，根據幾種方法(設計模式，或撰寫程式技巧)去做Context Mapping    - Shared Kernel\n - Partnership\n - Anti-corruption Layer\n - Open Host Service/Published Language\n - Separate Way\n - Big Ball of Mud\n - Customer-Supplier\n - Conformist\n     使用或設計軟體架構最小化建置與維護「需求系統」所需要的人力資源。\n   軟體的架構與功能需求沒有關係  軟體架構是非功性需球 Non-Funcitonal Requirement (系統達成的任務的能力)  常見軟體架構大概有這些類型：\n   MVC  MVP  Layered Architecture  Client Server  Microservice  Event-Driven Architecture  Pipe-Filter  MVVM  DDD 不等於 Clean Architecture，兩者關注的面向不同。DDD 的主要目的是將軟體的模型更貼近業務需求，架構只是為了達到目的的工具。     二、何謂Entity,如何定義他  根據電商前面例子, 身分管理、 商品目錄與選購中，你覺得什麼是Entity?  顧客、訂單、商品等等。這些物件不被他們的屬性所辨識(比如年齡、金額)，而是由一個專屬的身份標誌 (Identity)來辨識。這種時候，我們就需要 Entity 的幫助讓我們在不同的物件中找到我們要的那一個。  Entity 最大的特徵就是有 Identity 的概念，所以常會搭配一個擁有唯一值的 ID 欄位。但這邊要澄清一個誤解，不是有 ID 就是 Entity，重點是你在不在乎他生命週期的變化。  Entity具有幾個特徵   具有唯一值ID  具有狀態  生命週期有可能無限長  一個 Entity 是可變的、長壽的，所以通常會有複雜的生命週期變化，如一套 CRUD 的操作  不只會實作資料屬性，還會實作具有相關領域邏輯的作業或方法  實體代表領域物件，而且主要是由其身分識別、連續性及一段時間的持續性所定義，而不只是由包含這些項目的屬性所定義。 如同 Eric Evans 說，「主要由其身分識別定義的物件稱為「實體」（Entity）。 實體在領域模型中很重要，因為它們是模型的基礎。  已Order訂單為例子   訂單ID  訂單屬性(ID,Name,Address)  訂單操作Method(EditName, EditAddress)    三、跟ValueObject有何不同   當一個物件沒有概念上的標識 (conceptual identity)，而你只關心它的屬性時，這個物件就可以建立成 Value Object。  Value Object 的屬性都是為了要描述某一個事物的特徵。  判斷這兩者的標準就在於系統在不在乎這個物件的生命週期變化。      四、如何產出 Entity Id?   1. 來自用戶的輸入  這是一個非常直接的做法，比如使用用戶的 email 或是身分證字號等等作為 ID，但也容易造成額外的成本。最大的成本就在於，你需要由用戶負責產生符合需求的身份認證資料非常困難。此時的 ID 可能是唯一的，但卻有可能是不正確的。  甚至，身分證字號也有重複的可能性。  因此，我們可以將用戶輸入的資料作為 Entity 的屬性。這些屬性可以用來做搜尋用，但大多時候並不適合作為 Entity 的 ID。   2. 使用持久化機制來產生  最常見的就是使用資料庫自動生成 ID，最常見的就是 SQL 對 ID 下 AUTO_INCREMENT 讓 ID 的值自動遞增。又或者也可以向資料庫索取一個 UUID (或 GUID) 作為 ID 的值。  這樣的做法好處是可以減少程式的複雜性，直接把產生的工作交給持久化機制處理。但也容易招致效能問題的疑慮(UUID/GUID 的產生)。而且當你無法從程式碼找出 ID 的生產機制時，也會增加程式碼的隱含性不利於閱讀。  另外，使用持久化機制時，也需要特別考量這個 ID 的生成應該要在該物件持久化 (ie 存入資料庫) 之前或是之後，以配合程式的需求。  註：這裡會使用「持久化」一詞是因為儲存資料的方式不止資料庫一種，故用更通稱的方式描述。   3. 在程式中產生  在程式中產生 ID 是最常見的方法之一，這種方法好處是可以更容易掌握生產的時機，此外，更可以客製化你的 ID 格式，比如一筆訂單你可以用 order-20190930-c764e787-8182 作為 ID，如此一來，在 debug 時就不用被一堆天文數字般的 ID 搞得昏頭脹腦。所以以個人經驗來說，即使增加了一點複雜度，會最推薦這個方式。   4. 由另一個 Bounded Context 提供  最複雜的一種就是來自於另一個 Bounded Context 提供的 ID。這種可能出現在當你需要調用 API 的時候，得到對方的資料後存取下來。這種方式的複雜點在於，你不只要考慮本地端的 Entity，也需要考慮外部 Bounded Context 的改變情況，雖然可以透過訂閱另一個 Bounded Context 的方式做到，但仍舊十分麻煩。  五、.NET Core 實作微服務領域模型      參考 :\n  https://ithelp.ithome.com.tw/articles/10223150  https://docs.microsoft.com/zh-tw/dotnet/architecture/microservices/microservice-ddd-cqrs-patterns/microservice-domain-model  https://docs.microsoft.com/zh-tw/dotnet/architecture/microservices/microservice-ddd-cqrs-patterns/ddd-oriented-microservice"},{"id":"content:2.desktop:1.WindowsForm找不到類型xxxx上的建構涵式.md","path":"/desktop/windowsformxxxx","dir":"desktop","title":"WindowsForm找不到類型xxxx上的建構涵式","description":"","keywords":["情境","錯誤訊息","解決方法"],"body":"  WindowsForm找不到類型xxxx上的建構涵式  情境  近期在設計DeskTop頁面時有遇到幾個頁面基底邏輯相同的狀況，於是特別設置Base Page去讓UC繼承使用。因為頁面其實長差不多，所以最後決定不使用參考引用而直接使用繼承。  讓A(UC_3Dswitch_CalibrationFileManagement)繼承B(UC_3Dswitch_FileManagementBase)。  錯誤訊息  編譯上都沒有問題，但此時再使用Design模式時，發生找不到類型錯誤如下    解決方法    宣告無注入空的建構子，       // 宣告無注入空的建構子\n   public     UC_3Dswitch_FileManagementBase  ()\n   {\n     }\n   public     UC_3Dswitch_FileManagementBase  (  AppSetting     appSetting  ) \n   {\n       ProductLineDataPath   =   appSetting.ProductLineDataPath;\n       SNFolderNameLength   =   appSetting.SNFolderNameLength;\n         InitializeComponent  ();\n   }  因原先Base設定注入所宣告AppSetting物件，故發生上述無法載入錯誤。看起來Deisnger模式在Control物件使用上Defaul都是預設空的建構子設置。  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-086898{color:#6A737D;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}"},{"id":"content:3.cloud-gcp:GKE.md","path":"/cloud-gcp/gke","dir":"cloud-gcp","title":"GKE","description":"GKE為GCP提供的SAAS等級的kubernetes服務。相對於自行架設kubernetes運行環境簡化了許多建置部屬、管理及擴展設定的工作。以SAAS提供kubernetes容器平台服務的優點如下","keywords":[],"body":"  GKE  GKE為GCP提供的SAAS等級的kubernetes服務。相對於自行架設kubernetes運行環境簡化了許多建置部屬、管理及擴展設定的工作。以SAAS提供kubernetes容器平台服務的優點如下   容易上手 : Google對kubernetesr進行封裝及抽象，使用者不需要了解底層細節即可上手。能直接透過介面操作達到快速部屬與管理設定。不然一般許多部份都需要自己做相關起建，例如..\n   介面需安裝相對應的工具並做設定才可  版本升級修補需人為去維護  需自行安裝與配置監控與日誌工具  安全性設定  資源設定擴展需自行維護"},{"id":"content:4.database:1.MSSQL使用指令測試Server硬碟速度小技巧.md","path":"/database/mssqlserver","dir":"database","title":"MSSQL使用指令測試Server硬碟速度小技巧","description":"","keywords":["讀取速度","寫入速度"],"body":"  MSSQL使用指令測試Server硬碟速度小技巧  讀取速度  選抽一個資料庫 下BACKUP DATABASE指令，備分資料庫不做寫入，只做讀取，可得到讀取速度值。     BACKUP     DATABASE   [FUXIN_CPL]   TO     DISK     =  'NULL'     WITH     COPY_ONLY    下圖可看到結果每秒讀取速度為180MB/sec    寫入速度     BACKUP     DATABASE   [FUXIN_CPL]   TO     DISK     =  'C:\\TEST.BAK'     WITH     COPY_ONLY  此時會得到讀寫時間為每秒153MB    每秒153MB為讀寫時間，此時須作運算處裡將寫入時間算出  讀取總頁數共688頁，一頁8k => 688*8.0 / 1024 = 5.375M  寫入時間為 0.035-0.03 = 0.005  5.375M/0.005 = 1075M  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}"},{"id":"content:4.database:2.EF Query追蹤議題.md","path":"/database/ef-query","dir":"database","title":"大批量Create處理時間議題與Query追蹤議題","description":"","keywords":["一、大批量Create處理時間議題","二、Query追蹤議題"],"body":"  大批量Create處理時間議題與Query追蹤議題  一、大批量Create處理時間議題  a.情境  近期在使用EF新增資料時遇到一次要插入大量資料時的需求(一次約快1w5千筆)。因為EF Bulk Insert要錢(第三方套件)，顧很單純使用DbContext Add，插入時間要將近30秒。   Root Casue  EF POCO在做異動時，會呼叫DetectChanges()比對所有entry集合中每一個Property的新就值，顧會造成在大量操作時拉長時間。以下為會呼叫DetectChanges()的方法   DbSet.Find  DbSet.Local  DbSet.Add  DbSet.AddRange  DbSet.Remove  DbSet.RemoveRange  DbSet.Attach  DbContext.SaveChanges  DbCoNtext.GetValidationErrors  DbContext.Entry  DbChangeTracker.Entries  b.解法  根據上述原因，若要縮短大量資料建立時間有兩種方法   使用AddRange\n   直接使用AddRange一次性加入所有資料，會比起每次Add都要呼叫一次DetectChanges()時間來的短。  Configuration.AutoDetectChangesEnabled屬性設置成false\n   直接將AutoDetectChangesEnabled設成false，Add完後再設為true。  c.測試結果  實測結果如下，12544筆資料存取時間   Add : 34s  AddRange : 13s  AutoDetectChangesEnabled fasle : 13s  二、  Query追蹤議題  For處理大批量Create處理時間議題也稍微查一下Query部分，發現EF查詢預設有Tracking的設計。稍微寫一下Testing Code如下    操作步驟為   Step1: Context1 索取Mario資料  Step2: Context2 新增一筆Test資料  Step3: Context1 修改Mario資料為Jack  結果如下    可以發現Context1查覺到筆數有增加，但Context2獨到的Name能然為舊值。  因為Context2拿取了Cache資料內容，第二次Query並沒從DB讀取。所以讀到的值仍為舊值。此時如果我們將Context2第二查詢改為     dbContext2.TestRecord.AsNoTracking().SingleOrDefault(x => x.Id == 1).Name  此時讀到的值則就會是Context1改過的Name值Jack，因此如果在唯讀的使用情境下，使用不追蹤查詢可以加快查詢速度。(待實測)  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}"},{"id":"content:5.keycloak:1.身份驗證與授權與Keycloak.md","path":"/keycloak/keycloak","dir":"keycloak","title":"身份驗證與授權與Keycloak","description":"","keywords":["一、關於身份驗證與授權"],"body":"  身份驗證與授權與Keycloak  一、關於身份驗證與授權  身份驗證和授權是系統安全性非常重要的環節。身份驗證用於識別使用者是誰，而授權則賦予使用者某些特定權限。更具體來說，這整個過程可以分為四個部分：   身分識別 (Identification)：這是一個讓系統知道你是誰的過程。例如，當你使用用戶名或電子郵件地址登入系統時，就是進行身分識別。  身分驗證 (Authentication)：這個過程讓系統確認你確實是你聲稱的那個人。通常是通過輸入密碼、使用FaceID或OTP來完成的。  授權 (Authorization)：這涉及到角色分配。根據你的角色，系統會賦予你不同的權限。例如，一個“編輯者”角色可能有編輯內容的權限，而一個“閱讀者”角色則只能閱讀。  存取控制 (Access Control)：這涉及到具體的操作權限。比如，在一個IT管理系統中，一個普通使用者可能可以重啟伺服器和查看系統日誌，但不能部署新的程式碼。然而，一個開發者則可能有這樣的權限。  在你登入系統，輸入帳號密碼為身分識別與身分驗證，系統驗證完後，會根據身分授予角色。至於此角色權限則可以在後台系統上設置。至於這部分的詳細實做概念牽扯還是蠻多的...會再找時間針對這部分做一個細部講解。  1. 身分識別驗證與授權簡易實作  了解這些基礎概念後，我們可以考慮如何手動實現這四個部分。   身分識別 (Identification) : 最常見的實現方式是透過一個使用者註冊頁面，讓使用者輸入基本資料，例如用戶名和密碼。這些信息會被存儲在後端的資料庫中（密碼會被加密）。  身分驗證 (Authentication) : 驗證的方法有多種。   密碼驗證 : 簡單地說，就是將輸入的密碼與資料庫中存儲的密碼進行比對。  多因素驗證 : 二次驗證，例如OTP、FaceID，或是手機&Mail驗證  Session/Token管理 : 用戶登入後，系統會生成一個session或token並發送給用戶。後續的所有請求都需要這個token以確認身份。  授權 (Authorization) : 這部分簡易實作基本上會有三部份   角色管理：在資料庫中設計一個角色和權限的模型。例如，每個使用者可以有一個或多個角色，每個角色有不同的權限。  權限檢查：每次使用者請求某個資源或操作時，檢查他們的角色是否有相應的權限。  API設計：設計API時，確保每個API端點都有適當的授權檢查。  存取控制 (Access Control)   基於角色的存取控制 (RBAC)：根據使用者的角色決定他們可以訪問的資源。  細緻的權限設定：允許系統管理者為每個角色定制細緻的權限，例如某角色只能讀取資料但不能編輯。  其他考慮   日誌和監控：記錄所有的登錄嘗試、授權請求等，以便日後分析和審計。  資料庫安全性：確保資料庫有適當的加密和備份策略。  定期檢查和更新：隨著時間的推移，可能會出現新的安全威脅。定期檢查和更新你的身份驗證和授權策略，以確保它們始終是安全的。  2. 身分識別驗證、授權與Keycloak  通過使用Keycloak，我們能夠更為高效地實現身分識別、身分驗證、授權，以及存取控制等功能。   身分識別 (Identification)   不僅提供使用者註冊功能，讓使用者可以用基本資訊，比如用戶名或電子郵件進行註冊，還支持多種社交登入方式，如Google或Facebook。  身分驗證 (Authentication   支援多種身分驗證方法，包括密碼、OTP、FaceID等，也提供Token管理，當使用者成功登入後，Keycloak 會發放一個 token，使用者可以使用此 token 來存取其他受保護的資源。  授權 (Authorization)   可以定義多個角色，並為每個角色分配不同的權限，使用者可以被分配到一個或多個角色，這些角色決定了使用者可以訪問哪些資源。  存取控制 (Access Control)   支援基於角色的存取控制 (RBAC)。你可以設定哪些角色可以訪問哪些資源。  一般來說，要全面實施這四大功能通常需要大量的時間和資源。開發者不只需要寫大量的程式碼，還必須維護系統的安全性、效能，並確保與其他系統的良好整合。有了Keycloak，這一切都變得相對簡單。"},{"id":"content:5.keycloak:2.OIDC與SAML.md","path":"/keycloak/oidcsaml","dir":"keycloak","title":"OIDC vs SAML","description":"在這部分，我們將探討OIDC和SAML。正如前一章節所提，身分驗證和授權是整個安全流程中非常關鍵的環節。一般來說，會有專門的解決方案來處理這些問題。OIDC和SAML都是為這個目的而設計的標準協議，它們提供一個集中式的方法來驗證使用者身份，並界定他們可以訪問哪些資源或執行哪些操作。","keywords":["1. Open ID Connect (OIDC)："],"body":"  OIDC vs SAML  在這部分，我們將探討OIDC和SAML。正如前一章節所提，身分驗證和授權是整個安全流程中非常關鍵的環節。一般來說，會有專門的解決方案來處理這些問題。OIDC和SAML都是為這個目的而設計的標準協議，它們提供一個集中式的方法來驗證使用者身份，並界定他們可以訪問哪些資源或執行哪些操作。  1. Open ID Connect (OIDC)：  OIDC是一個建立在OAuth 2.0之上的身分認證層。OAuth 2.0本身是一個專注於授權的框架，而OIDC則在這個基礎上增加了身分驗證功能。這樣，應用程式不僅能知道使用者有哪些權限，還能瞭解使用者是誰，並獲取他們的基本資訊，比如名稱和電子郵件地址。。  a. OAuth 2.0  OAuth 2.0 是一個授權框架，允許第三方應用程式在使用者同意的情況下存取使用者在某個服務上的資訊，而不需要分享使用者的密碼。通常在OAuth 2.0，會有幾個角色，我們這邊舉一個簡單情境，你希望使用「快速日記」App，而這個App提供使用Google帳戶登入的功能來使用Google雲端硬碟服務。   Resource Owner(資源擁有者) : 通常就是User(你)， 能授予應用程式取得受保護資料的人，通常就是終端使用者（end-user）。例如你希望使用「快速日記」App，而這個App提供使用Google帳戶登入的功能。在OAuth的流程中，當App請求許可存取你的資料時，你會給予（或拒絕）這個請求。  Resource Server(Resource Server) : 存放使用者受保護資料的伺服器，以這個例子來說就是Google雲端硬碟，當「快速日記」App希望保存或讀取日記時，它會向此伺服器提出請求。  Client (客戶端)：通常指稱想要取得受保護資源的「應用程式」，以這個例子來說就是「快速日記」App。 當「快速日記」App希望保存或讀取日記時，它會向此伺服器提出請求。  Authorization Server (授權伺服器) : 驗證 Resource Owner 的身份，並且在獲得同意之後，發放「Access Token」給應用程式（Client）的伺服器。以這個例子來說就是 (Google的授權伺服器)。  下圖整個驗證Flow  \n   驗證Flow     Client 到 Resource Owner :   Request Credentials : 當你打開「快速日記」App並選擇使用Google帳戶登入時，App首先會引導你到Google的登入頁面。  Authenticate : 你將在Google的頁面上輸入你的Google帳戶憑證，即用戶名和密碼。這一步是由Google完成的，而「快速日記」App不會看到或知道你的密碼。  Consent : 一旦驗證成功，Google會顯示一個請求同意頁面。在這裡，Google會詢問你是否允許「快速日記」App訪問特定的Google帳戶資料。  Credentials : 「Resource Owner」（使用者）提供的身份資訊或某種用於辨識其身份的資料。這只是一個授權請求，而實際的身份驗證會在Resource Owner和Authorization Server之間完成。    Client 到 Authorization Server   Authorization Request : 如果你同意上述的權限請求，「快速日記」App會從Google的授權伺服器請求一個授權碼。  Authorization Code : Google的授權伺服器會回傳一個短暫的授權碼給「快速日記」App。  Access Token : ，「快速日記」App會使用這個授權碼再次向Google的授權伺服器請求取得訪問令牌（Access Token）。    Client 到 Resource Server   Access Token: 一旦取得訪問令牌，「快速日記」App便可以使用此令牌來存取Google雲端硬碟（或其他你同意的資料）。  Protected Resource: 當「快速日記」App希望保存或讀取日記時，它會使用這個Access Token向Google雲端硬碟（作為資源伺服器）提出請求，然後Google雲端硬碟會根據該令牌提供相對應的資料或服務。  更詳細的其實還有關係到Redirect部分，可以參照這篇   https://cloudsundial.com/salesforce-oauth-flows  寫得還算詳細。  "},{"id":"content:5.keycloak:3.關於SSO.md","path":"/keycloak/sso","dir":"keycloak","title":"關於SSO","description":"","keywords":["三、SSO 簡述"],"body":"  三、SSO 簡述  OIDC和SAML討論完後，我們來聊聊單一登入（SSO）。SSO是一個身份驗證方案，它讓使用者能透過一次登入就可訪問多個應用程式和網站。簡而言之：   透過單一的登入窗口，進行單一的身分驗證，就可以讓許多的服務共同來使用這個驗證結果  OIDC和SAML都可以實現這樣的SSO功能。舉例來說，假設你在工作中要使用三個不同的平台：   郵件系統（Email System）  公司內部網站（Intranet）  報表和數據分析平台（Analytics Platform）  通常你得記住這三個系統各自的帳號和密碼。但有了SSO，你只需透過一個統一的登入界面（比如由Keycloak管理）登入一次，然後就能自由地訪問這三個不同的平台，無需再逐一輸入帳號和密碼。  總之，SSO幫你將多個獨立的系統集成為單一的入口點。這樣不僅減少了你需要記住多組密碼的困擾，還降低了由於多個系統各自存儧行密碼所帶來的安全風險。  流程範例大致如下：   Step1(箭頭1) : 用者首先訪問「Application 01」的 URL 並按下登入按鈕，然後會被引導到 Keycloak 登入頁面。  Step2: 用戶在 Keycloak 成功登入後，將被重新導向回「Application 01」的主頁。  Step3(箭頭3): 如果用戶在合理的時間內再次訪問「Application 02」，則不需要重新登入。  箭頭2實際上是指示用戶從「應用程式01」被引導到Keycloak的登入頁面這一過程。簡單來說，當用戶嘗試在「應用程式01」登入（箭頭1）後，他們會被重定向到Keycloak以完成身份驗證（箭頭2）。  "},{"id":"content:5.keycloak:4.Keycloak Introduce.md","path":"/keycloak/keycloak-introduce","dir":"keycloak","title":"Keycloak Introduce","description":"我們剛才談到實現SSO（單一登入）的功能，那麼現在問題來了：有沒有一個工具或平台能讓我們更方便地實現這一切呢？答案是有的，那就是Keycloak。Keycloak是一個集成了多種身份驗證和授權機制（包括OIDC和SAML）的開源身份和訪問管理解決方案。","keywords":["2. Keycloak 細部解析"],"body":"  Keycloak Introduce  我們剛才談到實現SSO（單一登入）的功能，那麼現在問題來了：有沒有一個工具或平台能讓我們更方便地實現這一切呢？答案是有的，那就是Keycloak。Keycloak是一個集成了多種身份驗證和授權機制（包括OIDC和SAML）的開源身份和訪問管理解決方案。  Keycloak的特點如下   多協議支持: Keycloak支持OIDC和SAML，所以你不必為了不同的應用而選擇不同的解決方案。  易於管理: Keycloak有一個使用者友善的管理界面，你可以輕鬆設定用戶、角色和權限。  靈活性: 它是開源的，意味著你可以根據自己的需要對它進行定制。  詳細提供以下功能   身分驗證(Authentication)   單點登入/登出（Single Sign-On/Single Sign-Out）: 讓使用者只需登入一次，就能訪問多個不同的應用和服務。  多因素認證（Multi-Factor Authentication, MFA）: 除了密碼外，還可以透過SMS、郵件或其他方法進行身份驗證。  支持外部身分源: AD，LDAP，Social Login(Google, FB...)。  使用者管理（User Management）   使用者身分 CRUD（Create, Read, Update, Delete）: 簡單地管理使用者資訊，包括建立、查詢、更新和刪除。  屬性管理（Attribute Management）: 可以給使用者賬戶添加多種屬性和標籤。  使用者分組（User Grouping）: 組織使用者到不同的群組以方便管理。  授權（Authorization）   Role-Based Access Control(RBA）: 基於角色給予使用者不同的訪問權限。  Attribute-Based Access Control(ABAC）: 根據使用者的特定屬性（如年齡、部門等）來給予權限。  User-based Access Control (UBAC) : 直接對個別用戶賦予權限，而不是通過角色或屬性。這在只有少數用戶需要訪問特定資源的情況下特別有用。  Context-based Access Control (CBAC) : 更動態的授權方式，考慮到目前的情境或環境狀況（如目前正在執行的操作，或者資源的當前狀態）來做出授權決策。  Rule-based Access Control (Using JavaScript) : 使用 JavaScript 程式碼來定義特定的授權規則。這是一個非常靈活的方式，可以根據極其特定的需求來制定授權策略。  Time-based Access Control : 依據時間來決定是否允許訪問，例如只有工作時間允許訪問某個資源。  Support for custom Access Control Mechanism (ACMs) through a Service Provider Interface (SPI) : 高度定制的選項，允許你通過 Service Provider Interface (SPI) 來實現自己的授權機制。這對於需要非常特殊授權邏輯的場景來說是一個非常強大的工具。  安全性（Security）   密碼政策（Password Policies）: 可以設定密碼的複雜度、有效期等。  會話管理（Session Management）: 查看和管理當前活躍的使用者會話。  應用安全（Client Security）: 對接入的客戶端進行安全設定和驗證。  其他   事件監控（Events Monitoring）: 監控和記錄關於認證、授權等的事件。  擴展性（Extensibility）: 支持自定義插件和腳本，以擴展基礎功能。  2. Keycloak 細部解析  a. Keycloak Core  為了充分利用Keycloak，並根據我們的需求進行客製化，我們必須了解其核心組件以及它們是如何互動的。以下為Keycloak Core Block圖     Realm:master：可想像成一個隔離的命名空間，裡面有你的使用者、角色、客戶端等資料。你可以有多個Realm，每個Realm都有其獨立的設定。\n   Client：代表需要與Keycloak進行互動的應用程式或服務。像是網頁應用或API。這些客戶端會使用Keycloak進行身份驗證。  Roles：確定使用者在Client中可以執行哪些操作。  Security Defense：確保Realm的安全性，例如對抗暴力攻擊或強化密碼政策。  User Federation：允許你把外部的使用者來源（如LDAP）連接到Keycloak。  Roles：通常用於定義訪問權限。例如，管理員角色可能允許使用者更改系統設定。  Groups：一組使用者的集合，有助於管理與分配角色。  Events：記錄Keycloak的所有活動，如誰何時登錄或更改設定。  Users：這是指註冊到Realm的使用者。他們可以有不同的角色或屬於不同的群組。  Identity Provider (IdP)：確認使用者身份的部分，允許使用者用像是Google或Facebook這類的外部服務進行登錄。  核心和外部區塊的互動部分。例如，當一個使用者使用Twitter賬號登錄時，Twitter會和Keycloak中的Identity Provider互動，然後IdP再與Realm互動，確認使用者的身份並賦予他適當的角色和權限。  另外這邊稍微提一下Realm裡面的Roles跟外部的Roles有什麼不同   Realm裡面的Roles (Realm-level Roles)：這些角色是直接相對於Realm本身的。一旦你在Realm中定義了一個角色，你就可以將它指派給任何Realm內的使用者。它們通常是更通用的，例如“管理員”或“使用者”，並且可以跨多個客戶端使用。  外部的Roles (Client-level Roles)：這些角色是相對於特定的客戶端（應用程式或服務）的。所以它們是在特定客戶端的上下文中定義和使用的。允許你為每個應用程式定制更具體的角色。例如，你可能有一個\"編輯\"角色在你的CMS系統中，而有一個\"購物者\"角色在你的電商網站中。  簡單來說，Realm內的角色是全域性的，可以在整個Realm中使用，而Client-level角色是特定於某個應用程式的。  b. Kyecloak 角色  使用Keycloak服務時，裡面有幾種角色必須釐清他們的關係   Realm : 每一個Realm在Keycloak中都代表了一個獨特的命名空間或領域。在同一個Keycloak實例中，不同的Realm之間的資料和設定是完全隔離的。例如，行銷部門和研發部門可能有不同的應用程式和使用者，因此他們可以在不同的Realm中被管理。某種程度有點像Project概念。   設定 : 每個Realm都有自己獨特的設定，包括但不限於認證策略、令牌生命週期、SMTP設定等。這意味著你可以為不同的組織或專案客製化其身份和訪問管理策略。  使用者和客戶端 : 每個Realm內都有其專屬的Users和Clients。例如，兩個不同的Realm之間的Users是不可以交互認證的。  事件和審計 : 可以為每個Realm單獨配置事件和審計策略，以追踪和記錄Realm內的活動。  Clients : 通常代表你想要與Keycloak整合的應用程式或服務。定義了如何與那些應用程式或服務進行交互，包括認證方法、回調URL等。例如有一個Web應用程式和一個手機應用程式，兩者都需要身份驗證。在這種情況下，你可以為每個應用程式設定一個Client，並為它們設定不同的認證流程或訪問限制。   設定 : Clients的設定包括如何與其進行認證的具體細節，例如回調URL、認證方法、封裝方法等。包含協議，可能是OpenID Connect、SAML 2.0。  角色 : 你可以在每個Client裡設置特定的角色，這些角色可以賦予給使用者，以決定他們在該Client中可以進行哪些操作。  Users : Users基本上是真實的個體，如員工或客戶。   設定 : 在Keycloak中，User的憑證（如密碼）是存儲在User的設定中。但是，具體的認證流程（例如，如何驗證這些憑證）是由Client來定義的。  與Client的關係 : Users在Clients中獲得訪問令牌，這些令牌決定了他們在該Client中可以進行哪些操作。例如，一個User可能在一個Client中具有\"讀者\"的角色，在另一個Client中具有\"管理員\"的角色。  Groups : Groups代表了一種組織層次結構，可以將Users組合在一起。這允許管理者更容易地管理大量使用者和其訪問權限。   設定 : 跟Users一樣，Groups也可以有其自己的屬性和設定。例如，你可以為某個Group設定一個特定的屬性，然後所有屬於該Group的Users都可以看到或使用這個屬性。  角色賦予 : 可以將角色賦予給一個Group，然後所有屬於那個Group的Users都會繼承這些角色。  靈活性 : 一個User可以同時屬於多個Group。例如，一個User可能同時是\"研發團隊\"和\"高級工程師\"這兩個Groups的成員。  Role : 角色是一種用於表示使用者或群組所擁有的權限的方式。換句話說，角色定義了使用者或群組可以執行哪些操作或訪問哪些資源。例如，您可能有一個\"管理員\"角色，該角色允許使用者訪問和修改所有資源，而\"編輯者\"角色則可能只允許使用者修改，但不能刪除資源。   設定 : 在Keycloak中，角色可以分配給單個使用者或群組。當使用者試圖訪問某個資源或執行某個操作時，系統會檢查他們所分配的角色是否具有所需的權限。  角色的類型：Realm比較偏向Keycloak設定管理權限，客戶端角色比較偏向客戶端API使用權限。  Realm角色：可分配給Realm中的任何使用者或群組。例如，Realm範疇的\"管理員\"角色允許使用者管理整個Realm的設定。  客戶端角色：只能分配給特定客戶端的使用者或群組。例如，一個\"編輯器\"角色在\"新聞應用程式\"客戶端可能意味著使用者可以發布新的新聞文章，但在另一個\"帳單系統\"客戶端，相同的\"編輯器\"角色可能有完全不同的權限。  最後稍微用圖舉個他們關係的例子     Client：在此領域中的應用程式或服務，用戶會透過它進行認證。  Role 1, 2, 3, 4：這些是在該Realm中定義的角色。角色通常代表某些權限或能力。  Users：代表該Realm中的所有用戶。  Group 1, 2, 3… n：代表不同的用戶群組。每個群組可能有不同的權限和角色。  User 1：當此用戶登入時，被賦予了Role 1角色，並且是Group 1, Group 2, 和 Group 3的成員。 而 User 2 用戶登入時，被賦予了Role 2角色，並且是Group 2, Group 3, 和 Group 5的成員。最後User 3 用戶登入時，會被賦予了Role 4角色，並且是Group 2, Group 3, Group 4, 和 Group 5的成員。  可以看到Keycloak如何將角色和群組賦予用戶。這有助於管理哪些用戶可以訪問哪些資源，以及他們可以執行哪些操作。此外，通過將用戶分組，可以輕鬆地管理大量用戶的權限，而不必逐一配置。"},{"id":"content:50.Tool:1.SpecFlow Simple Usage.md","path":"/tool/specflow-simple-usage","dir":"tool","title":"SpecFlow Usage","description":"I need to share about TDD (Test-Driven Development) soon, and it reminded me of my notes from a BDD (Behavior-Driven Development) course I took. The course taught the concept of BDD and included some hands-on practice. However, for the C# implementation, it didn't utilize SpecFlow to edit DSL (Domain-Specific Language) syntax to map to the output code. So, I decided to give it a try on my own. There are quite a few articles about its usage online, but they mostly cover the simplest contexts. Coincidentally, the course provided an example that was a bit closer to real-world implementation for me to experiment with. I encountered some issues along the way, so I thought I'd jot down a brief record.","keywords":["BDD","Practice using SpecFlow  (Test Context)","Points to Note"],"body":"  SpecFlow Usage  I need to share about TDD (Test-Driven Development) soon, and it reminded me of my notes from a BDD (Behavior-Driven Development) course I took. The course taught the concept of BDD and included some hands-on practice. However, for the C# implementation, it didn't utilize SpecFlow to edit DSL (Domain-Specific Language) syntax to map to the output code. So, I decided to give it a try on my own. There are quite a few articles about its usage online, but they mostly cover the simplest contexts. Coincidentally, the course provided an example that was a bit closer to real-world implementation for me to experiment with. I encountered some issues along the way, so I thought I'd jot down a brief record.  BDD  Simple Concept  BDD focuses on the behavior and requirements of software throughout the development process. It emphasizes the use of natural language (close to human-understandable language) to describe the behavior that software should exhibit. This approach enables developers, testers, and non-technical stakeholders (such as product managers and business analysts) to clearly understand the requirements.  The difference between BDD and TDD  TDD  For TDD, or Test-Driven Development, let's say we're adding a feature to a calculator program to perform addition:   Write a test case to simulate the addition feature (for example, testing that 2 + 3 equals 5).  Write the actual addition functionality in the program.  Run the test.  If the test fails (which it likely will initially), refactor the code.  Run the test again. Continue the cycle of testing and refactoring until the test passes.  This TDD cycle ensures that the addition functionality works as expected before moving on to develop other features, ensuring each part of the program is tested and functional in isolation.  BDD  For BDD, or Behavior-Driven Development, the process focuses on the expected behavior of the addition functionality in a calculator program, described in natural language. Here’s how it might look:   Describe the expected behavior of the addition functionality in natural language. For example: \"When the user inputs two numbers and presses the 'add' button, the application should display the sum of these two numbers.\"  Write functional tests to check the addition feature, typically using a BDD testing framework (like Cucumber or SpecFlow) that allows behavior descriptions in natural language.  Implement the actual addition functionality in the program.  Execute the tests to ensure they pass, verifying that the program behaves as expected according to the described behaviors.  Refactor as needed while ensuring the tests still pass.  This BDD approach emphasizes clear communication and understanding among all stakeholders (developers, testers, and non-technical roles) about what the software is expected to do, ensuring the development aligns closely with user needs and expectations.  Simple Summary  The core idea of TDD, or Test-Driven Development, is to write tests before implementing features. Developers begin by writing test cases for a function or module, and then they write the corresponding code to make those test cases pass. The focus is on testing individual functions or methods. In contrast, BDD, or Behavior-Driven Development, focuses on the behavior of an entire feature or system. In BDD, test cases are closer to user requirements, which helps improve communication and understanding between the development team and non-technical stakeholders.  Practice using SpecFlow  (  Test Context )  Step1: Install SpecFlow   Install SpecFlow in Visual Studio IDE (navigate to Extensions and Updates to install, and remember to restart the IDE afterwards).  Create a new NUnit project.  For the test project, install SpecFlow.NUnit from NuGet (be mindful of the NuGet package name).  Step 2: Add a feature file to the test project, and edit it to include User Stories and Scenarios.  Add a feature file and edit the Feature section to include a User Story.   Feature: Add item to cart\n    In order to avoid the wrong total price\n    As a Customer\n    I want to get the current total price of items in the cart\n   A User Story is a type of scenario description that outlines a user's requirements for an application. It consists of three elements: Role, Feature, and Priority. For example, \"As a user, I want to be able to add products to my shopping cart\"; Priority indicates the importance of each feature, usually represented by a number, with 1 being the highest priority. This User Story template, composed of Role, Feature, and Priority, often follows the format \"As a (role), I want (feature), so that (benefit)\". This format helps teams better define and understand system requirements.   Scenario: Calculate the total price for cart items\n    Given there are cart items\n    When the customer completes the order\n    Then the total price should be the sum of the subtotal of all the cart items plus shipping fee NTD 60\n\nScenario: Alert customer when adding too many items\n    Given there are five items in the cart\n    When the customer tries to add one more item\n    Then the system should alert the customer not to do so and indicate which item cannot be added\n\nScenario: Alert customer when exceeding purchase limit\n    Given the customer has added a quantity of items to the cart\n    When the quantity of items exceeds the purchase limit\n    Then the system should alert the customer and indicate that the item has reached its purchase limit\n\nScenario: Apply free shipping for orders over 500\n    Given the customer selects the cart items\n    When the total price is over 500\n    Then the customer should receive free shipping\n  A Scenario is a descriptive statement used to outline the behavior of a specific feature. It often uses the \"Given-When-Then\" syntax to detail how software should behave under certain conditions. Scenarios serve as readable and understandable requirement specifications, helping developers and non-technical team members reach a consensus. Suppose we have a shopping website; we could write the following Scenario for the \"add a product to the shopping cart\" feature:   Scenario: 用戶將產品添加到購物車\n  Given 用戶已登錄購物網站\n  And 用戶瀏覽一個產品頁面\n  When 用戶點擊“添加到購物車”按鈕\n  Then 產品應該被添加到購物車中\n  And 購物車內的產品數量應該增加\n  Step 3: Generate the Test Code Framework  Right-click on the feature file and select \"Define Step.\" This action will automatically generate the test code framework for you, with names following the descriptions in the BDD Scenario.    Step 4: After completing the example, directly run the test.    Points to Note  Generating code results in anomalies.  When generating code using \"Define\" in a multi-scenario context, it's common to encounter issues where code for some scenarios isn't generated. The most frequent issue is missing code for certain scenarios. When a description fails to generate code successfully, it will turn purple as shown below.    Possible solutions include:   Rewrite the description. It's recommended to use ChatGPT 4.0 for assistance in generating descriptions. This can be especially helpful for those who are not proficient in English or unfamiliar with using SpecFlow, as it can be challenging to describe scenarios in a way that SpecFlow can recognize.  Right-click on the purple-highlighted sentences and choose \"Define.\" There is a \"Copy\" option to manually paste it. This method is the quickest.  Multi Scenario Object Initialization Using SharedContext Approach  If a Feature has multiple Scenarios and you need to initialize objects, you can use the SharedContext approach. The approach is demonstrated with the provided example scenario below.     // Declare the objects to be used with SharedContext.\n   public     class     SharedContext\n   {\n         public     Action     addToCart  ;\n         public     Cart     Cart   {   get  ;   set  ; }\n         public     CardItem     Erasier   {   get  ;   set  ; }\n         public     CardItem     Pencial   {   get  ;   set  ; }\n         public     CardItem     BluePen   {   get  ;   set  ; }\n         public     CardItem     Ruler   {   get  ;   set  ; }\n         public     CardItem     Notebook   {   get  ;   set  ; }\n         public     CardItem     PencilSharpener   {   get  ;   set  ; }\n   }     // Perform the following injection settings in the test object\n   private     readonly     SharedContext     _sharedContext  ;\n     public     AddItemToCartStepDefinitions  (  SharedContext     sharedContext  )\n   {\n       _sharedContext   =   sharedContext;\n   }\n     [  BeforeScenario  ]\n   public     void     Setup  ()\n   {\n       _sharedContext.Cart   =     new     Cart  ();\n       _sharedContext.Erasier   =     new     CardItem  (  name  :   \"Erasiers\"  ,   unitPrice  :   10  ,   maxPurchaseQty  :   10  );\n       _sharedContext.Pencial   =     new     CardItem  (  name  :   \"Pencial\"  ,   unitPrice  :   20  ,   maxPurchaseQty  :   10  );\n       _sharedContext.BluePen   =     new     CardItem  (  name  :   \"BluePen\"  ,   unitPrice  :   30  ,   maxPurchaseQty  :   10  );\n       _sharedContext.Ruler   =     new     CardItem  (  name  :   \"Ruler\"  ,   unitPrice  :   30  ,   maxPurchaseQty  :   10  );\n       _sharedContext.Notebook   =     new     CardItem  (  name  :   \"Notebook\"  ,   unitPrice  :   50  ,   maxPurchaseQty  :   5  );\n       _sharedContext.PencilSharpener   =     new     CardItem  (  name  :   \"PencilSharpener\"  ,   unitPrice  :   50  ,   maxPurchaseQty  :   5  );\n   }  Then, directly use the _sharedContext object in the test code.  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-086898{color:#6A737D;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}"},{"id":"content:51. ai:1.Generative AI 簡記.md","path":"/ai/generative-ai","dir":"ai","title":"Generative AI","description":"","keywords":["定義"],"body":"  Generative AI  定義   Artificial Intelligence (AI)   簡單闡述就是讓機器來實現智慧，定義範疇很大，目前沒有論文對他有很明確的定義。更明確來說他更像是實現目標，而不是定義。  Generative AI   機器產生的  複雜結構 物件，例如文章、影像與語音"},{"id":"content:100.ironman-gcp:Cloud.md","path":"/ironman-gcp/cloud","dir":"ironman-gcp","title":"Cloud","description":"","keywords":[],"body":""},{"id":"content:101.license:1.ACE.md","path":"/license/ace","dir":"license","title":"GCP Associate Cloud Engineer License","description":"","keywords":["考題與報名網址","題目類型分類","題目"],"body":"  GCP Associate Cloud Engineer License  考題與報名網址    題庫出處   模擬考題   註冊報名網址  題目類型分類  題目  以下為考題閱讀紀錄   Every employee of your company has a Google account. Your operational team needs to manage a large number of instances on Compute Engine. Each member of this team needs only administrative access to the servers. Your security team wants to ensure that the deployment of credentials is operationally efficient and must be able to determine who accessed a given instance. What should you do?   A.Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key in the metadata of each instance.\n\nB.Ask each member of the team to generate a new SSH key pair and to send you their public key.Use a configuration management tool to deploy those keys on each instance.\n\nC.Ask each member of the team to generate a new SSH key pair and to add the public key to their Google account. Grant the \"compute.osAdminLogin\" role to the Google group corresponding to this team.\n\nD.Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key as a project-wide public SSH key in your Cloud Platform project and allow project-wide public SSH keys on each instance.\n  \n    解題   題目問題，怎麼樣讓公司的運維團隊能夠有效且安全地管理大量的GCP（Google Cloud Platform）Compute Engine伺服器，同時還能追蹤到是誰存取了伺服器。   基本上這題考的是OS Login權限設置。OS Login功能是指在Google Cloud環境中，管理對虛擬機實例（VM）透過SSH（Secure Shell）進行安全登錄的一種方式。ref:  https://cloud.google.com/compute/docs/oslogin/set-up-oslogin?hl=zh-cn   解答會是C，要求每個團隊成員自己創建SSH金鑰對（就是一對密碼），並且把公開的那一半（公鑰）加到他們的Google帳號裡。接著給這個團隊的Google群組授予「compute.osAdminLogin」這個角色。(設置可以參考上面ref網址的  youtube )  其餘答案  A與D提到給予Private Key..這有安全疑慮。  B每個人有自己的Key，但要一台一台去設定，效率太低了!   You need to create a custom VPC with a single subnet. The subnet's range must be as large as possible.Which range should you use?   A.0.0.0.0/0\nB.10.0.0.0/8\nC.172.16.0.0/12\nD.192.168.0.0/16\n  \n    解題   題目問題詢問當你需要在Google Cloud Platform (GCP) 上創建一個自定義的虛擬私人網絡（VPC）並且只設置一個子網，你應該選擇哪一個IP範圍來確保這個子網的IP地址範圍最大？   此題考IP Range設定，會牽扯到對Classless Inter-Domain Routing，無類別域間路由的理解。當 CIDR 地址是 10.0.0.0/24 時，子網路遮罩位數為 24。這表示 IP 地址的前 24 位用於網路部分，後面的 8 位用於主機部分。   解答會是B，以CIDR邏輯來看，/後面數字越小，代表代表主機編號的數目越大，B的主機數目會是2^(32-8)。  其餘答案，0.0.0.0/0 代表所有可能的IP地址，不符合題目要的，其餘的就不用多加討論。   You want to select and configure a cost-effective solution for relational data on Google Cloud Platform. You are working with a small set of operational data in one geographic location. You need to support point-in-time recovery.\nWhat should you do?   A. Select Cloud SQL (MySQL). Verify that the enable binary logging option is selected.\nB. Select Cloud SQL (MySQL). Select the create failover replicas option.\nC. Select Cloud Spanner. Set up your instance with 2 nodes.\nD. Select Cloud Spanner. Set up your instance as multi-regional.\n  \n    解題   題目問題詢問如果想要為關聯式數據配置一個成本的方案，單區域並需要支持點時間恢復（Point-in-Time Recovery, PITR）的能力。你應該怎麼做?   考對數據存取方案的認知，基本上Cloud Spanner會比Cloud SQL來的貴。另外是對Cloud SQL的restoring是否有認知。ref:  https://cloud.google.com/sql/docs/mysql/backup-recovery/restore   解答會是A，Cloud SQL的Point-in-time recovery，可以透過binary logging來做Recovery。  其餘答案，Spanner基本上兩個答案都不符合成本，且沒有提到Point-in-time recovery開啟設定。   You want to configure autohealing for network load balancing for a group of Compute Engine instances that run in multiple zones, using the fewest possible steps. You need to configure re- creation of VMs if they are unresponsive after 3 attempts of 10 seconds each. What should you do?   A.Create an HTTP load balancer with a backend configuration that references an existing instance group. Set the health check to healthy (HTTP).\n\nB.Create an HTTP load balancer with a backend configuration that references an existing instance group. \nDefine a balancing mode and set the maximum RPS to 10.\n\nC.Create a managed instance group. Set the Autohealing health check to healthy (HTTP).\n\nD.Create a managed instance group. Verify that the autoscaling setting is on.\n  \n    解題   題目問題在於要為運行於多個區域的Compute Engine Instance群組配置網絡負載平衡的自動修復功能，並且希望使用盡可能少的步驟來配置如果Instance在3次嘗試、每次10秒的檢查後仍無回應，則重新建立VM。最合適的操作是   ref:   https://cloud.google.com/compute/docs/tutorials/high-availability-autohealing   解答會是C，建立一個instance group，並Configure a health check and an autohealing policy。  其餘答案，其他選項如創建HTTP負載均衡器並配置後端實例群組（A和B），雖然是配置負載均衡的重要步驟，但不直接涉及到自動修復不健康實例的配置。而確認自動擴展設置是否開啟（D）也是重要的，但它主要針對基於負載變化自動調整實例數量的情況，與自動修復不健康實例的需求不同。因此，對於配置自動修復功能的需求，選項C是最直接且有效的方法。   You are using multiple configurations for gcloud. You want to review the configured Kubernetes Engine cluster of an inactive configuration using the fewest possible steps.\nWhat should you do?   A.Use gcloud config configurations describe to review the output.\n\nB.Use gcloud config configurations activate and gcloud config list to review the output.\n\nC.Use kubectl config get-contexts to review the output.\n\nD.Use kubectl config use-context and kubectl config view to review the output.\n  \n    解題   題目問題在於要使用最少的步驟審查一個非活動配置的Kubernetes Engine集群   此題考對於GKE指令，gcloud config configurations專注於Google Cloud平台的資源管理，kubectl config專注於Kubernetes集群的操作。   解答投票率最高的是D，\n   kubectl config use-context 允許您切換到特定的Kubernetes上下文  kubectl config view 審查當前上下文（及相關配置）的詳細信息  其餘答案，gcloud config configurations為平台資源管理，而使用kubectl config選項C步驟不對   Your company uses Cloud Storage to store application backup files for disaster recovery purposes. You want to follow Google's recommended practices.\nWhich storage option should you use?   A.Multi-Regional Storage\nB.Regional Storage\nC.Nearline Storage\nD.Coldline Storage\n  \n    解題   題目問題在於用於災難恢復目的的應用備份文件，使用的Cloud Storage選項若要經濟又能滿足數據存取需求的選項。要選哪一個?  解答為D，Coldline Storage提供了一個成本效益高的解決方案，適用於需要長期存儲但僅偶爾訪問的數據，非常適合災難恢復的需求。  其餘答案，AB不是Storage選項，而Coldline Storage 如年度訪問一次，提供最低的存儲成本，但訪問成本高於Nearline Storage。以災難恢復來看，理想還是D   Several employees at your company have been creating projects with Cloud Platform and paying for it with their personal credit cards, which the company reimburses. The company wants to centralize all these projects under a single, new billing account.\nWhat should you do?   A.Contact cloud-billing@google.com with your bank account details and request a corporate billing account for your company.\n\nB.Create a ticket with Google Support and wait for their call to share your credit card details over the phone.\n\nC.In the Google Platform Console, go to the Resource Manage and move all projects to the root Organization.\n\nD.In the Google Cloud Platform Console, create a new billing account and set up a payment method.\n  \n    解題   題目問題在於該如何將公司內多個員工使用個人信用卡支付的Google Cloud Platform項目集中到一個新的統一賬單帳戶下  解答建議是D，創建一個新的賬單帳戶並設置支付方式。步驟如下     登錄到Google Cloud Platform控制台    尋找並進入「賬單」部分    選擇創建一個新的賬單帳戶，並按照提示完成設置，包括添加公司的支付方式（如公司信用卡或銀行帳戶）。  其餘答案   A.  聯繫  cloud-billing@google.com 並提供您的銀行帳戶詳情，請求為您的公司設立一個企業賬單帳戶。 雖然通過電子郵件聯繫Google Cloud的賬單團隊是可能的，但直接在GCP控制台中創建新的賬單帳戶並設置支付方法是一個更快且更直接的方法。  B. 創建一個Google支持的票據，並等待他們的電話來通過電話分享您的信用卡詳情。 這種方法不是設置或更改賬單帳戶的標準流程。  C. 在Google平台控制台中，轉到資源管理器並將所有項目移動到根組織。 雖然將項目移動到根組織是一種組織資源的好方法，但這並不涉及到設置或更改賬單帳戶的支付方式。   You have an application that looks for its licensing server on the IP 10.0.3.21. You need to deploy the licensing server on Compute Engine. You do not want to change the configuration of the application and want the application to be able to reach the licensing server.  What should you do?   A.Reserve the IP 10.0.3.21 as a static internal IP address using gcloud and assign it to the licensing server.\n\nB.Reserve the IP 10.0.3.21 as a static public IP address using gcloud and assign it to the licensing server.\n\nC.Use the IP 10.0.3.21 as a custom ephemeral IP address and assign it to the licensing server.\n\nD.Start the licensing server with an automatic ephemeral IP address, and then promote it to a static internal IP address.\n\n  \n    解題   題目問題詢問要部署授權伺服器到Compute Engine，同時不改變應用程式的配置，讓應用程式能夠通過IP 10.0.3.21連接到授權伺服器   主要考查在Google Cloud Platform (GCP) 上配置與管理IP地址，有關Compute Engine實例分配靜態內部IP地址配置。   解答建議A，使用gcloud預留IP 10.0.3.21作為靜態內部IP地址，並將其分配給授權伺服器。  其餘答案，B由於10.0.3.21是私有IP範圍，它不能作為公共IP地址預留。C在GCP中，您無法直接指定臨時IP地址為特定的IP。臨時IP地址在每次虛擬機重啟時可能會改變。D這個選項不確保您能獲得特定的IP地址（即10.0.3.21）。  ephemeral IP為臨時IP   You are deploying an application to App Engine. You want the number of instances to scale based on request rate. You need at least 3 unoccupied instances at all times.  Which scaling type should you use?\nd   A.Manual Scaling with 3 instances.\n\nB.Basic Scaling with min_instances set to 3.\n\nC.Basic Scaling with max_instances set to 3.\n\nD.Automatic Scaling with min_idle_instances set to 3.\n  \n    解題   題目問題，考查 App Engine 的自動伸縮 (Autoscaling) 功能，以及不同伸縮類型的特性。   一個 GCP Project 只能啟用一個 App Engine。這是因為 App Engine 是一種完全託管的服務，Google 需要為每個 App Engine 應用程式分配專用的資源。   解答建議為D，Automatic Scaling為根據應用程式流量自動調整執行個體 (instance) 數量。min_idle_instances用於指定最低閒置執行個體數量。將此值設為 3，可確保隨時至少有 3 個閒置執行個體。  其餘答案   A. 手動伸縮需自行設定執行個體數量，不符合依據流量自動調整的需求。  B. 基本伸縮雖可設定最低執行個體數量，但不會自動調整，無法滿足「依據請求速率伸縮」的需求。  C. 基本伸縮的最高執行個體設定，並非最低閒置執行個體，無法保證隨時有 3 個閒置執行個體。   You have a development project with appropriate IAM roles defined. You are creating a production project and want to have the same IAM roles on the new project, using the fewest possible steps.\nWhat should you do?   A.Use gcloud iam roles copy and specify the production project as the destination project.\n\nB.Use gcloud iam roles copy and specify your organization as the destination organization.\n\nC.In the Google Cloud Platform Console, use the `create role from role' functionality.\n\nD.In the Google Cloud Platform Console, use the `create role' functionality and select all applicable permissions.\n  \n    解題   題目問題在於需要在新的產品項目中設置與開發項目相同的IAM角色，並且使用最少的步驟。   這題主要考察的是如何在Google Cloud Platform (GCP) 中高效地管理和複製Identity and Access Management (IAM) 角色，確保不同專案間能夠共享相同的權限設置，而不需要手動一一配置 ref:   https://cloud.google.com/sdk/gcloud/reference/iam/roles/copy   解答建議為A，使用gcloud iam roles copy指令將iam角色相關設定複製到目標GCP專案  其餘答案，   B. 將角色複製到整個組織而不是特定的新項目，可能不會滿足題目中提到的將角色直接複製到一個新建項目的需求  C. 雖然這種方法理論上能夠從一個角色創建另一個角色，但它可能需要更多的手動操作，特別是如果需要複製多個角色到新項目時。  D. 使用控制台手動創建角色並選擇權限是可行的，但這是最耗時且容易出錯的方法，特別是當需要精確複製一個現有角色的所有權限設定時。   You need a dynamic way of provisioning VMs on Compute Engine. The exact specifications will be in a dedicated configuration file. You want to follow Google's recommended practices.\nWhich method should you use?   A.Deployment Manager\nB.Cloud Composer\nC.Managed Instance Group\nD.Unmanaged Instance Group\n  \n    解題   題目問題詢問若要以動態方式在Compute Engine上配置虛擬機（VM），並且要求根據一個專門的配置檔案來確定具體規格。   這個問題主要考察在Google Cloud Platform (GCP) 上動態配置Compute Engine虛擬機（VM）的方法，特別是當配置規格需要根據一個專門的配置文件來定義   解答建議使用A，Deployment Manager是Google Cloud Platform提供的一項服務，允許您透過聲明式的YAML、JSON或Python檔案來定義和部署資源。印象現在已經轉成用IAAC Code去做部屬設定。  其餘答案  B. Cloud Composer是基於Apache Airflow的全托管工作流自動化工具，主要用於協調複雜的工作流和數據處理管道。雖然它可以用來管理GCP資源的部署，但它不是專門為了動態配置VM而設計的。  C. Managed Instance Group，管理式實例群組（Managed Instance Group, MIG）提供了自動擴展、滾動更新和健康檢查等功能，適合於管理需要負載均衡和自動擴展的VM群組。雖然MIG支持動態管理VM實例，但它不直接支援從專門配置檔案動態定義VM規格。  D. Unmanaged Instance Group，非管理式實例群組（Unmanaged Instance Group）允許您將一組獨立的VM視為一個群組來管理，但缺乏自動擴展和健康檢查等管理功能。它不適用於需要根據配置檔案動態部署VM的情況。"},{"id":"content:101.license:2.PCA.md","path":"/license/pca","dir":"license","title":"Google Cloud Architect Professional License","description":"","keywords":["考題與報名網址","題目"],"body":"  Google Cloud Architect Professional License  考題與報名網址    題庫出處   模擬考題   註冊報名網址  題目   The JencoMart security team requires that all Google Cloud Platform infrastructure is deployed using a least privilege model with separation of duties for administration between production and development resources.   A. Create two G Suite accounts to manage users: one for development/test/staging and one for production. Each account should contain one project for every application\n\nB. Create two G Suite accounts to manage users: one with a single project for all development applications and one with a single project for all production applications\n\nC. Create a single G Suite account to manage users with each stage of each application in its own project\n\nD. Create a single G Suite account to manage users with one project for the development/test/staging environment and one project for the production environment\n  \n    解題   題目問題核心在於如何在Google Cloud Platform（GCP）上部署基礎設施，同時遵循最小權限模型(privilege model)和職責分離原則，尤其是在生產環境和開發資源之間的管理上。   privilege model，通常會透過專案項目隔離、IAM或是資源容器隔離來達到。   解答會是C，建立一個Google Workspace，然後每個stage(開發、測試、上線..)階段都有自己的Project會是最佳解。每個應用按環境分開項目的方法，增強了安全性和可管理性，並確保了職責的清晰分離。  其餘答案，   A. 要維護兩個Google WorkSpace，且每個應用都需要分別在開發和生產環境中擁有獨立的項目(Project)。這管理複雜度太高!!!  B. 創建兩個Google WorkSpace帳戶來管理用戶：一個包含所有開發應用的單一項目，另一個包含所有生產應用的單一項目。 這個選項同樣增加了管理的複雜性，並且將所有開發或生產應用放在單一項目中可能會對資源管理和訪問控制造成挑戰。  D. 基本上可行，但還是C隔離的比較乾淨!!   A few days after JencoMart migrates the user credentials database to Google Cloud Platform and shuts down the old server, the new database server stops responding to SSH connections. It is still serving database requests to the application servers correctly.  What three steps should you take to diagnose the problem? (Choose three.)   A. Delete the virtual machine (VM) and disks and create a new one\n\nB. Delete the instance, attach the disk to a new VM, and investigate\n\nC. Take a snapshot of the disk and connect to a new machine to investigate\n\nD. Check inbound firewall rules for the network the machine is connected to\n\nE. Connect the machine to another network with very simple firewall rules and investigate\n\nF. Print the Serial Console output for the instance for troubleshooting, activate the interactive console, and investigate\n  \n    解題   題目問題在於當面對一個新遷移到Google Cloud Platform的數據庫服務器突然停止響應SSH連接的問題時，但它仍然正確地處理應用服務器的數據庫請求。請問要如何診斷問題所在?此現象表明問題可能與網絡配置、系統配置或安全設置有關，而不是數據庫本身的問題。所以可以往這方向去探討問題。  解答會是CDF，詳細步驟如下   D. 檢查連接到機器的網絡的入站防火牆規則。 這是診斷SSH連接問題的第一步，因為防火牆規則可能被設置為阻止SSH流量。檢查確保沒有防火牆規則不當地限制了對該服務器的SSH訪問。  F. 輸出Instance的訊息至控制台輸出以進行故障排除，啟動互動控制台並進行調查。 這可以提供關於系統狀態和啟動過程中可能出現的問題的重要信息。串行控制台輸出有助於識別是否有系統級別的錯誤導致SSH服務無法正常工作。  C.透過對磁盤進行快照並將其掛載到另一台虛擬機上，可安全地檢查文件系統、系統日誌和其他配置，尋找導致SSH服務中斷的原因。  其餘答案，選擇 A 和 B （刪除虛擬機和磁盤或刪除實例並將磁盤附加到新的虛擬機）可能會對正在運行的服務造成不必要的風險，並且在初步診斷階段通常不推薦作為首選。而選項 E （將機器連接到另一個網絡進行調查）可能在排除網絡問題時有其用處，但在許多情況下，更改網絡配置可能不那麼直接或者可行，特別是當你想最小化對當前運行環境影響的時候。   JencoMart has decided to migrate user profile storage to Google Cloud Datastore and the application servers to Google Compute Engine (GCE). During the migration, the existing infrastructure will need access to Datastore to upload the data.   A. Provision service account keys for the on-premises infrastructure and for the GCE virtual machines (VMs)\n\nB. Authenticate the on-premises infrastructure with a user account and provision service account keys for the VMs\n\nC. Provision service account keys for the on-premises infrastructure and use Google Cloud Platform (GCP) managed keys for the VMs\n\nD. Deploy a custom authentication service on GCE/Google Kubernetes Engine (GKE) for the on-premises infrastructure and use GCP managed keys for the VMs\n  \n    解題   題目問題在於對於JencoMart決定將使用者資料儲存遷移到Google Cloud Datastore和應用程式伺服器遷移到Google Compute Engine (GCE)的情況，在遷移過程中，現有基礎架構需要存取Datastore以上傳資料。正確的做法是什麼?  解答投票為C，為本地基礎架構提供服務帳號金鑰，並為VM使用Google Cloud Platform (GCP) 管理的金鑰。詳細流程為     為本地基礎架構創建服務帳號並提供服務帳號金鑰，以便能夠安全使用GCP服務    GCE虛擬機（VMs）本身在GCP內運行時，可以直接利用GCP管理的服務帳號進行身份驗證，無需額外的服務帳號金鑰。  其餘答案，A忽略了GCP為GCE虛擬機提供的內建服務帳號管理能力，B使用用戶帳號進行身份驗證並不是一個推薦的做法。D直接使用GCP的身份與存取管理（IAM）和服務帳號就足以滿足安全存取GCP服務的需求，無需額外的自定義解決方案。   JencoMart has built a version of their application on Google Cloud Platform that serves traffic to Asia. You want to measure success against their business and technical goals.  Which metrics should you track?   A.Error rates for requests from Asia\nB.Latency difference between US and Asia\nC.Total visits, error rates, and latency from Asia\nD.Total visits and average latency for users from Asia\nE.The number of character sets present in the database\n  \n    解題   題目問題，要衡量JencoMart在Google Cloud Platform上構建的應用程式對亞洲流量服務的成功情況，並且對比它們的商業和技術目標。  解答投票率最高是C，   總訪問量：這可以幫助您了解應用程式吸引亞洲用戶的程度，是衡量用戶參與度和應用受歡迎程度的重要指標。  錯誤率：這反映了應用程式服務質量的重要面向，高錯誤率可能意味著後端問題或用戶體驗問題，這直接影響到用戶滿意度和應用的可靠性。  延遲：延遲是衡量用戶體驗的關鍵指標之一，特別是對於地理位置遙遠的用戶。低延遲確保了快速的響應時間，對於保持用戶滿意度和提高服務質量至關重要。  其餘答案，A. 來自亞洲的請求錯誤率 和 D. 來自亞洲用戶的總訪問量和平均延遲 都是有價值的指標，但它們提供的信息比選項C來得有限，沒有同時涵蓋訪問量、錯誤率和延遲的綜合視角。B. 美國和亞洲之間的延遲差異 雖然可以提供跨地區性能比較的見解，但它沒有直接關聯到衡量亞洲用戶體驗的目標。E. 資料庫中字符集的數量 與衡量應用服務於亞洲用戶的成功與否無直接關聯，不是衡量商業和技術目標的關鍵指標。"},{"id":"content:150.web-backend:1.HttpClient Log Record.md","path":"/web-backend/httpclient-log-record","dir":"web-backend","title":"HttpClient Log Record","description":"近期使用Net Core寫Web API有使用到","keywords":[],"body":"  HttpClient Log Record  近期使用Net Core寫Web API有使用到"},{"id":"content:150.web-backend:2. Quarkus Reactive Web API寫法紀錄.md","path":"/web-backend/quarkus-reactive-web-api","dir":"web-backend","title":"Quarkus Reactive Web API寫法紀錄","description":"近期因工作上需求，需要接觸新的後端框架...Quarkus，號稱有出色的啟動速度和低記憶體占用....因寫法上與一般Web API有點不同特別做個紀錄。","keywords":["一、RESTful Web API 阻塞式、非阻塞式、響應式與併發差異性","二、Quarkus 響應式與阻塞式寫法差異","Reference"],"body":"  Quarkus Reactive Web API寫法紀錄  近期因工作上需求，需要接觸新的後端框架...Quarkus，號稱有出色的啟動速度和低記憶體占用....因寫法上與一般Web API有點不同特別做個紀錄。  一、RESTful Web API 阻塞式、非阻塞式、響應式與併發差異性  一般API常見的有四種寫法，分別為阻塞式、非阻塞式、響應式與併發的寫法。他們的差異如下   阻塞式 (Blocking) : 每個請求都必須等待前一個請求完成後才能繼續進行。  非阻塞式 (Non-blocking) : 後續的請求不必等前一個請求完成，就可繼續進行。簡單來說就是允許系統在等待某個任務完成的同時，去做其他事情。  響應式 (Reactive) : 基立於Non-blocking(Thread Pool)、事件驅動（Event-Driven）和觀察者模式（Observer Pattern），相較於Non-blocking更能輕易做數據流的整合與操作，能更優雅的處理好的反壓（back-pressure）和資源彈性（resource elasticity）。  併發 (Concurrency) : 基本上併發是一種概念，簡單來說只要涉及同時間有多個請求或操作在進行就可以叫併發? 而Non-blocking與Reactive就是一種併發的實現方式。  a.Blocking  下圖為Blocking Flow，這Flow很簡單，簡單來說HTTP GET/user/joe打出去後，Client要等到HTTP Response，後端服務(Container)才可以接受處理另一個HTTP Request。    Java範例程式碼如下     @  GetMapping  (  \"/blocking\"  )\n   public   String   blocking  () throws InterruptedException {\n         // Simulate a long-running operation\n       Thread.  sleep  (  5000  );\n         return     \"Finished blocking operation\"  ;\n   }  b.Non-Blocking  Non-Blocking Flow如下，可以看到一個Key Point，在Container往Controller呼叫getUser()當下立即返回一個Future HTTP Result。這個Future HTTP Result會告知Container可以去處理其他事情，已收到任務命令。Container就可以接著去接收下一個Request。  詳細流程如下，基礎原理在Thread Pool與callback的應用實作   客戶端到容器（Container）   Step1 : Client發起HTTP GET請求，當客戶端（Client）發起一個HTTP GET到 /user/joe，這個請求首先會被送到容器（Container）。  容器到Controller   Step1 : HTTP請求到達Controller，容器會將這個HTTP請求轉交給Controller對應的getUser()方法。  Step2 : Controller回應Future HTTP Result，這裡的\"Future HTTP Result\"是一種占位符，告訴容器這個請求會被異步處理。Controller會立即回應，這樣容器就知道可以去處理其他請求。  Controller到Service   Step1 : Service方法開啟新執行緒，Controller會調用Service的getUserByName()方法。Service層會開啟一個新的執行緒（通常來自一個執行緒池(Thread Pool)）來處理這個請求。  Step2 : 釋放原始執行緒，一旦新的執行緒開始運行，用來處理原始HTTP請求的執行緒就會被釋放，返回到執行緒池中。  Service到Repository到Database   Step1 : 非同步操作和回調（Callbacks)，Service會進一步調用Repository的findAllByLastName()方法，然後Repository會非同步地與Database進行交互。  Step2 : Repository回應給Service，Repository會回傳一個CompletableFuture給Service。  Service到Controller到容器   Step1 : Service回應給Controller，Service會將CompletableFuture回傳給Controller。  Step2 : 回調由不同的執行緒執行：這個CompletableFuture一旦完成，它的回調會被一個不同的執行緒（也來自執行緒池）執行。  Step3 : 返回HTTP響應：最後，Controller會生成最終的HTTP響應（通常是CompletableFuture的實際內容）並回傳給容器，然後容器再回傳給客戶端。    Java範例程式碼如下       @  RestController\n   public     class     NonBlockingController   {\n         @  GetMapping  (  \"/non-blocking\"  )\n         public   CompletableFuture<  String  >   nonBlocking  () {\n             return   CompletableFuture.  supplyAsync  (()   ->   {\n                 try   {\n                     // Simulate a long-running operation\n                   Thread.  sleep  (  5000  );\n               }   catch   (InterruptedException   e  ) {\n                     // Handle exception\n               }\n                 return     \"Finished non-blocking operation\"  ;\n           });\n       }\n   }  c.Reactive  響應式是基立於Non-blocking(Thread Pool)的設計是上，並套用事件驅動（Event-Driven）和觀察者模式（Observer Pattern）設計模式。可以輕易達到資料流串接處理、back-pressure與resource elasticity。  這邊以flow圖去解釋要想個情境會稍微麻煩一些，故這小節紀錄Reactive，我想說直接以Srouce Code去解釋。  資料流串接處理  響應式寫法可以輕易的讓你客製資料流串接，例如三支服務處理完後，再去做事情。  舉個例子，假設你的系統需要從多個服務提供者（例如，不同的API或數據庫）中獲取數據，並且希望這些操作能夠非阻塞和高效地進行。       import   org.springframework.web.bind.annotation.GetMapping;\n   import   org.springframework.web.bind.annotation.RestController;\n   import   reactor.core.publisher.Flux;\n   import   reactor.core.publisher.Mono;\n     import   java.time.Duration;\n     @  RestController\n   public     class     ReactiveController   {\n           // 模擬從一個遠程API或數據庫中獲取單一數據\n         private   Mono<  String  >   getDataFromProvider1  () {\n             return   Mono.  just  (  \"Data from Provider 1\"  )\n                      .  delayElement  (Duration.  ofSeconds  (  2  ));\n       }\n           // 模擬從另一個遠程API或數據庫中獲取流式數據\n         private   Flux<  String  >   getDataFromProvider2  () {\n             return   Flux.  just  (  \"Data 1 from Provider 2\"  ,   \"Data 2 from Provider 2\"  )\n                      .  delayElements  (Duration.  ofSeconds  (  1  ));\n       }\n         @  GetMapping  (  \"/aggregate\"  )\n         public   Flux<  String  >   aggregateData  () {\n             // 合併兩個數據來源並返回\n             return   Flux.  concat  (  getDataFromProvider1  (),   getDataFromProvider2  ());\n       }\n   }  如果用一般寫法要達到同效果，寫法如下，相對起來會較於複雜，使用Reactive我們可以輕易達到當兩個數據都收到資料時，在實際做下一步動作.     import   org.springframework.web.bind.annotation.GetMapping;\n   import   org.springframework.web.bind.annotation.RestController;\n     import   java.util.ArrayList;\n   import   java.util.List;\n   import   java.util.concurrent.TimeUnit;\n     @  RestController\n   public     class     NonReactiveController   {\n           // 模擬從一個遠程API或數據庫中獲取單一數據\n         private   String   getDataFromProvider1  () {\n             try   {\n               TimeUnit.SECONDS.  sleep  (  2  );\n           }   catch   (InterruptedException   e  ) {\n               Thread.  currentThread  ().  interrupt  ();\n           }\n             return     \"Data from Provider 1\"  ;\n       }\n           // 模擬從另一個遠程API或數據庫中獲取流式數據\n         private   List<  String  >   getDataFromProvider2  () {\n           List<  String  > data   =     new   ArrayList<>();\n           data.  add  (  \"Data 1 from Provider 2\"  );\n             try   {\n               TimeUnit.SECONDS.  sleep  (  1  );\n           }   catch   (InterruptedException   e  ) {\n               Thread.  currentThread  ().  interrupt  ();\n           }\n           data.  add  (  \"Data 2 from Provider 2\"  );\n             return   data;\n       }\n         @  GetMapping  (  \"/aggregate\"  )\n         public   List<  String  >   aggregateData  () {\n           List<  String  > aggregatedData   =     new   ArrayList<>();\n             // 從第一個數據提供者獲取數據\n           aggregatedData.  add  (  getDataFromProvider1  ());\n             // 從第二個數據提供者獲取數據\n           aggregatedData.  addAll  (  getDataFromProvider2  ());\n             return   aggregatedData;\n       }\n   }  back-pressure  back-pressure為一種流量控制手段，當系統中的一個部分（通常是消費者）無法跟上另一個部分（通常是生產者）的速度時，它會發出一個信號，要求生產者減慢速度。換句話說，當系統過載時，響應式架構允許系統向上游（發送請求的來源）發出信號，告訴它「慢下來！」。  舉個簡單例子，假設我們的後端API需要從多個數據源拉取數據，然後進行一些耗時的計算或轉換，最後將結果返回給客戶端。使用Reactive方法程式碼如下，  這個例子中，我們創建了一個Flowable流，用於模擬一個快速生成數據的數據源。這個流生成0到999的整數。然後，我們用BackpressureStrategy.BUFFER來指定背壓策略，這樣如果後續的操作（例如，存儲到數據庫或進行計算）無法跟上數據生成的速度，這些數據會被緩存起來。  我們使用.observeOn(Schedulers.io())來指定後續操作應該在I/O線程上執行，以避免阻塞主線程。  這個.blockingSubscribe()方法會阻塞當前線程直到流被完全消費。這裡只是為了簡單示範，實際應用中應避免使用阻塞調用。     import   io.reactivex.rxjava3.core.BackpressureStrategy;\n   import   io.reactivex.rxjava3.core.Flowable;\n   import   io.reactivex.rxjava3.schedulers.Schedulers;\n   import   org.springframework.web.bind.annotation.GetMapping;\n   import   org.springframework.web.bind.annotation.RestController;\n     @  RestController\n   public     class     RxJavaBackPressureExampleController   {\n         @  GetMapping  (  \"/rxjava-backpressure-example\"  )\n         public   String   backPressureExample  () {\n             Flowable<  Integer  > flowable   =   Flowable.  create  (emitter   ->   {\n                 for   (  int   i   =     0  ; i   <     1000  ; i  ++  ) {\n                   emitter.  onNext  (i);\n               }\n               emitter.  onComplete  ();\n           }, BackpressureStrategy.BUFFER);\n             StringBuilder result   =     new     StringBuilder  ();\n             flowable.  observeOn  (Schedulers.  io  ())\n                   .  blockingSubscribe  (\n                           item   ->   {\n                                 // 模擬耗時操作\n                               Thread.  sleep  (  100  );\n                               result.  append  (item).  append  (  \", \"  );\n                           },\n                           throwable   ->   result.  append  (  \"Error: \"  ).  append  (throwable),\n                           ()   ->   result.  append  (  \"Done\"  )\n                   );\n               return   result.  toString  ();\n       }\n   }  resource elasticity  指一個系統能夠根據需求自動地調整其資源使用（例如，CPU、記憶體、帶寬等）。這邊舉個例子，  一個圖片轉換服務為例。這個服務需要大量的CPU資源來進行圖片處理。  情境描述   用戶會上傳圖片進行格式轉換（例如，從JPEG轉為PNG）。  在普通工作時間，只有少數請求。  在晚上或促銷活動期間，請求數量會劇增。  我們希望系統能夠根據請求的量動態調整資源。  程式碼如下，在這個例子中，我們使用了 Reactor 的 Elastic Scheduler。這個 Scheduler 會根據需要動態地創建或銷毀執行緒，從而達到資源彈性。     // Controller\n   @  RestController\n   public     class     ImageController   {\n       @  Autowired\n         private   ImageService imageService;\n         @  PostMapping  (  \"/convert\"  )\n         public   Mono<  Void  >   convert  (@  RequestBody   Flux<  Image  >   images  ) {\n             return   imageService.  convertImages  (images).  then  ();\n       }\n   }\n       // Service\n   @  Service\n   public     class     ImageService   {\n         public   Flux<  Image  >   convertImages  (Flux<  Image  >   images  ) {\n             return   images.  parallel  ()\n                        .  runOn  (Schedulers.  elastic  ())    // 使用Elastic Scheduler\n                        .  map  (  this  ::  heavyImageProcessing)\n                        .  sequential  ();\n       }\n           private   Image   heavyImageProcessing  (Image   image  ) {\n             // 耗CPU的圖片處理\n             return     new     Image  ();    // 返回轉換後的圖片\n       }\n   }  再舉個例子，你可以使用 Schedulers.io()如下 來自動地管理一個無界的線程池。根據需要創建更多的線程，並在完成後自動釋放，從而實現一種資源彈性。     Observable<  String  > observable   =   Observable.  create  (emitter   ->   {\n       emitter.  onNext  (  \"Hello\"  );\n       emitter.  onComplete  ();\n   });\n     observable\n       .  observeOn  (Schedulers.  io  ())\n       .  subscribe  (item   ->   {\n             // 這個操作將在 io Scheduler 上運行，具有資源彈性\n           System.out.  println  (item);\n       });  d.Reactive補充  在純後端的情境中，非阻塞（Non-blocking）的實現方式通常已經足夠應對大多數的需求，特別是在處理 I/O 等待、資料庫查詢或網絡請求等方面。但Reactive還是有其特定的應用場景，最常見的就是事件驅動架構及流量控制（Back-Pressure）。針對這兩個應用再給個情境。   流量控制（Back-Pressure）\n假設你有一個後端系統，它負責處理來自多個客戶端的圖片上傳。每個客戶端上傳的速度可能會不一樣。如果一個客戶端上傳速度非常快，而你的後端處理速度跟不上，這會造成資源過度使用或者其他客戶端的請求被延遲。  在這種情況下，你可以使用 Reactive 的 Back-Pressure 機制來控制接收速度。即，後端會告訴每個客戶端它目前可以接受的速度，這樣就不會因為一兩個過快的客戶端而影響整個系統。  在Java的世界中，這可以使用 Project Reactor 或者 RxJava 等庫來實現。以 Project Reactor 為例：     Flux.  create  (sink   ->   {\n         // 生產數據的邏輯，可能是讀取一個快速的數據源\n   })\n   .  onBackpressureBuffer  ()   // 這裡就是使用了背壓機制，當速度過快時會緩存數據\n   .  subscribe  (data   ->   {\n         // 處理數據的邏輯，可能是寫入一個慢速的數據源\n   });   事件驅動架構 (這範例有點爛...事件驅動架構是一個很大的Scope....)  在一個購物網站的後端，當用戶完成付款後，會有多個步驟需要同時進行，例如更新庫存、通知用戶、生成發票等。這些步驟之間相互獨立，但都是由「用戶完成付款」這一個事件觸發的。  在這種情況下，你可以使用事件驅動架構，當一個事件發生時，會通知（publish）給多個事件消費者（subscriber）。  例如，使用 Java 的 Spring Framework，你可以這樣做：     // 事件發布者\n   public     class     PaymentService   {\n       @  Autowired\n         private   ApplicationEventPublisher publisher;\n           public     void     completePayment  () {\n             // ...付款邏輯\n           publisher.  publishEvent  (  new     PaymentCompletedEvent  (  this  ));\n       }\n   }\n     // 事件消費者\n   public     class     StockService     implements     ApplicationListener  <  PaymentCompletedEvent  > {\n       @  Override\n         public     void     onApplicationEvent  (PaymentCompletedEvent   event  ) {\n             // ...更新庫存邏輯\n       }\n   }  每當 PaymentService 發布一個 PaymentCompletedEvent，所有訂閱了這個事件的服務（例如 StockService）都會收到通知，並執行相應的邏輯。   非阻塞（Non-blocking）   一般是基於執行緒池（Thread Pool）來實現的。  主要目的是讓執行緒可以在等待某個操作（如I/O操作或數據庫查詢）完成的期間，被釋放去做其他事情。  使用Future、CompletableFuture或其他異步機制來表示未來會完成的結果。  響應式（Reactive）   基於事件驅動（Event-Driven）和觀察者模式（Observer Pattern）。  更加關注數據流和變化的傳播，通常用於實現更為複雜的數據交互和流控制。  使用Reactive Streams API或者相關的庫（如RxJava、Project Reactor等）。  可以更好地處理背壓（Back-Pressure），即在高負載情況下控制數據流。  二、Quarkus 響應式與阻塞式寫法差異  一般來說Rest都是使用Java EE提供的Servlet的這個類別來實踐，傳統的Rest服務皆是如此。  而在Quarkus從底層就採用了新的引擎Vert.x來實作，而Vert.x又別於Servlet的不同是，他基於netty進行實作，也就是他是基於異步高併發的事件驅動的應用程式框架，也代表說為什麼在使用Quarkus要以響應式編程來實作你的程式。    框架的底層架構就先大概知道這裡就好了，詳細的有機會再去更深入的了解，目前我們需要專注的是怎麼寫好Quarkus的程序，像是常聽到的Reactive的相關套件，在Java和JavaScript最有名的就是RX，像是Spring Flux就是基於rxJava去實作的框架，但Quarkus選用的非rxJava而是Mutiny這個一個套件。  a. 阻塞 VS 非阻塞（響應式寫法）  我現在有一個需求會有兩個資料原，並且要將兩個資料原彙整後再回傳，以下分別用阻塞和非阻塞的寫法來說明。  阻塞  在以下的寫法必須先取得Data1後才能去發起請求取得Data2，因為此目的是要進行彙整，這樣我就會多了一個等待時間，導致程序在處理的速度較慢。     public   Data   aggregateData  () {\n       Data data1   =     fetchDataFromDatabase  ();       // 請求資料庫，此時執行緒被阻塞\n       Data data2   =     fetchDataFromExternalAPI  ();    // 請求外部 API，此時執行緒再次被阻塞\n       \n         return     combine  (data1, data2);               // 整合兩部分的資料\n   }  非阻塞(Mutiny)  而非阻塞再進行撰寫的時候，Data1和Data2是同時發出請求查詢資料，然後再進行彙整，這樣就可以減少要等待Data1查詢回來後的時間，這樣在程序處理的效能會比較快速。     public   Uni  <  Data  >     aggregateData  () {\n       Uni<  Data  > data1Uni   =     fetchDataFromDatabaseAsync  ();        // 非阻塞地請求資料庫\n       Uni<  Data  > data2Uni   =     fetchDataFromExternalAPIAsync  ();     // 非阻塞地請求外部 API\n       \n         return   Uni.  combine  ().  all  ().  unis  (data1Uni, data2Uni)       // 同時等待兩個 Uni 完成\n                .  asTuple  ()\n                .  onItem  ().  transform  (tuple   ->     combine  (tuple.  getItem1  (), tuple.  getItem2  ()));\n   }  b. Mutiny  在這個Mutiny主要提供了兩種介面來實踐Reactive如下列說明：   Uni<?>\n   提供0個或一個執行結果的異步操作，可被做資料串流整合，通常單獨使用只有onItem(onSucess)與onFailure，兩種事件。  Multi<?>\n   提供多個結果的異步操作，可以做資料串流整合，onItem與onFailure。  在這裡還是要提醒一下，Reactive不管是rxJava還是Mutiny都是基於Functional Programing (函數式程式設計)，如果有寫過前端框架如Angular，Rest，Vue的人應該會相對熟悉。  Uni 提供0個或1個執行結果的異步操作  Uni如果以Java原生的套件可以相當等於Future和CompletionStage。  Create Uni     //建立一個直接回傳值的Uni\n   Uni<  String  > uniFromValue   =   Uni.  createFrom  ().  item  (  \"Hello, Mutiny!\"  );\n     //創建一個有邏輯處理過後的Uni\n   Uni<  String  > uniFromSupplier   =   Uni.  createFrom  ().  item  (()   ->   {\n         // dosomething\n         return     \"Async result\"  ;\n   });  將CompletionStage轉換為Uni     Uni<  String  > uniFromCompletionStage   =   Uni.  createFrom  ().  completionStage  (  this  ::  asyncMethod);  組合多個Uni     Uni<  String  > uni1   =   Uni.  createFrom  ().  item  (  \"Hello\"  );\n   Uni<  String  > uni2   =   Uni.  createFrom  ().  item  (  \"Mutiny\"  );\n     Uni<  String  > combined   =   uni1.  onItem  ()\n                              .  combine  ()\n                              .  with  (uni2)\n                              .  by  ((item1, item2)   ->   item1   +     \" \"     +   item2);  錯誤處理     Uni<  String  > recovered   =   uniFromValue\n       .  onFailure  ()\n       .  recoverWithItem  (  \"Fallback item in case of failure\"  );  非阻塞異步轉換為阻塞同步 (這樣寫就變成同步了要注意)     String result   =   uniFromValue.  await  ().  indefinitely  ();\n   System.out.  println  (result);   // 輸出：Hello, Mutiny!  Multi 提供多個結果的異步操作  與 Uni 不同的是，Multi 可以表示多筆資料項目，例如來自資料庫的多筆記錄或者一系列的事件  Create Multi     // 創建一個集合的Multi\n   Multi<  String  > names   =   Multi.  createFrom  ().  items  (  \"Alice\"  ,   \"Bob\"  ,   \"Charlie\"  );  過濾和轉換     Multi<  Integer  > lengths   =   names\n       .  filter  (name   ->     !  name.  equals  (  \"Bob\"  ))        // 過濾掉 \"Bob\"\n       .  onItem  ().  transform  (String  ::  length);        // 將名稱轉換為其長度  錯誤處理     Multi<  String  > processed   =   names\n       .  onItem  ().  transform  (name   ->   {\n             if   (name.  equals  (  \"Bob\"  )) {\n                 throw     new     RuntimeException  (  \"Bob not allowed!\"  );\n           }\n             return   name;\n       })\n       .  onFailure  ().  recoverWithMulti  (Multi.  createFrom  ().  item  (  \"Error occurred\"  ));  合併     Multi<  String  > multi1   =   Multi.  createFrom  ().  items  (  \"A\"  ,   \"B\"  );\n   Multi<  String  > multi2   =   Multi.  createFrom  ().  items  (  \"C\"  ,   \"D\"  );\n     Multi<  String  > merged   =   Multi.  createBy  ().  merging  ().  streams  (multi1, multi2);    // 輸出：A, B, C, D  連結     Multi<  String  > multi1   =   Multi.  createFrom  ().  items  (  \"A\"  ,   \"B\"  );\n   Multi<  String  > multi2   =   Multi.  createFrom  ().  items  (  \"C\"  ,   \"D\"  );\n     Multi<  String  > concatenated   =   Multi.  createBy  ().  concatenating  ().  streams  (multi1, multi2);    // 輸出：A, B (等待完成), C, D  c. Infrastructure撰寫注意  實作過程串接有遇到一些問題，實測串接MongoDB段，Quarkus有提供一個響應式DB物件叫ReactiveMongoClient。如果直接使用這個物件，他基底基本上就是使用Mutiny去包MongoDB官方的MogoClient物件，預設就是響應式寫法。但實際用起來有很大的問題，在資料MongoDB Collection資料量破千後，使用ReactiveMongoClient寫法實際跑起來會比阻塞的寫法還慢。稍微查一下，感覺這個  Issue 目前還是Open。所以建議寫法，還是自己使用Mutiny去包DB物件。  d. K6.io測試結果   範例點我  針對範例程式去使用k6.io測試，模擬1000個user同時打API，並執行1000次結果如下，數據部分可以看到響應式寫法比阻塞式寫法在connecting時間差了7倍，成功率也比較高。這為簡單以極端例子去測試，做個簡易參考。    註:實際測試checks應該要達100%，通常有幾個原因，系統乘載量真的太大(極端模擬硬體Support不足)，不然就是程式碼真的寫得有問題。  Reference   https://quarkus.pro/guides/getting-started-reactive.html  https://blog.csdn.net/Tong_Rui/article/details/110683703  https://zhuanlan.zhihu.com/p/63228618  https://quarkus.io/blog/mutiny-back-pressure/   https://dassum.medium.com/building-a-reactive-restful-web-service-using-spring-boot-and-postgres-c8e157dbc81d  http://blog.nostratech.com/2018/12/building-reactive-rest-api-using-spring.html  https://howtodoinjava.com/spring-webflux/spring-webflux-tutorial/  https://engineering.linecorp.com/en/blog/reactive-streams-armeria-2  https://smallrye.io/smallrye-mutiny/1.6.0/tutorials/creating-uni-pipelines/  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-086898{color:#6A737D;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}.ct-157101{color:#E36209;}.dark .ct-157101{color:#FFAB70;}"}]