[{"id":"content:0.code:1.Refactoring00_Quick Review Notes.md","path":"/code/refactoring00_quick-review-notes","dir":"code","title":"Refactoring 00 : Quick Review Notes","description":"Reorganizing study notes for better code understanding","keywords":["Bad Code Smell Item List"],"body":"  Refactoring 00 : Quick Review Notes  Reorganizing study notes for better code understanding  Bad Code Smell Item List    Duplicated Code  : Identical or similar code appears in multiple places, increasing the burden of maintenance and updates.  Scenario: Two methods in different classes perform the same calculation.       // Class A\n   public     class     Order   {\n         public     double     calculateTotalPrice  (List<  Item  >   items  ) {\n             double   total   =     0  ;\n             for   (Item item   :   items) {\n               total   +=   item.  getPrice  ()   *   item.  getQuantity  ();\n           }\n             return   total;\n       }\n   }\n     // Class B\n   public     class     Invoice   {\n         public     double     calculateTotalPrice  (List<  Item  >   items  ) {\n             double   total   =     0  ;\n             for   (Item item   :   items) {\n               total   +=   item.  getPrice  ()   *   item.  getQuantity  ();\n           }\n             return   total;\n       }\n   }    Long Method  : A method (function) is overly long, making it difficult to understand and maintain; it should be broken down into smaller functional units.  Scenario: A method handles multiple responsibilities and is too lengthy.     public     void     processOrder  (Order order) {\n         // Validate order\n         // Calculate totals\n         // Apply discounts\n         // Update inventory\n         // Send notifications\n         // Generate invoice\n         // ... (method continues)\n   }    Large Class  : A class contains too many responsibilities and functionalities, violating the Single Responsibility Principle; it should be split into smaller, more focused classes.  Scenario: A class manages orders, customers, inventory, and reporting.     public     class     StoreManager   {\n         // Order management methods\n         // Customer management methods\n         // Inventory management methods\n         // Reporting methods\n         // ... (class continues)\n   }    Long Parameter List  : A single module needs frequent modifications for different reasons, leading to maintenance difficulties and violating the Single Responsibility Principle.  Scenario: A method requires numerous parameters to perform its task.     public     void     createUser  (String firstName, String lastName, String email, String phone,\n                          String address, String city, String state, String zipCode) {\n         // Method implementation\n   }    Divergent Change  : A single module needs frequent modifications for different reasons, leading to maintenance difficulties and violating the Single Responsibility Principle.  Scenario: A single class changes for multiple, unrelated reasons.     public     class     Employee   {\n         // Fields and methods related to employee data\n           // Methods related to payroll calculations\n         // Methods related to employee scheduling\n         // Methods related to performance reviews\n   }    Shotgun Surgery  : A small change requires modifications in multiple classes or modules, increasing the risk of errors and maintenance costs.  Scenario: Changing a database field name requires updates in many classes.     // Changing \"userName\" to \"username\" affects multiple classes\n   public     class     UserDAO   {\n         public   User   findByUserName  (String   userName  ) {   /* ... */   }\n   }\n     public     class     AuthenticationService   {\n         public     boolean     authenticate  (String   userName  , String   password  ) {   /* ... */   }\n   }\n     public     class     AuditService   {\n         public     void     logLogin  (String   userName  ) {   /* ... */   }\n   }    Feature Envy  : A method excessively uses data or functions from other classes; it should be moved to the related class.  Scenario: A method accesses the data of another class excessively.     public     class     SalaryCalculator   {\n         public     double     calculateSalary  (Employee   employee  ) {\n             double   base   =   employee.  getBaseSalary  ();\n             double   bonus   =   employee.  getPerformanceBonus  ();\n             double   tax   =   employee.  getTaxRate  ();\n             return   (base   +   bonus)   *   (  1     -   tax);\n       }\n   }    Data Clumps  : A group of related data frequently appears together; it should be encapsulated into a new class or structure.  Scenario: Multiple methods use the same group of parameters.     public     void     printAddress  (String street, String city, String state, String zip) {   /* ... */   }\n     public     void     validateAddress  (String street, String city, String state, String zip) {   /* ... */   }\n     public     void     saveAddress  (String street, String city, String state, String zip) {   /* ... */   }    Primitive Obsession  : Overusing primitive types to represent complex concepts; custom classes should be created to encapsulate these concepts.  Scenario: Using primitive types instead of small objects for simple tasks.     public     class     Rectangle   {\n         private     double   width;\n         private     double   height;\n           public     double     calculateArea  () {\n             return   width   *   height;\n       }\n   }\n     // Instead of using a Dimension class like as below\n   public     class     Dimension   {\n         private     double   width;\n         private     double   height;\n           public     Dimension  (  double     width  ,   double     height  ) {\n             this  .width   =   width;\n             this  .height   =   height;\n       }\n           // Getters, setters, and other relevant methods\n   }\n     public     class     Rectangle   {\n         private   Dimension dimension;\n           public     Rectangle  (Dimension   dimension  ) {\n             this  .dimension   =   dimension;\n       }\n           public     double     calculateArea  () {\n             return   dimension.  getWidth  ()   *   dimension.  getHeight  ();\n       }\n   }    Switch Statements  : Overreliance on switch or if-else structures; consider using polymorphism or design patterns as alternatives.  Scenario: Using a switch to determine behavior based on type.     public     double     calculateDiscount  (Customer customer) {\n         switch   (customer.  getType  ()) {\n             case     \"Regular\"  :\n                 return     0.05  ;\n             case     \"Premium\"  :\n                 return     0.10  ;\n             case     \"VIP\"  :\n                 return     0.20  ;\n             default:\n                 return     0  ;\n       }\n   }    Parallel Inheritance Hierarchies  : Adding a subclass in one hierarchy necessitates adding a corresponding subclass in another hierarchy.  Scenario: Adding a new shape requires adding new classes in different hierarchies.     // Shape hierarchy\n   public     abstract     class     Shape   {   /* ... */   }\n   public     class     Circle     extends     Shape   {   /* ... */   }\n   public     class     Square     extends     Shape   {   /* ... */   }\n     // Renderer hierarchy\n   public     abstract     class     ShapeRenderer   {   /* ... */   }\n   public     class     CircleRenderer     extends     ShapeRenderer   {   /* ... */   }\n   public     class     SquareRenderer     extends     ShapeRenderer   {   /* ... */   }    Lazy Class  : A class does too little to justify its existence; it should be merged with another class or removed.  Scenario: A class that doesn't do enough to justify its existence.     public     class     HelperUtils   {\n        public     static   String   toUpperCase  (String   input  ) {\n            return   input.  toUpperCase  ();\n      }\n   }    Speculative Generality  : Adding complexity for possible future needs, making the code harder to understand and maintain.  Scenario: Adding hooks and abstractions for future use cases that aren't needed yet.     public     interface     DataProcessor   {\n        void     processData  (Map<  String  ,   Object  >   data  );\n   }\n     public     class     JsonDataProcessor     implements     DataProcessor   {\n      @  Override\n        public     void     processData  (Map<  String  ,   Object  >   data  ) {\n            // Process JSON data\n      }\n   }\n     // Only JSON processing is needed, but the abstraction is added prematurely.    Temporary Field  : Certain fields in an object have values only under specific conditions and are empty otherwise, increasing the difficulty of understanding the code.  Scenario: An object has fields that are only set under certain conditions.     public     class     Order   {\n        private   Discount couponDiscount;   // Only used if a coupon is applied\n        private   Discount seasonalDiscount;   // Only used during seasonal sales\n          public     double     calculateTotal  () {\n            // Implementation\n      }\n   }    Message Chains  : Chaining multiple method calls across objects, like a.b().c().d(), leading to high coupling and fragility.  Scenario: Code that navigates through multiple objects to get data.     public     class     OrderService   {\n        public   String   getCustomerCity  (Order   order  ) {\n            return   order.  getCustomer  ().  getAddress  ().  getCity  ();\n      }\n   }    Middle Man  : A class where most methods simply delegate to methods in other classes, lacking substantial functionality.  Scenario: A class delegates most of its work to another class.     public     class     CustomerManager   {\n        private   CustomerDAO customerDAO   =     new     CustomerDAO  ();\n          public     void     addCustomer  (Customer   customer  ) {\n          customerDAO.  addCustomer  (customer);\n      }\n          public   Customer   getCustomer  (  int     id  ) {\n            return   customerDAO.  getCustomer  (id);\n      }\n          // Most methods just delegate to CustomerDAO\n   }    Inappropriate Intimacy  : Classes overly depend on each other's internal implementations, violating the principle of encapsulation.  Scenario: Two classes rely too heavily on each other's internal details.     public     class     ClassA   {\n        private     int   secretValue;\n          public     int     getSecretValue  () {\n            return   secretValue;\n      }\n   }\n     public     class     ClassB   {\n        public     void     doSomething  (ClassA   a  ) {\n            int   value   =   a.  getSecretValue  ();   // Accessing internal details\n            // Manipulate value\n      }\n   }    Alternative Classes with Different Interfaces  : Classes perform similar functions but have different interfaces; interfaces should be unified or classes should be combined.  Scenario: Classes with similar functionality but different method signatures.     public     class     XmlParser   {\n        public   Document   parseXml  (String   xml  ) {   /* ... */   }\n   }\n     public     class     JsonParser   {\n        public   JSONObject   parse  (String   json  ) {   /* ... */   }\n   }\n     // Interfaces are different despite similar purposes    Incomplete Library Class  : The library class in use doesn't fully meet the needs; it requires extension or wrapping.  Scenario: A library class doesn't provide necessary functionality.     // Suppose the library's List doesn't have a sort method\n   List<  Integer  > numbers   =     new   ArrayList<>();\n   // Need to write a custom sort\n   Collections.  sort  (numbers);   // Library method is insufficient    Data Class  : A class contains only fields without behavior; it may need methods added or a redesign.  Scenario: A class with only fields and no methods (behavior).     public     class     Point   {\n        public     double   x;\n        public     double   y;\n   }\n     // No methods to manipulate or utilize the data    Refused Bequest  : A subclass inherits unwanted functionality from its parent class, violating the Liskov Substitution Principle.  Scenario: A subclass doesn't use inherited methods or overrides them improperly.     public     class     Animal   {\n        public     void     eat  () {   /* ... */   }\n        public     void     sleep  () {   /* ... */   }\n   }\n     public     class     RobotDog     extends     Animal   {\n      @  Override\n        public     void     eat  () {\n            // Doesn't need to eat; method is irrelevant\n      }\n   }    Comments  : Excessive or unnecessary comments may indicate that the code is hard to understand; the code itself should be improved.  Scenario: Excessive comments that may indicate confusing code.     public     int     calculate  (  int   a,   int   b) {\n        // Check if a is greater than b\n        if   (a   >   b) {\n            // Subtract b from a\n            return   a   -   b;\n      }   else   {\n            // Add a and b\n            return   a   +   b;\n      }\n   }  Bad Smell Problem Classification  Issues caused by code smells:   Understandability : Refers to issues that make the code harder to read and understand.  Modifiability :  Makes it more difficult to change or maintain the code.  Testability : Hinders testing or lowers test coverage.  Performance : Can lead to slower system performance.  Reusability : Limits the ability to reuse code across different projects or situations.  Scalability :  Affects the system’s ability to grow and handle increased demand.  Maintainability : Increases the long-term effort needed to maintain the code.  Complexity : Adds unnecessary complexity to the codebase.  Design Quality : Violates fundamental design principles.  Code Style : Deviates from coding standards or best practices.  This article aims to provide a quick overview of the key points in refactoring. By summarizing common code smells and presenting practical code examples, it helps readers rapidly understand what typical code smells are and the issues they can cause. By recognizing these smells, developers can more easily identify areas that need improvement in their daily programming, take appropriate refactoring actions, and enhance the quality and maintainability of their code.  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-086898{color:#6A737D;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-157101{color:#E36209;}.dark .ct-157101{color:#FFAB70;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}"},{"id":"content:0.code:1.Refactoring01.md","path":"/code/refactoring01","dir":"code","title":"Refactoring 01 : Detail","description":"","keywords":["Lazy Class"],"body":"  Refactoring 01 : Detail  Lazy Class  If a class has very few functionalities and hardly implements any methods or responsibilities, it might be considered a Lazy Class. For example:         class     Address   {\n         private   String street;\n         private   String city;\n           // getter 和 setter\n   }  The Address class might only be used within the Customer class, serving a simple and uncomplicated purpose. In this case, you might consider merging Address into Customer:     class     Customer   {\n       private   String name;\n       private   String street;\n       private   String city;\n         // getter 和 setter\n   }  Generally, you can handle this situation using several techniques:  Inline Class  Integrate simple classes into the classes that use them, reducing unnecessary layers of abstraction and making the code more concise.     // Before\n   class     Address   {\n         private   String fullAddress;\n         public   String   getFullAddress  () {   return   fullAddress; }\n   }\n     class     Customer   {\n         private   Address address;\n         public   String   getAddress  () {   return   address.  getFullAddress  (); }\n   }\n     // After\n   class     Customer   {\n         private   String address;\n         public   String   getAddress  () {   return   address; }\n   }  Move Method  If a class has only one method, consider turning it into a method of another related class. This reduces the number of classes and centralizes related logic.     // Before\n   class     TaxCalculator   {\n         public     double     calculateTax  (  double     amount  ) {   return   amount   *     0.05  ; }\n   }\n     // After\n   class     OrderProcessor   {\n         private     double     calculateTax  (  double     amount  ) {   return   amount   *     0.05  ; }\n   }  Extract Interface  When a class's functionality is simple but important, you can convert it into an interface. This increases flexibility and allows for different implementations.     // Before\n   class     SimpleLogger   {\n         public     void     log  (String   message  ) { System.out.  println  (message); }\n   }\n     // After\n   interface     Logger   {\n         void     log  (String   message  );\n   }\n     class     ConsoleLogger     implements     Logger   {\n         public     void     log  (String   message  ) { System.out.  println  (message); }\n   }  Merge Classes  If two classes have similar or related functionalities, consider merging them. This consolidates related logic and reduces duplicate code.     // Before\n   class     Circle   {\n         private     double   radius;\n         public     double     area  () {   return   Math.PI   *   radius   *   radius; }\n   }\n     class     Circumference   {\n         private     double   radius;\n         public     double     length  () {   return     2     *   Math.PI   *   radius; }\n   }\n     // After\n   class     Circle   {\n         private     double   radius;\n         public     double     area  () {   return   Math.PI   *   radius   *   radius; }\n         public     double     circumference  () {   return     2     *   Math.PI   *   radius; }\n   }  Convert to Attribute  This is a composite refactoring technique that often involves the following specific refactoring methods:   Inline Class: Merge the functionality of the EmployeeId class into the Employee class.  Remove Middle Man: Eliminate unnecessary delegation methods.  Encapsulate Field: Ensure that the id field still has proper encapsulation.     // Before\n   class     EmployeeId   {\n         private   String value;\n       \n         public   String   getValue  () {\n             return   value;\n       }\n       \n         public     void     setValue  (String   value  ) {\n             this  .value   =   value;\n       }\n   }\n     class     Employee   {\n         private   EmployeeId id;\n       \n         public   String   getId  () {\n             return   id.  getValue  ();\n       }\n       \n         public     void     setId  (String   id  ) {\n             this  .id.  setValue  (id);\n       }\n   }\n     // After\n   class     Employee   {\n         private   String id;\n       \n         public   String   getId  () {\n             return   id;\n       }\n       \n         public     void     setId  (String   id  ) {\n             this  .id   =   id;\n       }\n   }  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-086898{color:#6A737D;}.ct-157101{color:#E36209;}.dark .ct-157101{color:#FFAB70;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}"},{"id":"content:0.code:2.Dependency Injection.md","path":"/code/dependency-injection","dir":"code","title":"Dependency Injection","description":"Dependency Injection (DI) is an essential design pattern in many software architectures today. This chapter will explain the concept of DI and introduce its usage in .NET.","keywords":["A. Dependency Injection","B. Container Concept","C. Ioc(Inversion of Control)","D. C# Dependency Injection Framework [Example]"],"body":"  Dependency Injection (DI) is an essential design pattern in many software architectures today. This chapter will explain the concept of DI and introduce its usage in .NET.  A. Dependency Injection  Let's start by discussing the concepts of dependency and injection.  a. Dependency  As the term suggests, 'dependency' means that objects in software often rely on each other. For example, imagine a calculator that needs to print the results. This calculator 'depends on' a printer. So, when you create a calculator, you also need to create a printer and give it to the calculator       public     class     Calculator\n   {\n         public     Printer     Printer   {  get  ;  private     set  }\n         public     Add  (  int     a  ,   int     b  ){\n             var     sum     =   a  +  b;\n           Printer.  ConsoleOut  (sum);\n       }\n         public     Calculator  (  Printer     printer  ){\n           Printer   =   printer;\n       }\n   }\n   public     class     Printer\n   {\n         public     void     ConsoleOut  (  string     txt  ) \n       {\n           Console.  WriteLine  (txt);\n       }\n   }\n   static     class     Program\n   {\n         static     void     Main  ()\n       {\n             var     printer     =     new     Printer  ();\n             var     calculator     =     new     Calculator  (printer);\n           calculator.  Add  (  1  +  1  ); \n       }\n   }  Because the Calculator relies on the Printer in this example, the control is not with the Printer. In other words, modifications to the Printer can have implications for the Calculator.  I found a fitting analogy online to illustrate object dependency. Imagine a teenager (let's call them '8+9') addicted to drugs. The '8+9' might believe they're in control and using drugs for their own pleasure. However, in reality, the '8+9's behavior is now controlled by the drug, making it impossible to break free.    b. Dependency With Interface  How can we reduce dependency? A common approach is to make objects dependent on interfaces. By doing this, objects become decoupled, as illustrated in the diagram.       public     class     Calculator\n   {\n         public     IPrinter     Printer   {  get  ;  private     set  }\n         public     Add  (  int     a  ,   int     b  ){\n             var     sum     =   a  +  b;\n           Printer.  ConsoleOut  (sum);\n       }\n         public     Calculator  (  IPrinter     printer  ){\n           Printer   =   printer;\n       }\n   }\n   public     class     Printer  :  IPrinter\n   {\n         public     void     ConsoleOut  (  string     txt  ) \n       {\n           Console.  WriteLine  (txt);\n       }\n   }\n     public     interface     IPrinter  {\n         void     ConsoleOut  (  string     txt  );\n   }\n     static     class     Program\n   {\n         static     void     Main  ()\n       {\n             IPrinter     printer     =     new     Printer  ();\n             var     calculator     =     new     Calculator  (printer);\n           calculator.  Add  (  1  +  1  ); \n       }\n   }  The Dependency Inversion Principle (DIP) enables us to achieve loose coupling between modules. In this example, by depending on an interface rather than a concrete class, we can easily swap out the printer implementation for a text file output without affecting the higher-level component. This promotes code reusability and maintainability.   The high-level module in this scenario is the Calculator, which is the caller, while the low-level module is the Printer, which is the callee.  Let's go back to the example of the \"8+9\". If we want to help them turn their life around and become a good young person, we can start by replacing their direct dependency on drugs with a dependency on medication, which acts as an interface.    Let's say, hypothetically, we successfully replace the \"8+9\"'s drug dependency with a dependence on medication. Then, we'll create a \"health food\" category and secretly swap out the medication with actual health food. The \"8+9\" will think they're still taking their drugs, but in reality, they'll be consuming daily doses of nutritious food, leading to improved cognitive function, better posture, and even perfect test scores, transforming them into pillars of society.  c. Injection  Let's talk about injection. In the Calculator example, we used constructor injection. Actually, injection is something we use all the time, but we might not notice it. There are three common injection patterns:   Constructor Injection  Property Injection  Method Injection  Constructor Injection     // Car interface\n   public     interface     ICar   {\n         // Method to start the car\n         void     run  ();\n   }\n     // Driver interface\n   public     interface     IDriver   {\n         // Method for the driver to drive a car\n         void     drive  ();\n   }\n     // Concrete implementation of the Driver interface\n   public     class     Driver     implements     IDriver   {\n         private   ICar car;\n           // Constructor injection: Injects the car object during object creation\n         public     Driver  (ICar   car  ) {\n             this  .car   =   car;\n       }\n         @  Override\n         public     void     drive  () {\n           car.  run  ();\n       }\n   }  Property Injection     // Car interface\n   public     interface     ICar   {\n         // Method to start the car\n         void     run  ();\n   }\n     // Driver interface\n   public     interface     IDriver   {\n         // Method for the driver to drive a car\n         void     drive  ();\n   }\n     // Driver class with property injection\n   public     class     Driver     implements     IDriver   {\n         private   ICar car;\n           // Property injection: Injects the car object using a setter method\n         public     void     setCar  (ICar   car  ) {\n             this  .car   =   car;\n       }\n         @  Override\n         public     void     drive  () {\n           car.  run  ();\n       }\n   }  Property Injection     // Car interface\n   public     interface     ICar   {\n         // Method to start the car\n         void     run  ();\n   }\n     // Driver interface\n   public     interface     IDriver   {\n         // Method for the driver to drive a car\n         void     drive  ();\n   }\n       // Driver class with method injection\n   public     interface     IDriver   {\n         // Method for the driver to drive a car, taking the car object as a parameter\n         void     drive  (ICar   car  );\n   }\n     public     class     Driver     implements     IDriver   {\n       @  Override\n         public     void     drive  (ICar   car  ) {\n           car.  run  ();\n       }\n   }  After understanding Dependency and Injection, it should be clear that the dependency injection process involves placing the corresponding object into the instance object.  B. Container Concept  After discussing the DI concept, let's move on to the container concept. In Section A, we mentioned the issue of dependencies. Although we can decouple objects using interfaces, in large-scale projects, the complexity and quantity of objects become more intricate. This is where the container mechanism comes into play.    The concept of a container is to store all objects in a \"box\". When you need to use an object, you simply retrieve it from the box. It's like a golfer who carries a golf bag to store their clubs and has a caddy to assist them. Whenever the golfer needs a specific club, they just tell the caddy.  a. Comparing \"poor\" and \"rich\" DI  After understanding dependency injection and containers, let's talk about the two main approaches: \"poor man's DI\" and \"rich man's DI\".   Poor man's DI means creating objects manually using the new keyword. This can be tedious and error-prone, especially when dealing with complex object graphs.  Rich man's DI involves using a DI container. This is like having a caddy in golf - the container manages the creation and lifetime of objects for you. You simply configure the container and it handles the rest.  C. Ioc(Inversion of Control)  We'd all rather be rich than poor, right? That's why we use DI containers! When we use a DI container to manage our objects, we're essentially implementing inversion of control.  Simply put, if object A needs to use object B, traditionally A would create a new instance of B. But with a DI container, we hand over this responsibility to the container. Instead of A actively controlling the creation of B, the container takes care of it.  This is a great example of inversion of control. In web frameworks, for instance, the framework handles tasks like listening for HTTP requests and parsing them. As developers, we don't need to worry about these low-level details; the framework takes care of them for us.   Inversion of Control (IoC) is a design pattern that shifts the control of object creation and lifecycle management to an external container or framework. Instead of a component directly controlling its dependencies, it receives them from an outside source.  D. C# Dependency Injection Framework   [Example]  After discussing DI, containers, DIP, and IOC, we should now move on to the practical implementation. In .NET, there are two commonly used DI frameworks:   Microsoft.Extensions.DependencyInjection  Autofac  As mentioned above, there are three common injection methods (constructor, property, and method), which Autofac supports. However, ASP.NET DI currently only supports constructor injection. While most online examples focus on web applications, we will be using both DI tools for desktop Windows Forms in this context.  Microsoft.Extensions.DependencyInjection  DI configuration is set in DIServiceConfigure.cs.  The lifespan of injected services  Service lifetime specifies whether a new instance of a component is created on each request via dependency injection, or if a shared instance is used throughout the application. This behavior is controlled by the service's lifetime scope.   1.AddSingleton  2.AddTransient  3.AddScoped  1. AddSingleton  Only one instance of this object is created for the entire application and is shared among all parts of the application. In simpler terms, whenever different parts of the program need to use this object, they are always using the same exact object. It's similar to a static variable, but with the advantage of allowing object-oriented design and dependency injection.  The diagram below illustrates the lifetime of objects in ASP.NET DI, with object dependencies flowing from left to right.    In our simulation scenario, we can map the objects as follows:   Request: Button_Singleton_Click event  First circle: Call LogController  Second circle: Call LogRepository  Instance: SingletonLogRepository  For the Repository injection, we use AddSingleton. To observe if the instances are indeed the same, we will generate a GUID when the object is instantiated.         // Use a singleton pattern for system-wide unique objects or when there are no concurrency or high-load request issues.\n   collection.  AddSingleton  <  ISingletonLogRepository  ,   LogRepository  >();  When the Button_Singleton_Click button is pressed, both LogController and LogController2 will call the SingletonLogRepository object and print its GUID.     private     void     Button_Singleton_Click  (  object     sender  ,   EventArgs     e  )\n   {\n         //var provider = DIServiceConfigure.GetProvider();\n         //var logController = provider.GetRequiredService<ILogController>();\n         var     log     =     \"Log1(Singleton):\"  +  \"UUID-\"  +  LogController.  OperationId  (  \"Singleton\"  );\n         var     log2     =     \"Log2(Singleton):\"     +     \"UUID-\"     +   LogController2.  OperationId  (  \"Singleton\"  );\n       richTextBox_Info.  AppendText  (log  +  \"  \\n  \"  );\n       richTextBox_Info.  AppendText  (log2   +     \"  \\n  \"  );\n       richTextBox_Info.  AppendText  (LogController.  QueryLogCount  ());\n   }  We can observe that the two printed UUIDs are identical, indicating that the SingletonLogRepository instance is shared between the LogController and LogController2.    2. AddTransient  By definition, a new instance of the object, along with any of its dependent objects, is created every time a request for it is made within the application.  The diagram below depicts the lifecycle of objects within the ASP.NET Dependency Injection system. The dependencies between objects are represented from left to right.    In our simulation model, the objects can be mapped as follows:   Request: Triggered by the Button_Transient_Click event.  First node: Invokes the LogController.  Second node: Invokes the LogController2.  Instance: Utilizes a TransientLogRepository.  To implement the Repository injection, we employ AddTransient. In order to verify that distinct instances are created, we will generate a unique identifier (GUID) upon object instantiation.     collection.  AddTransient  <  ITransientLogRepository  ,   LogRepository  >();  For scenarios requiring asynchronous operations and a high volume of requests, using the Transient lifetime is recommended. However, simulating this in a simple Windows Forms application can be challenging.  When the Button_Transient_Click button is pressed, both LogController and LogController2 will call the SingletonLogRepository object and print its GUID.     private     void     Button_Transient_Click  (  object     sender  ,   EventArgs     e  )\n   {\n        // Simulate multiple requests\n        //var provider = DIServiceConfigure.GetProvider();\n        //var logController = provider.GetRequiredService<LogController>();\n        //var log = \"Log1(Transient):\" + \"UUID-\" + logController.GUID.ToString();\n        //richTextBox_Info.AppendText(log + \"\\n\");\n              // Simulate a single requests\n        var     log     =     \"Log1(Transient):\"     +     \"UUID-\"     +   LogController.  OperationId  (  \"Transient\"  );\n        var     log2     =     \"Log2(Transient):\"     +     \"UUID-\"     +   LogController2.  OperationId  (  \"Transient\"  );\n      richTextBox_Info.  AppendText  (log   +     \"  \\n  \"  );\n      richTextBox_Info.  AppendText  (log2   +     \"  \\n  \"  );\n          //richTextBox_Info.AppendText(LogController.QueryLogCount());\n   }  We can see that the two printed UUIDs are different, indicating that the TransientLogRepository instances in Controller and Controller2 are distinct objects.    The above simulation example only demonstrates a single Request1 scenario. According to the definition, a new object is created every time a component is requested in the program. To simulate this, we have repeatedly clicked the button. To simulate multiple requests, we will uncomment the \"multiple requests\" block and comment out the \"single request\" code.     private     void     Button_Transient_Click  (  object     sender  ,   EventArgs     e  )\n   {\n         // Simulate multiple requests\n         var     provider     =   DIServiceConfigure.  GetProvider  ();\n         var     logController     =   provider.  GetRequiredService  <  LogController  >();\n         var     log     =     \"Log1(Transient):\"     +     \"UUID-\"     +   logController.GUID.  ToString  ();\n       richTextBox_Info.  AppendText  (log   +     \"  \\n  \"  );\n           // Simulate a single requests\n         //var log = \"Log1(Transient):\" + \"UUID-\" + LogController.OperationId(\"Transient\");\n         //var log2 = \"Log2(Transient):\" + \"UUID-\" + LogController2.OperationId(\"Transient\");\n         //richTextBox_Info.AppendText(log + \"\\n\");\n         //richTextBox_Info.AppendText(log2 + \"\\n\");\n           //richTextBox_Info.AppendText(LogController.QueryLogCount());\n   }  The fact that the UUIDs are unique for every request proves that the Transient lifetime of the Controller is working as expected. Each time the container is asked to provide a Controller instance, a new one is created, ensuring that there are no shared state issues between different requests. This behavior is also observed for other dependencies registered with a transient lifetime.    3. AddScope  The Scoped lifetime is arguably the most complex to understand. When visualizing the ASP.NET DI lifecycle, object dependencies are typically represented as a directed graph from left to right. A key characteristic of the Scoped lifetime is that within the boundaries of a single request, all components and their dependencies are resolved from the same scope, ensuring that they reference the same instances. Nevertheless, a new scope is initiated for each incoming request, guaranteeing that subsequent requests will receive fresh instances of these components.    In order to examine the behavior of multiple requests, we implemented a simulation by calling the Dialog method. The diagram below provides a visual representation of the experiment's outcomes. A notable observation is that the Instance property of each object...   SingletonLogRepository  TransientLogRepository  ScopedLogRepository  We can observe that within the same request flow, the ScopedLogRepository instances in LogController and LogService share the same UUID. However, when the dialog is opened a second time, the UUID becomes different from the first one. This behavior is distinct from that of a singleton.  Additionally, we can see that the Transaction instances have unique UUIDs for each object and every request.    Differentiating between Scoped and Transient lifetimes can be tricky,  here's a simpler way to grasp their differences.  The Scoped lifetime is tied to a specific scope, usually a single request in web applications. Within that scope, the first instance of a component is created and reused for all dependencies as long as the scope is active. This ensures that all components within the same request share the same instance. Once the request is complete and the scope ends, the instance is disposed. For each new request, a new instance is created, ensuring no shared state between different requests.  On the other hand, the Transient lifetime creates a new instance of a component every time it's requested, regardless of scope. This means that even within the same request or operation, multiple calls to resolve the same component will result in entirely new instances, mimicking the behavior of traditional object creation using the new keyword. As a result, Transient components are more lightweight but don't share state across different parts of the application, even within a single request.  Summary of Dependency Injection Lifetimes  The concepts of Scoped, Singleton, and Transient lifetimes are crucial in understanding how dependency injection functions across different application architectures. In web applications, where the HTTP request lifecycle is relatively short, the Scoped lifetime is commonly used. This is because each HTTP request typically represents a distinct scope, allowing objects related to the request to be instantiated once and then disposed of at the end of the request. This helps ensure efficient resource management during the short-lived nature of web requests.  In contrast, Windows Forms applications often favor the Singleton lifetime. Due to the long-running nature of desktop applications, Singletons provide a convenient way to share state across the entire application without repeatedly creating new instances. The Scoped lifetime is less frequently applied in this context since there is no natural \"request boundary\" as seen in web applications. However, it can still be useful in specific cases where scoping is required for certain operations.  The Transient lifetime, which creates a new instance of a component each time it's requested, is commonly used for short-lived objects that don't need to be shared or reused. In many cases, such objects are disposed of immediately after use. While Transient lifetimes are useful, many developers prefer manual object creation using the \"new\" keyword, especially in scenarios where the overhead of dependency injection might not be necessary. This manual approach can sometimes offer more control over object instantiation and disposal.  Dynamic injection and swapping of dependencies  After discussing object lifetimes, let's move on to the concept of swapping concrete objects. In our example, we use the IPrinter interface to inject and swap between PrinterMethodA and PrinterMethodB.  Initially, PrinterMethodA is injected into IPrinter. When the button_Printer_Click button is pressed, the console outputs \"MethodA Print: Printer Out\".    Then, we replace PrinterMethodA with PrinterMethodB.     collection.  AddTransient  <  IPrinter  ,   PrinterMethodB  >();  Upon clicking the button_Printer_Click button, the console will display the message \"MethodB Print: Printer Out\".    Additionally, we can inject a list of IPrinter instances by injecting multiple concrete implementations.     collection.  AddTransient  <  IPrinter  ,   PrinterMethodA  >();\n   collection.  AddTransient  <  IPrinter  ,   PrinterMethodB  >();  We can inject multiple PrinterMethod methods for flexibility.     private     LogController     LogController  ;\n   private     LogController2     LogController2  ;\n     // Multiple injection\n   private     IEnumerable  <  IPrinter  >   Printer  ;\n     public     Form1  (  IEnumerable  <  IPrinter  >   printer  ,\n                  LogController     logController  , \n                  LogController2     logController2  )\n   {\n         InitializeComponent  ();\n       LogController   =   logController;\n       LogController2   =   logController2;\n         // Multiple injection\n       Printer   =   printer;\n   }    Through altering the container configuration, we can dynamically replace all components that rely on the IPrinter interface, while keeping the main program's codebase unchanged. This demonstrates the power of dependency injection.  AutoFac  The sample DI configuration is specified in AutofacConfig.cs.  Autofac and ASP.NET DI share a similar concept of lifetime scope, as illustrated in the table below   Source    As indicated in the mapping table, to transition from the sample program to an Autofac implementation, you merely need to comment out the .NET DI code within Bootstrapper and Form.cs and uncomment the associated Autofac code. This straightforward modification enables you to examine the distinct object lifetime behaviors governed by each dependency injection framework.  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}.ct-086898{color:#6A737D;}.ct-157101{color:#E36209;}.dark .ct-157101{color:#FFAB70;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}"},{"id":"content:0.code:3.Pattern-Strategy.md","path":"/code/pattern-strategy","dir":"code","title":"Pattern-Strategy","description":"","keywords":["Introduction","Usage Context"],"body":"  Pattern-Strategy  Introduction  Recently, I've begun to organize the design patterns I've previously utilized. Among these patterns, the Strategy pattern is a common one, categorized under Behavioral Patterns.   Behavioral design patterns are concerned with algorithms and the assignment of responsibilities between objects.  The Strategy pattern is generally applied in scenarios where methods vary in their implementation. In such cases, an interface is declared, and different objects implement this interface. When there's a need to change the implementation method, it can be directly swapped within the Context, or the change can be made directly through Dependency Injection (DI). This approach facilitates flexible and interchangeable behavior within software components.  Usage Context  Recently, while working on national instrument communication, I encountered a piece of code that initially used GPIB for communication. Later on, USB communication was added. GPIB communication utilized the MessageBasedSession Class from NI's VISA DLL files, whereas USB communication employed the Device Class from NI's N4882 DLL files.  In the past, when writing structured programs, my instinct was to directly use a variable to switch between communication methods, as shown in the 5th line of the program below. This resulted in having to perform a check in every method (lines 29, 43). If a new communication method, such as TCP, were to be added at this point, it would require significant adjustments to the NormalBaseDevice (every function would need to be modified).     public     class     NormalBaseDevice\n   {\n           private     bool     IsGpib  ;\n           // Gpib Com\n         MessageBasedSession     GpibCom  ;\n       \n         // Usb Com\n         Device     UsbCom  ;\n       \n         public     NormalBaseDevice  (  bool     isGPIB  )\n       {\n             IsGpib   =   isGPIB;\n                   if   (isGPIB)\n               {\n                   GpibCom   =     new     MessageBasedSession  ();\n               }\n                 else\n               {\n                   UsbCom   =     new     Device  ();\n               }\n             }\n               public     string     Write  (  string     cmd  )\n           {\n                   if   (IsGpib)\n               {\n                     return   GpibCom.  Write  (cmd);\n               }\n                 else\n               {\n                     return   UsbCom.  Write  (cmd);\n               }\n             }\n               public     int     Read  ()\n           {\n                   if   (IsGpib)\n               {\n                     return   GpibCom.  Read  ();\n               }\n                 else\n               {\n                     return   UsbCom.  Read  ();\n               }\n             }\n         }  This approach results in all communication methods being tightly coupled together, making any adjustments inevitably impactful. At this point, we can use an interface to achieve separation, allowing each communication method to implement the interface. Adjustments can then be made directly in the Context.  First, we declare an IStrategy interface, with the interface methods for Write and Read as follows:     public     interface     IStrategy\n   {\n         string     Write  (  string     cmd  );\n           int     Read  ();\n   }  Next, we write the GpibCom and UsbCom concrete classes and implement the IStrategy methods.     public     class     GpibCom   :   IStrategy\n   {\n         private     MessageBasedSession     Device  ;\n           public     GpibCom  ()\n       {\n           Device   =     new     MessageBasedSession  ();\n       }\n           public     string     Write  (  string     cmd  )\n       {\n             return   Device.  Write  (cmd);\n            \n       }\n           public     int     Read  ()\n       {\n             return   Device.  Read  ();\n       }\n   }        public     class     UsbCom   :   IStrategy\n   {\n         private     Device     Device  ;\n           public     UsbCom  ()\n       {\n           Device   =     new     Device  ();\n       }\n           public     string     Write  (  string     cmd  )\n       {\n             return   Device.  Write  (cmd);\n       }\n           public     int     Read  ()\n       {\n             return   Device.  Read  ();\n       }\n   }  At this point, within the Context, we can freely swap to the method we need, as shown below:     class     Program\n   {\n          static     void     Main  (  string  []   args  )\n       {\n             IStrategy     com  ;\n           \n           com   =     new     GpibCom  ();\n             var     cmd     =   com.  Write  (Action);\n           \n           com   =     new     UsbCom  ();\n             var     cmd     =   com.  Write  (Action);\n           \n       }\n   }  Additionally, it's common to pair the Strategy pattern with a simple factory to encapsulate the switch for selecting which object to instantiate, as shown below:     public     class     ComFacotry\n   {\n         public     enum     ComType   {   Gpib  ,   Usb  }\n           public     static     IStrategy     CreateCom  (  ComType     type  )\n       {\n             IStrategy     com  ;\n               switch   (type)\n           {\n                 case   ComType.Gpib:\n                   com   =     new     GpibCom  ();\n                     break  ;\n                   \n                 default  :\n                   com   =     new     UsbCom  ();\n                     break  ;\n             }\n               return   com;\n       }\n   }  The use of Context then becomes:     class     Program\n   {\n          static     void     Main  (  string  []   args  )\n       {\n             var     usbCom     =   ComFacotry.  CreateCom  (ComType.Usb);\n             var     cmd     =   usbCom.  Write  (Action);\n       }\n   }  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-086898{color:#6A737D;}"},{"id":"content:0.code:4.Pattern-Singleton.md","path":"/code/pattern-singleton","dir":"code","title":"Pattern-Singleton","description":"","keywords":["Introduction","Context Usage"],"body":"  Pattern-Singleton  Introduction  Recently, I began organizing and documenting the design patterns I've used before. The Singleton pattern is one of the more frequently used patterns, categorized under Creational Patterns.   Creational Patterns: Creational patterns provide various object creation mechanisms, which increase flexibility and reuse of existing code.  The Singleton pattern is among the commonly used design patterns, falling under the category of Creational Patterns. Creational patterns provide various mechanisms for creating objects, enhancing flexibility and reuse of existing code.  A Singleton object is instantiated automatically in memory upon the program's startup, similar to the commonly seen Static Method. This method can be used without needing to be created explicitly.  Context Usage   Static Method  Static Object Definition  For instance, if we're looking to implement a certain static method, we typically use the static keyword. For example, it's common in systems to declare shared configuration variables that are read from a config file. This approach allows these variables to be accessed globally without creating an instance of the class. The Singleton pattern can be applied to ensure that only one instance of the configuration settings is created and shared across the system, enhancing consistency and preventing potential issues from multiple instantiations.     //Static Method\n      public     class     StaticAppSetting\n   {\n         public     static     readonly     string     AppName     =     GetConfigValue  (  \"AppName\"  );\n           public     static     string     GetConfigValue  (  string     value  )\n       {\n             return   ConfigurationManager.AppSettings[value];\n       }\n   }\n     // Context\n   class     Program\n   {\n         static     void     Main  (  string  []   args  )\n       {\n           Console.  WriteLine  (StaticAppSetting.AppName);\n       }\n   }  Using static methods allows us to easily implement shared static functionality, which is more commonly utilized in structured programming. However, if we want to incorporate object-oriented concepts, such as inheritance and interfaces, relying solely on static methods won't meet our needs for object-oriented design.  When implementing the Singleton pattern, typically two key actions are undertaken:   Creating a static private field (INSTANCE): This field holds the singleton instance. You implement logic to check whether this field is null. If it is, a new instance of the class is created. This ensures that only one instance of the class exists within the application at any time. Additionally, the constructor of the class is made private or inaccessible to prevent external instantiation.  Creating a public static method or property to access the instance: This method or property provides a global access point to the singleton instance. It checks if the INSTANCE field is null; if so, it initializes the instance. If not, it returns the existing instance. This method ensures that the class adheres to the singleton property, providing a single, globally accessible instance throughout the application.  Online content can be presented in various styles, and currently, the method my friend shared is considered the most elegant as described below.     //Singleton Pattern\n   public     class     SingletonAppSetting\n   {\n         //Instance Method\n         private     static     class     SingletonHolder\n       {\n             static     SingletonHolder  () { }\n             internal     static     readonly     SingletonAppSetting     INSTANCE     =     new     SingletonAppSetting  ();\n       }\n       \n         // Instance Field\n         public     static     SingletonAppSetting     Instance   {   get   {   return   SingletonHolder.INSTANCE; } }\n           public     readonly     string     AppName  ;\n    \n         // Construct\n         protected     SingletonAppSetting  ()\n       {\n           AppName   =     GetConfigValue  (  \"AppName\"  );\n       }\n           public     string     GetConfigValue  (  string     value  )\n       {\n             return   ConfigurationManager.AppSettings[value];\n       }\n   }\n     // Context\n   class     Program\n   {\n         static     void     Main  (  string  []   args  )\n       {\n            Console.  WriteLine  (SingletonAppSetting.Instance.AppName);\n       }\n   }  If we have another system that we want to adapt, which involves network-related settings, we can revise it by using a static approach. One method is to add a new static class, and the other is to modify the existing static class as follows.        public     class     StaticAppSetting\n   {\n         public     static     readonly     string     AppName     =     GetConfigValue  (  \"AppName\"  );\n           public     static     readonly     string     LocalIp     =     GetConfigValue  (  \"LocalIp\"  );\n           \n         public     static     readonly     string     LocalPort     =     GetConfigValue  (  \"LocalPort\"  );\n           public     static     string     GetConfigValue  (  string     value  )\n       {\n                 return   ConfigurationManager.AppSettings[value];\n       }\n   }  If using the Singleton pattern, you can declare a new InternetAppSetting object that inherits from the original AppSetting as follows.     public     class     SingletonInternetAppSetting   :   SingletonAppSetting\n   {\n         private     static     class     SingletonHolder\n       {\n             static     SingletonHolder  () { }\n             internal     static     readonly     SingletonInternetAppSetting     INSTANCE     =     new     SingletonInternetAppSetting  ();\n       }\n         public     static     new     SingletonInternetAppSetting     Instance   {   get   {   return   SingletonHolder.INSTANCE; } }\n           public     readonly     string     LocalIp  ;\n         public     readonly     string     LocalPort  ;\n           protected     SingletonInternetAppSetting  ()\n       {\n           LocalIp   =     GetConfigValue  (  \"LocalIp\"  );\n           LocalPort   =     GetConfigValue  (  \"LocalPort\"  );\n       }\n   }\n     //Context\n   class     Program\n   {\n         static     void     Main  (  string  []   args  )\n       {\n           Console.  WriteLine  (SongletonInternetAppSetting.Instance.AppName);\n           Console.  WriteLine  (SongletonInternetAppSetting.Instance.LocalIp);\n           Console.  WriteLine  (SongletonInternetAppSetting.Instance.LocalPort);\n       }\n   }  Additionally, we can utilize an interface to work with the Singleton method. For instance, we can declare an ISetting interface.     public     interface     ISetting\n   {\n         string     GetConfigValue  (  string     value  );\n   }  Have the object implement the interface.   public class SingletonAppSetting : ISetting\npublic class SingletonInternetAppSetting : ISetting  This would then allow for direct substitution of the implementation in the context of use.     class     Program\n   {\n         static     void     Main  (  string  []   args  )\n       {\n             ISetting     appSetting     =   SingletonAppSetting.Instance;\n             var     valueFromConfig     =   appSetting.  GetConfigValue  (  \"Value\"  );\n             appSetting   =   SongletonInternetAppSetting.Instance;\n             var     valueFromIni     =   appSetting.  GetConfigValue  (  \"Value\"  );\n       }\n   }  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-086898{color:#6A737D;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}"},{"id":"content:0.code:5.Aspect-Oriented Programming.md","path":"/code/aspect-oriented-programming","dir":"code","title":"AOP (Aspect-Oriented Programming)","description":"","keywords":["Introduction","Dynamic proxies","AspectCore","Summary"],"body":"  AOP (Aspect-Oriented Programming)  Introduction  Recently, during a discussion with a friend about mechanisms for logging, I learned about Aspect-Oriented Programming (AOP) and decided to delve a little into its usage and implementation. Although King's use of AspCore proved to be a quick way to implement AOP, I still wanted to spend some time understanding how to achieve this without relying on a framework.  AOP employs a cross-cutting technique to insert new methods into the original class without altering or disrupting the original class methods. This technique is a skillful way to introduce new functionalities into a program, enhancing its modularization and flexibility without compromising the integrity of the original code structure.   Separate cross-cutting concerns from the core business logic to further enhance the modularity of the code!  Let me give you an example. Suppose you received the following service code that has already been written:     public     interface     IXXXService  {\n         void     QueryData  ()\n   }\n     public class XXXService:IXXXService{\n       \n         public     IXXXService  (){\n       \n       }\n       \n       public   void     QueryData  (){\n           ....\n       }\n   }  This service method does not provide a logging mechanism. If you want to add logging, what should you do?  The first method you might consider is directly modifying the source code.     public     interface     IXXXService  {\n         void     QueryData  ()\n   }\n     public class XXXService:IXXXService{\n       \n       private readonly ILog _log;\n       \n         public     IXXXService  (  ILog     log  ){\n           _log   =   log;\n       }\n       \n       public   void     QueryData  (){\n             try  {\n              _log.  I  (  \"QueryData\"  );\n              ....\n           }\n             catch  (  Exception     e  )\n           {\n             _log.  E  (  $\"Imp QueryData \"     +   {e.  ToString  ()});\n           }\n           ....\n       }\n   }  We modify the service directly by injecting your own logging function and incorporating try-catch mechanisms within the method. However, this approach clearly alters the original source code. Moreover, logging and the original logic are not directly related, so combining them isn't an ideal method.  What alternatives exist that allow us to add logging without modifying the internal code, ensuring the service retains its sole responsibility of handling business logic? One solution is to employ a simple static proxy method to achieve this.       // 原程式碼\n   public     interface     IXXXService  {\n         void     QueryData  ()\n   }\n     public class XXXService:IXXXService{\n       \n         public     IXXXService  (){\n       \n       }\n       \n       public   void     QueryData  (){\n           ....\n       }\n   }\n     //使用Proxy\n     public     class     ProxyXXX   :   IXXXService  {\n           private     readonly     IXXXService     _xxxService  ;\n         private     readonly     ILog     _log  ;\n       \n         public     ProxyXXX  (  IXXXService     xxxService  ,   ILog     log  ){\n           _xxxService   =   xxxService;\n           _log   =   log;\n       }\n       \n         public     void     QueryData  (){\n             try  {\n               _log.  I  (  \"QueryData\"  );\n               _xxxService.  QueryData  ();\n               \n           }  catch  (  Exception     e  ){\n               _log.  E  (  $\"Imp QueryData \"     +   {e.  ToString  ()});\n           }\n       }\n     }  Declare a Proxy Class to implement the IXXXService method. Since this is through a proxy, there's an opportunity to intervene before and after execution actions (try-catch, log). This allows for the addition of logging functionality without altering the original code.  This concept is essentially a form of aspect-oriented programming (AOP), which separates concerns (e.g., logging) from the business core, further decoupling them.  In more detail, many program requirements, such as logging, are not directly related to the program's logic but need to be interjected into functions at appropriate times. Such requirements are typically addressed using aspects or cross-cutting concerns. This approach separates these unrelated needs from the original logical functions, enhancing the design's modifiability and reducing coupling.  The diagram conceptually illustrates that in programming, we often need to handle functions like error logging, authentication, and potentially adding user activity tracking. By applying the AOP concept, we don't have to individually add these functionalities to each method. Instead, as indicated by the grey, yellow, and red arrows, all methods must go through authentication, data handling, and error processing. This represents a type of middleware design concept.    After discussing the above, if the service you're working with has hundreds of methods, it's impractical to implement a static proxy for each method to introduce additional functionalities. In such cases, we would use dynamic proxies to achieve this goal.  Dynamic proxies  In C#, dynamic proxies can be implemented using two classes: RealProxy and DispatchProxy. The former is available for use in the standard .NET Framework, while the latter is intended for use with .NET Core.  Practical Scenario  Based on the Dynamic Proxy tutorial scenario from   MSDN , let's assume we have a Customer Model scenario, where we interact with the data through a Repository.  Therefore, we first focus on designing the foundational infrastructure for the Context, including the implementation related to the Customer Model and Repository. The Repository will handle the usual CRUD (Create, Read, Update, Delete) operations.    Customer Model        public     class     Customer\n   {\n       public     int     Id   {   get  ;   set  ; }\n       public     string     Name   {   get  ;   set  ; }\n       public     string     Address   {   get  ;   set  ; }\n   }  IRepository        public     interface     IRepository  <  T  >\n   {\n         void     Add  (  T     entity  );\n         void     Delete  (  T     entity  );\n         void     Update  (  T     entity  );\n         IEnumerable  <  T  >   GetAll  ();\n         T     GetById  (  int     id  );\n   }  Repository     public     class     Repository  <  T  > :   IRepository  <  T  >\n   {\n             public     void     Add  (  T     entity  )\n           {\n               Console.  WriteLine  (  \"Adding {0}\"  , entity);\n           }\n             public     void     Delete  (  T     entity  )\n           {\n               Console.  WriteLine  (  \"Deleting {0}\"  , entity);\n           }\n             public     void     Update  (  T     entity  )\n           {\n               Console.  WriteLine  (  \"Updating {0}\"  , entity);\n           }\n             public     IEnumerable  <  T  >   GetAll  ()\n           {\n               Console.  WriteLine  (  \"Getting entities\"  );\n                 return     null  ;\n           }\n             public     T     GetById  (  int     id  )\n           {\n               Console.  WriteLine  (  \"Getting entity {0}\"  , id);\n                 return     default  (  T  );\n           }\n   }  After implementing the foundational infrastructure, in a scenario without using a Proxy, you can directly create a Repository instance to perform CRUD operations.  Main        class     Program\n       {\n             static     void     Main  (  string  []   args  )\n           {\n                 //Simple Use - No Logger\n               Console.  WriteLine  (  \"***  \\r\\n   Begin program - no logging  \\r\\n  \"  );\n                 IRepository  <  Customer  >   customerRepository     =\n                   new     Repository  <  Customer  >();\n                 var     customer     =     new     Customer\n               {\n                   Id   =     1  ,\n                   Name   =     \"Customer 1\"  ,\n                   Address   =     \"Address 1\"\n               };\n               customerRepository.  Add  (customer);\n               customerRepository.  Update  (customer);\n               customerRepository.  Delete  (customer);\n               Console.  WriteLine  (  \"  \\r\\n  End program - no logging  \\r\\n  ***\"  );\n             }\n       }  The output is as follows    Implementing Dynamic Proxy using RealProxy    Add logging to the Repository layer.  Now, we implement logging and Try-Catch in every CRUD operation at the Repository layer through RealProxy. Implementing Dynamic Proxy with RealProxy is quite straightforward, requiring only the implementation of the Invoke method. The underlying principle uses C# reflection to implement the methods of the entity being proxied. For those interested in a deeper exploration, consider reviewing the article on   Aspect-oriented programming . It discusses the principles of AOP, which are largely based on Reflection, Metaobject Protocols, and Composition Filters.     public     class     DynamicProxy  <  T  > :   RealProxy\n       {\n             private     readonly     T     _decorated  ;\n             public     DynamicProxy  (  T     decorated  )\n             :   base  (  typeof  (  T  ))\n           {\n               _decorated   =   decorated;\n           }\n             // Log Fun\n             private     void     Log  (  string     msg  ,   object     arg     =     null  )\n           {\n               Console.ForegroundColor   =   ConsoleColor.Red;\n               Console.  WriteLine  (msg, arg);\n               Console.  ResetColor  ();\n           }\n             // Impleation Invoke\n             public     override     IMessage     Invoke  (  IMessage     msg  )\n           {\n                 var     methodCall     =   msg   as     IMethodCallMessage  ;\n                 var     methodInfo     =   methodCall.MethodBase   as     MethodInfo  ;\n                   Log  (  \"In Dynamic Proxy - Before executing '{0}'\"  , methodCall.MethodName);\n                   try\n               {\n                     var     result     =   methodInfo.  Invoke  (_decorated, methodCall.InArgs);\n                     Log  (  \"In Dynamic Proxy - After executing '{0}' \"  , methodCall.MethodName);\n                     return     new     ReturnMessage  (result,   null  ,   0  , methodCall.LogicalCallContext, methodCall);\n               }\n                 catch   (  Exception     e  )\n               {\n                     Log  (string.  Format  (  \"In Dynamic Proxy- Exception {0} executing '{1}'\"  , e),methodCall.MethodName);\n                     return     new     ReturnMessage  (e, methodCall);\n               }\n           }\n       }  After implementing the dynamic proxy, the client-side usage is as follows.     var     repository     =     new     Repository  <  Customer  >();                      \n   var     customerRepoProxy     =  (  IRepository  <  Customer  >)  new     DynamicProxy  <  IRepository  <  Customer  >>(repository);\n     var     newcustomer     =     new     Customer\n    {\n       Id   =     1  ,\n       Name   =     \"New Customer \"  ,\n       Address   =     \"New Address\"\n    };\n   customerRepoProxy.  Add  (newcustomer);\n   customerRepoProxy.  Update  (newcustomer);\n   customerRepoProxy.  Delete  (newcustomer);  We can create a Repository factory that can flexibly generate or assemble different proxies.     public     class     RepositoryFactory\n   {\n             public     static     IRepository  <  T  >   Create  <  T  >()\n           {\n                 var     repository     =     new     Repository  <  T  >(); \n                 var     decoratedRepository     =  (  IRepository  <  T  >)  new     DynamicProxy  <  IRepository  <  T  >>(repository).  GetTransparentProxy  ();\n                 return   decoratedRepository;\n           }\n   }  Add authentication to the Repository  By utilizing Dynamic Proxy, after adding logging and try-catch to the original Repository CRUD operations, we then attempt to construct a Dynamic Proxy to simulate method-level permission verification.  Implement an AuthenticationProxy.     public     class     AuthenticationProxy  <  T  > :   RealProxy\n   {\n             private     readonly     T     _decorated  ;\n             public     AuthenticationProxy  (  T     decorated  )\n             :   base  (  typeof  (  T  ))\n           {\n               _decorated   =   decorated;\n           }\n             private     void     Log  (  string     msg  ,   object     arg     =     null  )\n           {\n               Console.ForegroundColor   =   ConsoleColor.Green;\n               Console.  WriteLine  (msg, arg);\n               Console.  ResetColor  ();\n           }\n             public     override     IMessage     Invoke  (  IMessage     msg  )\n           {\n                 var     methodCall     =   msg   as     IMethodCallMessage  ;\n                 var     methodInfo     =   methodCall.MethodBase   as     MethodInfo  ;\n                   try\n               {\n                     Log  (  \"User authenticated - You can execute '{0}' \"  ,methodCall.MethodName);\n                     var     result     =   methodInfo.  Invoke  (_decorated, methodCall.InArgs);\n                     return     new     ReturnMessage  (result,   null  ,   0  ,\n                     methodCall.LogicalCallContext, methodCall);\n               }\n                 catch   (  Exception     e  )\n               {\n                     Log  (string.  Format  (\n                       \"User authenticated - Exception {0} executing '{1}'\"  , e),methodCall.MethodName);\n                     return     new     ReturnMessage  (e, methodCall);\n               }\n                   Log  (  \"User not authenticated - You can't execute '{0}' \"  ,methodCall.MethodName);\n                 return     new     ReturnMessage  (  null  ,   null  ,   0  , methodCall.LogicalCallContext, methodCall);\n             }\n   }  After implementing the AuthenticationProxy, we modify the original Repository Factory accordingly.        public     class     RepositoryFactory\n   {\n             public     static     IRepository  <  T  >   Create  <  T  >()\n           {\n                 var     repository     =     new     Repository  <  T  >();\n               \n               \n                 var     decoratedRepository     =  (  IRepository  <  T  >)  new     DynamicProxy  <  IRepository  <  T  >>(repository).  GetTransparentProxy  ();\n                   // Create a dynamic proxy for the class already decorated\n               decoratedRepository   =  (  IRepository  <  T  >)  new     AuthenticationProxy  <  IRepository  <  T  >>(decoratedRepository).  GetTransparentProxy  ();\n                     return   decoratedRepository;\n           }\n   }  The client-side is as follows.     //Use Dynamic Proxy \n   Console.  WriteLine  (  \"***  \\r\\n   Begin program - logging with dynamic proxy  \\r\\n  \"  );\n   IRepository  <  Customer  >   customerRepoProxy     =   RepositoryFactory.  Create  <  Customer  >();\n   var     newcustomer     =     new     Customer\n   {\n      Id   =     1  ,\n      Name   =     \"New Customer \"  ,\n      Address   =     \"New Address\"\n   };\n   customerRepoProxy.  Add  (newcustomer);\n   customerRepoProxy.  Update  (newcustomer);\n   customerRepoProxy.  Delete  (newcustomer);\n   Console.  WriteLine  (  \"  \\r\\n  End program - logging with dynamic proxy  \\r\\n  ***\"  );\n   Console.  ReadLine  ();    Implement a Dynamic Proxy using DispatchProxy    Add logging to the Repository  Based on the above example, we implement it again using DispatchProxy. The operation of DispatchProxy is similar, but in addition to implementing Invoke, we also need to specifically implement (decorate) the part about creating class instances.     public     class     DynamicProxy  <  T  > :   DispatchProxy     where     T   :   class\n   {\n               public     T     Target   {   get  ;   private     set  ; }\n           \n             public     DynamicProxy  () :   base  ()\n           {\n             }\n               public     static     T     Decorate  (  T     target     =     null  )\n           {\n                 var     proxy     =     Create  <  T  ,   DynamicProxy  <  T  >>()   as     DynamicProxy  <  T  >;\n                 proxy.Target   =   target   ??   Activator.  CreateInstance  <  T  >();\n                   return   proxy   as     T  ;\n           }\n                 private     void     Log  (  string     msg  ,   object     arg     =     null  )\n           {\n               Console.ForegroundColor   =   ConsoleColor.Red;\n               Console.  WriteLine  (msg, arg);\n               Console.  ResetColor  ();\n           }\n               protected     override     object     Invoke  (  MethodInfo     targetMethod  ,   object  []   args  )\n           {\n                 Log  (  \"In Dynamic Proxy - Before executing '{0}'\"  , targetMethod.Name);\n                   try\n               {\n                     // 使用Class Method\n                     var     result     =   targetMethod.  Invoke  (Target, args);\n                     Log  (  \"In Dynamic Proxy - After executing '{0}' \"  , targetMethod.Name);\n                     return   result;\n                 }\n                 catch  (  Exception     e  )\n               {\n                     Log  (string.  Format  (  \"In Dynamic Proxy- Exception {0} executing '{1}'\"  , e), targetMethod.Name);\n                     return     null  ;\n               }\n           }\n   }  After implementation, the method of use on the client side is as follows.        var     repository     =     new     Repository  <  Customer  >();\n      var     messageDispatchProxy     =   DynamicProxy<  IRepository  <  Customer  >>.  Decorate  (repository);   Implement a RepositoryFactory to flexibly generate or assemble different proxies.     public     class     RepositoryFactory\n   {\n             public     static     IRepository  <  T  >   Create  <  T  >()\n           {\n                 var     repository     =     new     Repository  <  T  >();\n                 var     proxyRepo     =   DynamicProxy<  IRepository  <  T  >>.  Decorate  (repository);\n                 return   proxyRepo;\n           }\n   }  The client-side usage is as follows.     var     messageDispatchProxy     =   RepositoryFactory.  Create  <  Customer  >();\n   var     customer     =     new     Customer\n   {\n     Id   =     1  ,\n     Name   =     \"Customer 1\"  ,\n     Address   =     \"Address 1\"\n   };\n   messageDispatchProxy.  Add  (customer);\n   messageDispatchProxy.  Update  (customer);\n   messageDispatchProxy.  Delete  (customer);    AspectCore  For AOP (Aspect-Oriented Programming), .NET Core already has ready-to-use tools, one of which is called AspectCore. Regarding   AspectCore , I find Neil Tsai's article   AspectCore | .Net Core Lightweight AOP Implementation  quite clear in its explanation. However, I will continue to use the context described in the Microsoft MSDN tutorial.  As mentioned above, we talked about using a dynamic proxy to fetch Customer data from a repository. Next, we will use AspectCore on the web to achieve this. In terms of web architecture, we design using a common centralized architecture, writing a Customer Service to use the repository. And with AspCore, when calling the service from a controller, we add logging to display.  Step1:Add a new Web MVC project.  We use Visual Studio to add a new MVC project. There are no special settings during the process, so there's no need for detailed description of the initial project creation.  Step2:Install AspCore  Directly install AspCore using the CLI, or use the NuGet Package Manager to install AspectCore.Extensions.DependencyInjection.   dotnet add package AspectCore.Extensions.DependencyInjection\n  Step3:Implement the CustomerService  Next, we start implementing the service. First, add the Customer Service Interface, implementing only AddCustomer.     public     interface     ICustomerService\n   {\n       void     AddCustmoer  (  Customer     customer  );\n   }  Then proceed with the implementation.     public     class     CustmoerService   :   ICustomerService\n   {\n           private     readonly     IRepository  <  Customer  >   _repo  ;\n           public     CustmoerService  (  IRepository  <  Customer  >   repo  )\n       {\n               _repo   =   repo;\n       }\n           public     void     AddCustmoer  (  Customer     customer  )\n       {\n               _repo.  Add  (customer);\n       }\n     }  Step4:Design a service interceptor using the AbstractInterceptorAttribute.  After completing the service implementation, begin writing the interceptor with AspCore (utilizing the service as a proxy). This will intercept service calls and insert log displays both before and after the calls.     public     class     ServiceInterceptor   :   AbstractInterceptorAttribute\n   {\n       [  FromServiceContext  ]\n         public     ILogger  <  ServiceInterceptor  >   Logger   {   get  ;   set  ; }\n           public     async     override     Task     Invoke  (  AspectContext     context  ,   AspectDelegate     next  )\n       {\n             try\n           {\n               Logger.  LogInformation  (  \"In Dynamic Proxy - Before executing '{0}'\"  , context.ServiceMethod.Name); \n                 await     next  (context);    // 進入 Service 前會於此處被攔截（如果符合被攔截的規則）...\n               Logger.  LogInformation  (  \"In Dynamic Proxy - After executing '{0}'\"  , context.ServiceMethod.Name);\n           }\n             catch   (  Exception     ex  )\n           {\n               Logger.  LogError  (ex.  ToString  ());    // 記錄例外錯誤...\n                 throw  ;\n           }\n       }\n   }  Step5:Configure the DI (Dependency Injection) and AspCore proxy in Startup  A. In Configure, set up Services and Repository  Set up Service and Repository DI configurations in Startup     services.  AddTransient  <  IRepository  <  Customer  >,   Repository  <  Customer  >>();\n   services.  AddTransient  <  ICustomerService  ,   CustmoerService  >();  B.Set up the dynamic proxy  Set up DynamicProxy DI configurations in Startup.     services.  ConfigureDynamicProxy  (  config     =>   { config.Interceptors.  AddTyped  <  ServiceInterceptor  >(Predicates.  ForService  (  \"*Service\"  )); });  Common proxy rule configurations   Everything will be proxied:     config.Interceptors.  AddTyped  <  ServiceInterceptor  >();   Services with a suffix of \"Service\" will be proxied:     config.Interceptors.  AddTyped  <  ServiceInterceptor  >(Predicates.  ForService  (  \"*Service\"  )   Methods with a prefix of \"Execute\" will be proxied:     config.Interceptors.  AddTyped  <  ServiceInterceptor  >(Predicates.  ForMethod  (  \"Execute*\"  ));   Services under the namespace \"App1\" will not be proxied:     config.NonAspectPredicates.  AddNamespace  (  \"App1\"  );   Services in namespaces that have \"App1\" as the last segment will not be proxied:     config.NonAspectPredicates.  AddNamespace  (  \".App1\"  );   \"ICustomService\" will not be proxied:     config.NonAspectPredicates.  AddService  (  \"ICustomService\"  );   Services with a suffix of \"Service\" will not be proxied:     config.NonAspectPredicates.  AddService  (  \"Service\"  );   Methods named \"Query\" will not be proxied:     config.NonAspectPredicates.  AddMethod  (  \"Query\"  );   Methods with a suffix of \"Query\" will not be proxied:     config.NonAspectPredicates.  AddMethod  (  \"*Query\"  );  AspectCore also provides the NonAspectAttribute to prevent services or methods from being proxied. Simply add   NonAspect  to the method on the interface, and this method of the service will be ignored and not proxied.     public     interface     IXXXService\n   {\n     [  NonAspect  ]\n     void XXXMethod;\n   }  C. In program.cs, add UseServiceProviderFactory at the CreateHostBuilder location.  In Program.cs, at the CreateHostBuilder section, add UseServiceProviderFactory(new DynamicProxyServiceProviderFactory()) to delegate the default DI (Dependency Injection) handling to AspectCore.     public     static     IHostBuilder     CreateHostBuilder  (  string  []   args  )   =>\n      Host.  CreateDefaultBuilder  (args)\n          .  ConfigureWebHostDefaults  (  webBuilder     =>\n          {\n              webBuilder.  UseStartup  <  Startup  >();\n          })\n          .  UseServiceProviderFactory  (  newDynamicProxyServiceProviderFactory  ());  Step 6: Inject ICustomerService into HomeController, and add a service action in the Privacy API.  To easily demonstrate the AspCore service interception functionality, we add the AddCustomer service action to the Privacy API of the HomeController in the .Net Core Web initial project. When a user clicks on the Privacy tab, it will call the CustomerService function. The interceptor will intercept this call, print a log first, and then execute the service method.     private     readonly     ILogger  <  HomeController  >   _logger  ;\n   private     readonly     ICustomerService     _repoService  ;\n     public     HomeController  (  ILogger  <  HomeController  >   logger  ,   ICustomerService     repoService  )\n   {\n      _logger   =   logger;\n      _repoService   =   repoService;\n   }\n     public     IActionResult     Privacy  ()\n   {\n                 var     customer     =     new     Customer\n               {\n                   Id   =     1  ,\n                   Name   =     \"Customer 1\"  ,\n                   Address   =     \"Address 1\"\n               };\n                 _repoService.  AddCustmoer  (customer);\n                   return     View  ();\n   }  Step 7: Execute the test.      Summary  This article roughly organizes the use cases and methods of AOP and also provides a simple demonstration of how to use AspCore. It is hoped that those who have never heard of or used AOP can quickly gain an understanding of the concept.   Code Demo  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}.ct-086898{color:#6A737D;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}"},{"id":"content:0.code:6.FP(Functional programming)簡易釐清 (含Currying 與 Closure 理解).md","path":"/code/fp(functional-programming)-(currying-closure-)","dir":"code","title":"A Simplified Clarification of Functional Programming (Including Currying and Closure)","description":"","keywords":["Differences between OOP and FP in Pure Data Processing Scenarios","Closures and Currying in Functional Programming","Notes"],"body":"  A Simplified Clarification of Functional Programming (Including Currying and Closure)  tags:   Code Sense    Design-Pattern  I was recently watching a YouTube video on   Currying  when a comment piqued my interest in the role of closures in functional programming. As an OOP developer who hasn't done much FP in years, I decided to revisit these concepts. This post is a personal reflection on my understanding of Currying and closures, and an attempt to clarify the differences between OOP and FP, especially from the perspective of someone who has been primarily focused on object-oriented programming.  Differences between OOP and FP in Pure Data Processing Scenarios  OOP excels in designing real-world architectures using objects and design patterns. However, FP seems to be more prevalent in data processing applications, often presented in the form of APIs. This is my initial understanding, and I aim to delve deeper into this topic through further research.  Mathematical and Scientific Computing  A brief exploration of the literature reveals that FP draws its concepts and philosophy from a mathematical logic system known as lambda calculus. In lambda calculus, everything is a function, and the only operation is application. This simple and unified framework aligns closely with the mathematical concept of functions, aiding in the construction of concise, predictable, and easily reasoned programs.   Purity: In mathematics, the value of a function depends solely on its inputs, independent of any external state. This aligns with the concept of functions in lambda calculus and forms the foundation of \"pure functions\" in functional programming.  First-class functions: In mathematics, functions can be passed as inputs to other functions and can also be returned as outputs from other functions. This aligns with the concept of functions in lambda calculus and is the basis for the \"functions as first-class citizens\" concept in functional programming.  Therefore, it can be said that FP was deeply influenced by mathematics from its inception. However, this does not mean that functional programming is only suitable for mathematical problems. In fact, functional programming concepts such as immutability,   pure functions , and   function composition  are useful in many different application contexts and domains.  Those who are more familiar with OOP might feel that these characteristics can also be implemented conceptually in OOP. However, in scenarios where data needs to be processed in large-scale parallel and asynchronous operations, FP's immutability and statelessness can offer significant advantages.  Another simpler example to illustrate the difference in data applications is as follows. Suppose we have a task to square each element in a dataset (e.g., a list of numbers). In an object-oriented implementation, we might create a DataSet class and provide a method for this class to perform the squaring operation:     class     DataSet  :\n         def     __init__  (self, data):\n             self  .data   =   data\n           def     square  (self):\n             for   i   in     range  (  len  (  self  .data)):\n                 self  .data[i]   =     self  .data[i]   **     2  The context is as follows:     data   =   DataSet([  1  ,   2  ,   3  ,   4  ,   5  ])\n   data.square()\n   print  (data.data)    # Outputs: [1, 4, 9, 16, 25]  The problem with this implementation is that whenever we need to perform a new operation on the data (such as taking the square root, logarithm, etc.), we need to add a new method to the DataSet class. If there are many types of operations, the DataSet class will become very bloated.  In FP design, we abstract these operations into functions, and these functions can be easily applied to each element in the dataset as follows:     def     square  (x):\n         return   x   **     2\n     data   =   [  1  ,   2  ,   3  ,   4  ,   5  ]\n   data   =     list  (  map  (square, data))\n     print  (data)    # Outputs: [1, 4, 9, 16, 25]  In this scenario, when we need to add a new operation, we simply add a new function without modifying the data structure. Moreover, these functions can be reused for any type of dataset, not just our defined DataSet class. Therefore, for situations that require the same operations on a large amount of data, the functional programming style often provides greater flexibility and reusability.  You might think this example is a bit contrived and wonder why anyone would design objects for this application. You're absolutely right! FP tends to express computation as the evaluation of mathematical functions, focusing more on data manipulation and transformation rather than encapsulating data and behavior into objects as in object-oriented programming and describing the computational process using object interactions.  This doesn't mean that FP is unsuitable for scenarios that require object design, but rather that they have different approaches to solving problems. In fact, in many modern languages, FP and OOP are both considered important parts of the language and can be used together in many scenarios.  In the aforementioned example, if these data were encapsulated in objects in a more complex system and the objects had their own behaviors and states, then using OOP might be more appropriate. However, when we need to process large amounts of data and perform various transformations and manipulations on this data, the higher-order functions like map, reduce, and filter provided by FP allow us to describe these operations more directly.  With this explanation, you should have a clearer understanding of the differences between FP and OOP in the context of mathematical and scientific computing applications.  Big data processing and analytics  The combination of functional programming (FP) and immutable data structures is highly effective in big data processing and analysis applications. For example, Apache Spark is an open-source cluster computing system for big data processing written in Scala, a language that supports FP, and leverages many FP concepts such as immutable datasets and higher-order functions.  Let's delve deeper into Apache Spark and the application of functional programming in big data processing.  Apache Spark is an open-source cluster computing system for large-scale data processing. It provides a high-level API for data manipulation called Resilient Distributed Dataset (RDD). An RDD is an immutable, distributed collection of elements of the same type.  In Spark, RDDs are the primary data structure, and all operations (such as map, filter, reduce) are performed on RDDs. Since RDDs are immutable, once created, they cannot be modified, allowing Spark to effectively track data lineage and changes in large-scale parallel computations.  Here's a simple example of a map-reduce operation using Spark's Python API (PySpark):     from   pyspark   import   SparkContext\n     sc   =   SparkContext(  \"local\"  ,   \"count app\"  )\n   nums   =   sc.parallelize([  1  ,   2  ,   3  ,   4  ,   5  ])\n     # Add one to each number\n   add_one   =   nums.map(  lambda   x: x   +     1  )\n     #  Calculate the sum of all numbers\n   sum     =   add_one.reduce(  lambda   a, b: a   +   b)\n     print  (  sum  )    # Output: 20  In this example, we first create an RDD (nums) and then use the map operation to increment each number in the RDD by one. Following that, we use the reduce operation to sum all the numbers. These operations are performed in the form of functions (lambda functions) and can be executed in parallel across multiple nodes automatically.  This is a practical application of functional programming in big data processing. By utilizing immutable data structures (RDDs) and higher-order functions (map, reduce, etc.), Spark enables the abstraction of large-scale data processing problems and provides an efficient, concise, and scalable solution.  If this example is too complex, let's consider a simpler one to understand the advantages of functional programming in big data processing. Suppose we have a dataset containing millions of records, each representing a person's personal information (e.g., name, age, address). Our task is to find all individuals who are over 18 years old.  In an object-oriented approach, we might handle this as follows:       class     Person  :\n         def     __init__  (self, name, age, address):\n             self  .name   =   name\n             self  .age   =   age\n             self  .address   =   address\n     people   =   [  ...  ]    # Assuming this is a huge list containing millions of Person objects.\n     adults   =   []\n   for   person   in   people:\n         if   person.age   >=     18  :\n           adults.append(person)  In functional programming, this task can be abstracted as a filtering operation.     # Assume this is a list containing millions of tuples, where each tuple represents a person's information.\n   people   =   [  ...  ]   \n     adults   =     filter  (  lambda   person: person[  1  ]   >=     18  , people)  In this example, the built-in filter function and a lambda function are used to achieve the goal. This approach is more concise and intuitive, and it can automatically leverage parallel processing for faster computation (e.g., when using a distributed computing framework like PySpark).  Speaking of which, when we look back at the example, FP designs are characterized by immutability and pure functions (where the output depends solely on the input). Therefore, they are particularly well-suited for data processing applications. However, the real world is not entirely pure. Operations like fetching data from a database and displaying it on the frontend involve many changes to state (different from data state). In these cases, OOP would be a better choice.  Closures and Currying in Functional Programming  Closures: Capturing State in Functions  While functional programming emphasizes immutability, some data manipulation processes still require the use of state. In such cases, closures provide a more concise and advanced way to achieve this. A closure is a special type of function that can access and manipulate variables that are outside its own scope, even when the function is called from outside its defining scope. This is especially useful when creating functions that need to remember a specific state. In Python, closures allow a function to \"remember\" the environment in which it was defined. For example:     def     make_multiplier  (x):\n         def     multiplier  (n):\n             return   x   *   n\n         return   multiplier\n     times3   =   make_multiplier(  3  )\n   print  (times3(  10  ))    # Outputs: 30  The multiplier(n) function in this example is a closure. Why is that? Because it \"remembers\" the environment where it was created. When we call make_multiplier(3), we're essentially creating a new multiplier(n) function. This function \"knows\" that x is 3 (and this state doesn't change), even though multiplier(n) is defined inside make_multiplier(x). When it's returned and assigned to the times3 variable, it's actually left the \"scope\" or \"environment\" of make_multiplier(x). In other words, the multiplier(n) function is being called outside of the make_multiplier(x) function.  To further emphasize this point, the multiplier(n) function is indeed being called outside of the make_multiplier(x) function.  In Python, when you define a function, that function has its own scope, meaning it can only directly access variables defined within itself. In the make_multiplier(x) function, the function can access the variable x because x is passed in as an argument.  Then, inside the make_multiplier(x) function, a new function multiplier(n) is defined. This new function can access n because n is passed in as an argument, but it can also access x, even though x is defined in the outer make_multiplier(x) function. This is because multiplier(n) is created within the scope of make_multiplier(x), so it has access to the scope of make_multiplier(x).  The multiplier(n) function is then returned as the return value of the make_multiplier(x) function. So when you call make_multiplier(3), you actually get a new function. When this new function is called, it will multiply the input value by 3.  When you assign the return value of make_multiplier(3) (which is the multiplier(n) function) to times3 and call times3(10), even though the multiplier(n) function is now being called outside of the make_multiplier(x) function, it still \"remembers\" the value of x (which is 3 in this case). When you call times3(10) (which is actually calling multiplier(10)), it knows to multiply 10 by 3 because it \"remembers\" that the value of x is 3.   This is what we call a closure: a function that remembers and can access variables from its outer scope (the x in our example), even when it's called outside the scope where it was created.  In other words, multiplier(n) remembers the environment it was created in, and it still has the ability to remember and access variables from its outer function (make_multiplier(x)).  Grasping the concept of Currying  Functional programming often involves composing functions, which can lead to more concise and readable code. Currying is a technique that transforms a function that takes multiple arguments into a sequence of functions that each take a single argument. For example, a function f(x, y) that takes two arguments can be curried into a function g(x) that takes one argument and returns another function. In this case, you can provide the first argument first (e.g., g(2)) and get a new function that takes the second argument and returns the final result (e.g., g(2)(3) returns the same result as f(2, 3)). Simply put, currying is a technique for transforming a multi-argument function into a sequence of single-argument functions.       def     multiply  (x):\n         def     multiply_x  (y):\n             return   x   *   y\n         return   multiply_x\n     double   =   multiply(  2  )\n   print  (double(  5  ))    # Outputs: 10  Transforming a function multiply that takes two arguments into a function multiply_x that takes one argument and returns another function, which also takes one argument. We can observe that multiply_x is also a closure because it remembers the value of x. Although this example appears similar to a closure, there are still some differences in their applications... let's continue!  Distinctions in application scenarios between Currying and Closure  Let's summarize:  Closure: The primary purpose of a closure is to \"remember\" variables from its outer function. Even when the inner function is returned and used elsewhere, it can still access and manipulate those variables, even if the original outer function has finished executing. This can be used to create stateful functions, meaning their behavior is influenced by their \"environment\". In the aforementioned example, multiplier(n) is a closure.  Currying: The main purpose of currying is to transform a function that takes multiple arguments into a sequence of functions that each take a single argument. This allows for more flexible use of functions, especially when functions are passed as arguments. In your second example, multiply(x) has been curried.  Imagine we're developing a game and want to count the player's score. We can use closures to implement this:     def     create_score_counter  ():\n       score   =     0\n         def     add_score  (points):\n             nonlocal   score\n           score   +=   points\n             return   score\n         return   add_score\n     counter   =   create_score_counter()\n   print  (counter(  10  ))    # Outputs: 10\n   print  (counter(  20  ))    # Outputs: 30  The create_score_counter function returns an add_score function, which is a closure that remembers and modifies the score variable in its outer environment. Even after the create_score_counter function has finished executing, the add_score function can still access and modify the score variable.  In this example, a closure is a more intuitive and simpler approach to achieve the requirement. We need to record and update a \"state\" (score), and this state needs to be preserved across consecutive function calls. Closures allow us to bundle the state (score in this case) with the function that operates on the state (add_score), and this state will be \"remembered\" across subsequent function calls.  (Currying) is primarily used to transform a function that takes multiple arguments into a sequence of functions that each take a single argument. It is useful when we need to \"fix\" some of the arguments (or default parameters) of a function, and the remaining arguments are provided later. However, in this example, our primary need is to \"record and update state\", not just to \"fix some arguments\".    In another scenario, suppose we are processing a list, and we want to apply a function to each element in the list. We can use currying to create a function that takes a function and a list as arguments, and then returns a new function that takes an element and applies the function we passed in earlier:     def     map_function  (func):\n         def     apply_func_to_list  (lst):\n             return   [func(x)   for   x   in   lst]\n         return   apply_func_to_list\n     double   =     lambda   x: x   *     2\n   map_double   =   map_function(double)\n     print  (map_double([  1  ,   2  ,   3  ,   4  ,   5  ]))    # Outputs: [2, 4, 6, 8, 10]  The map_function is essentially performing currying: it takes a function as an argument and returns a new function, apply_func_to_list. This new function accepts a list as an argument and applies the function passed to map_function to each element in the list.  This example does not require a \"state\" that needs to be maintained and updated. The map_function in this example can be seen as a curried function: it first takes a function func and then returns a new function apply_func_to_list, which takes a list lst and applies func to each element of lst.  This characteristic of receiving function arguments in stages allows you to fix some parameters in advance (in this case, func) and generate a new function to handle subsequent parameters (in this case, lst). This feature can make code more concise and flexible in certain situations. In contrast, closures are a concise way to manage and update state.  After reading this, you should have a clearer understanding of FP design concepts, as well as closures and currying. However, to truly grasp these concepts, you need to apply them in practical, context-specific design scenarios.  Notes  currying can also be implemented in C# using delegates, as shown below:     public     static     void     Main  ()\n   {\n         Func  <  int  ,   Func  <  int  ,   int  >>   curriedMultiply     =     MultiplyCurried  ();\n         Func  <  int  ,   int  >   multiplyBy2     =     curriedMultiply  (  2  );\n           int     result     =     multiplyBy2  (  3  );   // result will be 6\n       Console.  WriteLine  (result);\n   }\n     static     Func  <  int  ,   Func  <  int  ,   int  >>   MultiplyCurried  ()\n   {\n         return     a     =>     b     =>   a   *   b;\n   }  Although it can be implemented, the primary design of delegates is for event handling, asynchronous calls, and callback functions, which differs from the application of closures in the data world. In the realm of data analysis and processing, the primary purpose of closures is to \"remember\" variables from outer functions and manipulate them within the closure. This is particularly useful for scenarios where internal state needs to be maintained and updated. For example, when performing statistical calculations or counting, closures can make our code more concise and easier to understand.  Delegates, on the other hand, are applied in general software design areas such as event handling, asynchronous calls, and callback functions. A delegate is essentially an object that holds a reference to a function or method. When a delegate is invoked, it can call the function or method it references. This allows for dynamic changes to function or method behavior and makes code more flexible at runtime.  I feel that I can have a more comprehensive understanding of FP. Whenever I hear people say there are two opposing beliefs, I think it's because they don't have a complete understanding of OOP and FP, or they lack sufficient experience in applying them in appropriate scenarios, which leads to a bias towards one or the other.  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}.ct-086898{color:#6A737D;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}"},{"id":"content:0.code:7.var使用灌輸.md","path":"/code/var","dir":"code","title":"A deep dive into the usage of \"var\"","description":"I'm used to using 'var' in C# and found it very convenient. However, I know many people are against it because the implicit typing can significantly reduce code readability. If 'var' is really that bad, I don't think Java and .NET, two strongly-typed languages, would have adopted it. So I've done some research on this topic.","keywords":["一、A brief explanation of \"var\"","二、Exploring usage scenarios","三、Summary of var Usage:"],"body":"  A deep dive into the usage of \"var\"  I'm used to using 'var' in C# and found it very convenient. However, I know many people are against it because the implicit typing can significantly reduce code readability. If 'var' is really that bad, I don't think Java and .NET, two strongly-typed languages, would have adopted it. So I've done some research on this topic.  一、A brief explanation of \"var\"  The var keyword is used for implicit type declaration. Essentially, when you use var, you let the compiler infer the type of the variable. For example:     var     i     =     10  ;   // Inferred as int\n   var     s     =     \"hello\"  ;   // Inferred as string  Why use var instead of explicit types? The primary design intent of var was to simplify code and improve readability, especially in constructs like LINQ where it can reduce verbosity. However, the appropriateness of var can be context-dependent, as will be explored in subsequent sections.  二、Exploring usage scenarios  This chapter will delve into the suitable and unsuitable contexts for the application of var.  a. Simplicity  Imagine you're using a long and complex type name, like Dictionary<int, List  >. If you had to write out this entire type name every time you declared a variable, your code would quickly become cluttered and time-consuming. The same applies to long object declarations, such as Google Cloud's KMS Package KeyManagementServiceClient.  I.A suitable application of LINQ  Let's take .NET Core LINQ as an example to illustrate the benefits of using var for complex types. Imagine you have a list named users, where each element is an instance of a User class. You want to select all users whose age is greater than 20.  Without using var, you would need to write:     IEnumerable  <  User  >   usersAbove20     =   users.  Where  (  u     =>   u.Age   >     20  );  However, if your query becomes more complex and the return type is more intricate, for instance, you want to extract the names and ages of these users and then sort them. In this case, you would need to write:     IOrderedEnumerable  <  AnonymousType1  >   result     =   users.  Where  (  u     =>   u.Age   >     20  )\n                                                    .  Select  (  u     =>     new   { u.Name, u.Age })\n                                                    .  OrderBy  (  u     =>   u.Age);  In contrast, using var simplifies this significantly:     var     result     =   users.  Where  (  u     =>   u.Age   >     20  )\n                     .  Select  (  u     =>     new   { u.Name, u.Age })\n                     .  OrderBy  (  u     =>   u.Age);  In this code snippet, it will be an anonymous type as follows:    An anonymous type is a type without an explicit name. It's primarily used to store temporary, simple data structures. In the LINQ world, when you need to extract specific information from a complex data structure but don't want to define a new class, you often use var, making the operation much simpler. The key point here is that it's ideal for data transition operations, especially when you don't need to store or reuse a specific data structure for an extended period.  Let's take another example: a task that involves processing data from various sources (databases, APIs, user input, etc.). You'll need to work with multiple types, collections, and use lambda expressions with var.     var     query     =   dataContext.Customers\n                 .  Where  (  c     =>   c.Age   >     18  )\n                 .  SelectMany  (  c     =>   c.Orders, (  c  ,   o  )   =>     new   { c.Name, o.OrderId, o.Amount })\n                 .  GroupBy  (  x     =>   x.Name)\n                 .  Select  (  g     =>     new   { Customer   =   g.Key, TotalAmount   =   g.  Sum  (  x     =>   x.Amount) });  Without using var, it would look like this:     IEnumerable  <  IGrouping  <  string  ,   anonymousType  <  string  ,   int  ,   decimal  >>>   query     =   \n         dataContext.Customers\n                    .  Where  (  c     =>   c.Age   >     18  )\n                    .  SelectMany  (  c     =>   c.Orders, (  c  ,   o  )   =>     new   { c.Name, o.OrderId, o.Amount })\n                    .  GroupBy  (  x     =>   x.Name)\n                    .  Select  (  g     =>     new   { Customer   =   g.Key, TotalAmount   =   g.  Sum  (  x     =>   x.Amount) });  In this example, not using var makes the type very explicit, but at the cost of making the code overly verbose and difficult to read.  II.Good use cases for overly long object names:  Lambda syntax sugar is ubiquitous in many languages. Another common scenario involves var being used to declare variables with lengthy object names. For instance, consider the following example of an asymmetric decryption method encapsulated in Google Cloud KMS:  In this example, the object naming conventions clearly delineate the process:   Creating a keyManagementServiceClient  Generating a cryptoKeyVersionName  Decoding cryptoKeyVersionName into bytes  Initiating asymmetric decryption  Returning the result     @Override\n       public     String     asymmetricDecrypt  (  KeyInfoDto     keyInfo  ,   String     ciphertext  ) throws IOException {\n         try   (KeyManagementServiceClient keyManagementServiceClient = KeyManagementServiceClient.create()) {\n              // Generate the crypto key version name based on the provided key information\n           CryptoKeyVersionName     cryptoKeyVersionName      =     generateCryptoKeyVersionName  (keyInfo);\n              // Decode the base64-encoded ciphertext into a byte array\n           byte  []   decryptedBytes     =     decodeBase64ToBytes  (ciphertext);\n              // Perform asymmetric decryption using the specified crypto key version\n           AsymmetricDecryptResponse     asymmetricDecryptResponse     =   keyManagementServiceClient.  asymmetricDecrypt  (cryptoKeyVersionName, ByteString.  copyFrom  (decryptedBytes));\n             // Convert the decrypted bytes to a UTF-8 string and return it\n           return   asymmetricDecryptResponse.  getPlaintext  ().  toStringUtf8  ();\n       }\n     }  If we were to replace the code with var, we could ask ourselves: does this affect readability? Actually, no. On the contrary, for declaring type objects, it seems more concise. However, the key point is that as long as the naming is clear, long object names won't hinder readability. Unless you're unsure about the type of a particular part of the flow and want to understand it, you can use your IDE to hover over it for more information.     @Override\n       public     String     asymmetricDecrypt  (  KeyInfoDto     keyInfo  ,   String     ciphertext  ) throws IOException {\n         try   (var keyManagementServiceClient = KeyManagementServiceClient.create()) {\n             // Generate the crypto key version name based on the provided key information\n           var     cryptoKeyVersionName      =     generateCryptoKeyVersionName  (keyInfo);\n             // Decode the base64-encoded ciphertext into a byte array\n           byte  []   decryptedBytes     =     decodeBase64ToBytes  (ciphertext);\n             // Perform asymmetric decryption using the specified crypto key version\n           var     asymmetricDecryptResponse     =   keyManagementServiceClient.  asymmetricDecrypt  (cryptoKeyVersionName, ByteString.  copyFrom  (decryptedBytes));\n             // Convert the decrypted bytes to a UTF-8 string and return it\n           return   asymmetricDecryptResponse.  getPlaintext  ().  toStringUtf8  ();\n       }\n     }  I think the above example doesn't strongly convey the point. Let's consider a code snippet for RSA encryption using a specified DER-formatted public key (derKeyBytes). In this example, we'll use var throughout the entire process:   Reading the public key: x509EncodedKeySpec = new X509EncodedKeySpec(derKeyBytes); This line reads the DER-formatted public key data.  Generating an RSA public key object: Using the public key data, an RSA public key object is created.  Initializing the cipher: A specific RSA encryption mode and padding scheme is chosen, determined by Cipher.getInstance(\"RSA/ECB/OAEPWithSHA-256AndMGF1Padding\");.  Setting parameters: In this example, OAEP (Optimal Asymmetric Encryption Padding) and SHA-256 are used as the hash algorithm.  Encryption: rsaCipher.doFinal(plaintext.getBytes(StandardCharsets.UTF_8)); This line converts the plaintext to a byte array and encrypts it.  Returning the result: The encrypted data is converted to a Base64-encoded string and returned.  As you can see, even with var, readability is not compromised, especially when names are chosen well. In fact, it simplifies the declaration of lengthy object names, making the code more concise and easier to read.     /**\n      * Encrypts the given plaintext using the specified DER-encoded public key.\n      *\n      * @param plaintext    The plaintext to be encrypted.\n      * @param derKeyBytes  The DER-encoded public key as a byte array.\n      * @return             The Base64-encoded ciphertext.\n      * @throws InvalidKeySpecException  If there is an error in the key specification.\n      */\n       private     String     encryptUsingPublicKey  (  String     plaintext  ,   byte  []   derKeyBytes  ) throws InvalidKeySpecException, NoSuchAlgorithmException, NoSuchPaddingException, InvalidKeyException, InvalidAlgorithmParameterException, IllegalBlockSizeException, BadPaddingException {\n         var     x509EncodedKeySpec     =     new     X509EncodedKeySpec  (derKeyBytes);\n            // Generate an RSA public key object from the DER-encoded key\n         var     rsaPublicKey      =   KeyFactory.  getInstance  (  \"RSA\"  ).  generatePublic  (x509EncodedKeySpec);\n           // Initialize the cipher with RSA/ECB/OAEPWithSHA-256AndMGF1Padding mode\n         var     rsaCipher     =   Cipher.  getInstance  (  \"RSA/ECB/OAEPWithSHA-256AndMGF1Padding\"  );\n         var     oaepParameters     =     new     OAEPParameterSpec  (  \"SHA-256\"  ,   \"MGF1\"\n               , MGF1ParameterSpec.SHA256, PSource.PSpecified.DEFAULT);\n       rsaCipher.  init  (Cipher.ENCRYPT_MODE, rsaPublicKey , oaepParameters );\n            // Encrypt the plaintext using the initialized cipher\n         byte  []   encryptedData      =   rsaCipher.  doFinal  (plaintext.  getBytes  (StandardCharsets.UTF_8));\n         return     encodeBase64ToString  (encryptedData);\n     }  For comparison, let's look at a control group that uses strong typing. By reading the following code, you'll immediately feel that it's slightly harder to read compared to the previous example.     /**\n      * Encrypts the given plaintext using the specified DER-encoded public key.\n      *\n      * @param plaintext    The plaintext to be encrypted.\n      * @param derKeyBytes  The DER-encoded public key as a byte array.\n      * @return             The Base64-encoded ciphertext.\n      * @throws InvalidKeySpecException  If there is an error in the key specification.\n      */\n       private     String     encryptUsingPublicKey  (  String     plaintext  ,   byte  []   derKeyBytes  ) throws InvalidKeySpecException, NoSuchAlgorithmException, NoSuchPaddingException, InvalidKeyException, InvalidAlgorithmParameterException, IllegalBlockSizeException, BadPaddingException {\n         X509EncodedKeySpec     x509EncodedKeySpec     =     new     X509EncodedKeySpec  (derKeyBytes);\n           // Generate an RSA public key object from the DER-encoded key\n         PublicKey     rsaPublicKey      =   KeyFactory.  getInstance  (  \"RSA\"  ).  generatePublic  (x509EncodedKeySpec);\n            // Initialize the cipher with RSA/ECB/OAEPWithSHA-256AndMGF1Padding mode\n         Cipher     rsaCipher     =   Cipher.  getInstance  (  \"RSA/ECB/OAEPWithSHA-256AndMGF1Padding\"  );\n         OAEPParameterSpec     oaepParameters     =     new     OAEPParameterSpec  (  \"SHA-256\"  ,   \"MGF1\"\n               , MGF1ParameterSpec.SHA256, PSource.PSpecified.DEFAULT);\n       rsaCipher.  init  (Cipher.ENCRYPT_MODE, rsaPublicKey , oaepParameters );\n           // Encrypt the plaintext using the initialized cipher\n         byte  []   encryptedData      =   rsaCipher.  doFinal  (plaintext.  getBytes  (StandardCharsets.UTF_8));\n         return     encodeBase64ToString  (encryptedData);\n     }  III. Negative use cases:  After discussing the benefits of concise code using var, let's explore some inappropriate use cases. Consider the following test code snippet for symmetric and asymmetric encryption/decryption: we want to compare encrypt and decrypt. In this scenario, if we use var, it becomes ambiguous whether we are comparing String or byte  . In contexts where data types need to be explicitly defined, using var is not suitable.     // Set a test plaintext string\n   String     testPlaintext     =  \"PAL\"  ;\n   // Create a KeyInfoDto object to hold key information\n   var     keyInfoDto     =     new     KeyInfoDto  ();\n     keyInfoDto.  setProjectId  (  \"affable-cacao-389805\"  );\n   keyInfoDto.  setLocationId  (  \"asia-east1\"  );\n   keyInfoDto.  setKeyRingId  (  \"cathy-sample-project\"  );\n   keyInfoDto.  setKeyVersion  (  \"1\"  );\n     // Test symmetric encryption and decryption using the KMS service\n   // Set the key ID to use for encryption and decryption\n   keyInfoDto.  setKeyId  (  \"kms-sdk-testing\"  );\n   // Encrypt the plaintext using the specified key\n   var     encrypt     =   kmsService.  symmetricEncrypt  (keyInfoDto,testPlaintext);\n   // Decrypt the ciphertext using the same key\n   var     decrypt     =   kmsService.  symmetricDecrypt  (keyInfoDto,encrypt);\n   // Verify that the decrypted text matches the original plaintext\n   Assert.  hasText  (decrypt,testPlaintext);  Therefore, in certain situations, declaring variables with var is not appropriate. For example,   Cross-platform or multilingual integration:   If your backend microservices need to interact with other platforms or services written in different languages, explicit typing makes it easier for other developers to understand the code.     // It's inappropriate to use 'var' because other developers or platforms need to know the exact data type.\n   HttpResponse  <  String  >   response     =   client.  send  (request, HttpResponse.BodyHandlers.  ofString  ());  Easily confused types:   Using \"var\" can lead to confusion when handling two or more similar but not identical types.     // Explicit typing can prevent confusion between BigDecimal and BigInteger.\n   BigDecimal     bigDecimal     =     new     BigDecimal  (  \"10.0\"  );\n   BigInteger     bigInteger     =     new     BigInteger  (  \"10\"  );  Mathematical operation:   Using \"var\" can also cause problems when performing mathematical calculations. For example:     // Is it an int or a double?\n   var     result     =     10     /     2  ;  Unclear initialization value   When the initial value doesn't provide enough type information, using \"var\" can make the type ambiguous.     // Should I use List<String> or ArrayList<String>, or something else?\n   var     names     =   Arrays.  asList  (  \"Alice\"  ,   \"Bob\"  );  Unit testing and data validation, exemplified by encrypt and decrypt functions   Explicitly defined variable types in unit tests enhance code readability and make it easier to understand the expected behavior of the code under test.     // It is required to assert that the getResult method returns a List object in our test case.\n   List  <  String  >   result     =   someObject.  getResult  ();  foreach   Using var in a foreach loop can be less readable, particularly when the collection contains elements of different types or when the code is complex.     List  <  DetailedOrder  >   orders     =     GetOrders  ();\n     // Use explicit typing\n   foreach   (  DetailedOrder     order     in   orders)\n   {\n         // ...\n   }\n     // use var\n   foreach   (  var     order     in   orders)\n   {\n         // ...\n   }  Therefore, in situations where explicit data types are required to enhance code readability and maintainability, especially when dealing with cross-platform interactions, ambiguous types, mathematical operations, unclear initialization values, unit testing, and data validation, using   var  is often not the best choice.  b. Follow guidelines for using meaningful names  When you use var to declare a variable, the variable's type isn't explicitly stated in the code. To help both yourself and others understand the variable's purpose more quickly, you're encouraged to give it a more descriptive name.     List  <  Student  >   list     =     new     List  <  Student  >();  Here, you might simply name the variable    list  since the type    List<Student>  already tells you that it's a list of students.  However, if you use    var :     var     students     =     new     List  <  Student  >();  At this point, you might opt for a more descriptive name like students to clearly indicate that this variable is used to store student data. Even without the explicit type in the code, the variable name students immediately conveys its purpose.  However, the effectiveness of using var largely depends on the developer's adherence to clean code principles. If a developer doesn't understand clean code practices, using var can lead to significant code readability issues.  三、Summary of var Usage:    Code Conciseness :    var  can make code more concise, especially when dealing with long type names.   Type Inference :    var  is necessary for anonymous types, such as Linq query results.   Readability : In some cases, avoiding repetitive type names can improve code readability.   Ambiguous Types : If the initialization expression doesn't clearly indicate the type, using    var  can lead to confusion about the variable's actual type.   Overuse : Excessive use of    var  without considering the context can make code harder to maintain and understand.  Using    var  can enhance code readability and flexibility, particularly when handling anonymous or dynamic types. However, developers should use var judiciously and ensure that the code's intent remains clear.  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}.ct-086898{color:#6A737D;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}"},{"id":"content:0.index.md","path":"/","dir":"","title":"Home","description":"","keywords":[],"body":"     Hakuna Matata.   I am a dedicated learner, and together with my friend, we have formed a team called Code Sense in Kaohsiung. We engage in research and study on a weekly basis, with a passion for learning new technologies and skills.Continual learning is the driving force of my life.     8-9 years of software development and maintenance experience  3-4 years of experience in automation domain development  1-2 years of experience in department software training and management  Familiar with integration and connection of various factory systems (  MES ,   WMS ,   PLC )  Proficient in web system front-end and back-end development and database design  Well-versed in measurement instrument and motor PC Base control (  RS232 ,   ModBusTCP ,   EtherCAT )      My Personal Moments.      Reading Blog   Regular Reading and Public Journal of Thoughts and Moods.[  Medium ]    Code Sense Trello   Regular Study and Discussion Sessions with Friends.\n[  Code Sense Trello ]    Slider   My Presentation Slides.[  Slider Link ]    Technical Documentation   Regular document writing and temporary storage.[  Hackmd ]"},{"id":"content:2.desktop:1.WindowsForm找不到類型xxxx上的建構涵式.md","path":"/desktop/windowsformxxxx","dir":"desktop","title":"WindowsForm cannot find the constructor on type xxxx.","description":"","keywords":["Context","Error Message","Solution"],"body":"  WindowsForm cannot find the constructor on type xxxx.  Context  Recently, when designing desktop pages, I encountered several scenarios where the underlying logic of the pages was the same. So I specifically set up a Base Page for the User Controls (UCs) to inherit from. Since the pages are actually very similar, I finally decided not to use reference and directly use inheritance.  Let A (UC_3Dswitch_CalibrationFileManagement) inherit from B (UC_3Dswitch_FileManagementBase).  Error Message  There are no compilation errors, but when I switch to Design mode, I encounter the following \"Type not found\" error.    Solution    Declares a parameterless constructor and an injected constructor       // Declares a parameterless constructor\n   public     UC_3Dswitch_FileManagementBase  ()\n   {\n     }\n   public     UC_3Dswitch_FileManagementBase  (  AppSetting     appSetting  ) \n   {\n       ProductLineDataPath   =   appSetting.ProductLineDataPath;\n       SNFolderNameLength   =   appSetting.SNFolderNameLength;\n         InitializeComponent  ();\n   }  Due to the injected AppSetting object in the original Base setting, the aforementioned loading error occurred. It seems that the Designer mode, when used with Control objects, defaults to setting an empty constructor.  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-086898{color:#6A737D;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}"},{"id":"content:4.database:1.MSSQL使用指令測試Server硬碟速度小技巧.md","path":"/database/mssqlserver","dir":"database","title":"Quick Tips for Testing Server Disk Read Speed Using MSSQL Commands","description":"","keywords":["Read Speed","Write Speed"],"body":"  Quick Tips for Testing Server Disk Read Speed Using MSSQL Commands  Read Speed  Select a database and execute the BACKUP DATABASE command. By setting the backup to read-only (no write operations), you can obtain the read speed value.     BACKUP     DATABASE   [FUXIN_CPL]   TO     DISK     =  'NULL'     WITH     COPY_ONLY    The following figure shows the result: a read speed of 180 MB/sec.    Write Speed     BACKUP     DATABASE   [FUXIN_CPL]   TO     DISK     =  'C:\\TEST.BAK'     WITH     COPY_ONLY  You'll get a read/write speed of 153 megabytes per second at this point.    The read/write speed is 153 MB per second. Given that there are 688 pages, each 8KB in size (totaling 688 * 8.0 / 1024 = 5.375 MB), and the write time difference is 0.035 - 0.03 = 0.005 seconds, we can calculate the write time.  5.375M/0.005 = 1075M  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}"},{"id":"content:4.database:2.EF Query追蹤議題.md","path":"/database/ef-query","dir":"database","title":"Issues with large-scale create processing time and query tracking.","description":"","keywords":["一、Performance bottlenecks in bulk data creation","二、Query tracking issue"],"body":"  Issues with large-scale create processing time and query tracking.  一、Performance bottlenecks in bulk data creation  a.Context  We faced a performance bottleneck when inserting approximately 15,000 records using Entity Framework. Due to the cost associated with EF Bulk Insert, we opted for the standard DbContext.Add approach, which resulted in an insertion time of nearly 30 seconds.   Root Casue  When modifying EF POCO entities, the DetectChanges() method is invoked to compare the new and old values of all properties in the entry set. This can lead to significant performance degradation when dealing with large datasets. The following methods will trigger DetectChanges():   DbSet.Find  DbSet.Local  DbSet.Add  DbSet.AddRange  DbSet.Remove  DbSet.RemoveRange  DbSet.Attach  DbContext.SaveChanges  DbCoNtext.GetValidationErrors  DbContext.Entry  DbChangeTracker.Entries  b.Solution  Based on the aforementioned reasons, there are two primary methods to reduce the creation time for large datasets:   Using AddRange :   By employing AddRange to insert all data in a single operation, we can significantly reduce the time spent invoking DetectChanges() for each individual Add operation.  Setting Configuration.AutoDetectChangesEnabled to false :   Disabling automatic change detection by setting AutoDetectChangesEnabled to false before adding entities, and then re-enabling it afterward, can also improve performance.  c.Test Result  The actual test results for accessing 12,544 records are as follows:   Add : 34s  AddRange : 13s  AutoDetectChangesEnabled fasle : 13s  二、  Query tracking issue  To address the long processing time for bulk creation operations in EF, I also investigated the query aspect. I found that EF queries have a default tracking design. Here's a brief testing code snippet:    The operation steps are as follows:  Step 1: Context 1 retrieves Mario's data.\nStep 2: Context 2 adds a new Test record.\nStep 3: Context 1 modifies Mario's data to Jack.  The results are as follows:    It's observed that Context1 detects an increase in the number of records, but Context2 still reads the old value for the Name.  Since Context2 fetches data from the cache, the second query doesn't hit the database, resulting in the retrieval of the old value. If we modify the second query in Context2 to...     dbContext2.TestRecord.AsNoTracking().SingleOrDefault(x => x.Id == 1).Name  Under these circumstances, the retrieved value will be the updated Name \"Jack\" as modified by Context1. Consequently, in read-only contexts, employing untracked queries can enhance query efficiency. (Empirical verification pending)  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}"},{"id":"content:50.Tool:1.SpecFlow Simple Usage.md","path":"/tool/specflow-simple-usage","dir":"tool","title":"SpecFlow Usage","description":"I need to share about TDD (Test-Driven Development) soon, and it reminded me of my notes from a BDD (Behavior-Driven Development) course I took. The course taught the concept of BDD and included some hands-on practice. However, for the C# implementation, it didn't utilize SpecFlow to edit DSL (Domain-Specific Language) syntax to map to the output code. So, I decided to give it a try on my own. There are quite a few articles about its usage online, but they mostly cover the simplest contexts. Coincidentally, the course provided an example that was a bit closer to real-world implementation for me to experiment with. I encountered some issues along the way, so I thought I'd jot down a brief record.","keywords":["BDD","Practice using SpecFlow  (Test Context)","Points to Note"],"body":"  SpecFlow Usage  I need to share about TDD (Test-Driven Development) soon, and it reminded me of my notes from a BDD (Behavior-Driven Development) course I took. The course taught the concept of BDD and included some hands-on practice. However, for the C# implementation, it didn't utilize SpecFlow to edit DSL (Domain-Specific Language) syntax to map to the output code. So, I decided to give it a try on my own. There are quite a few articles about its usage online, but they mostly cover the simplest contexts. Coincidentally, the course provided an example that was a bit closer to real-world implementation for me to experiment with. I encountered some issues along the way, so I thought I'd jot down a brief record.  BDD  Simple Concept  BDD focuses on the behavior and requirements of software throughout the development process. It emphasizes the use of natural language (close to human-understandable language) to describe the behavior that software should exhibit. This approach enables developers, testers, and non-technical stakeholders (such as product managers and business analysts) to clearly understand the requirements.  The difference between BDD and TDD  TDD  For TDD, or Test-Driven Development, let's say we're adding a feature to a calculator program to perform addition:   Write a test case to simulate the addition feature (for example, testing that 2 + 3 equals 5).  Write the actual addition functionality in the program.  Run the test.  If the test fails (which it likely will initially), refactor the code.  Run the test again. Continue the cycle of testing and refactoring until the test passes.  This TDD cycle ensures that the addition functionality works as expected before moving on to develop other features, ensuring each part of the program is tested and functional in isolation.  BDD  For BDD, or Behavior-Driven Development, the process focuses on the expected behavior of the addition functionality in a calculator program, described in natural language. Here’s how it might look:   Describe the expected behavior of the addition functionality in natural language. For example: \"When the user inputs two numbers and presses the 'add' button, the application should display the sum of these two numbers.\"  Write functional tests to check the addition feature, typically using a BDD testing framework (like Cucumber or SpecFlow) that allows behavior descriptions in natural language.  Implement the actual addition functionality in the program.  Execute the tests to ensure they pass, verifying that the program behaves as expected according to the described behaviors.  Refactor as needed while ensuring the tests still pass.  This BDD approach emphasizes clear communication and understanding among all stakeholders (developers, testers, and non-technical roles) about what the software is expected to do, ensuring the development aligns closely with user needs and expectations.  Simple Summary  The core idea of TDD, or Test-Driven Development, is to write tests before implementing features. Developers begin by writing test cases for a function or module, and then they write the corresponding code to make those test cases pass. The focus is on testing individual functions or methods. In contrast, BDD, or Behavior-Driven Development, focuses on the behavior of an entire feature or system. In BDD, test cases are closer to user requirements, which helps improve communication and understanding between the development team and non-technical stakeholders.  Practice using SpecFlow  (  Test Context )  Step1: Install SpecFlow   Install SpecFlow in Visual Studio IDE (navigate to Extensions and Updates to install, and remember to restart the IDE afterwards).  Create a new NUnit project.  For the test project, install SpecFlow.NUnit from NuGet (be mindful of the NuGet package name).  Step 2: Add a feature file to the test project, and edit it to include User Stories and Scenarios.  Add a feature file and edit the Feature section to include a User Story.   Feature: Add item to cart\n    In order to avoid the wrong total price\n    As a Customer\n    I want to get the current total price of items in the cart\n   A User Story is a type of scenario description that outlines a user's requirements for an application. It consists of three elements: Role, Feature, and Priority. For example, \"As a user, I want to be able to add products to my shopping cart\"; Priority indicates the importance of each feature, usually represented by a number, with 1 being the highest priority. This User Story template, composed of Role, Feature, and Priority, often follows the format \"As a (role), I want (feature), so that (benefit)\". This format helps teams better define and understand system requirements.   Scenario: Calculate the total price for cart items\n    Given there are cart items\n    When the customer completes the order\n    Then the total price should be the sum of the subtotal of all the cart items plus shipping fee NTD 60\n\nScenario: Alert customer when adding too many items\n    Given there are five items in the cart\n    When the customer tries to add one more item\n    Then the system should alert the customer not to do so and indicate which item cannot be added\n\nScenario: Alert customer when exceeding purchase limit\n    Given the customer has added a quantity of items to the cart\n    When the quantity of items exceeds the purchase limit\n    Then the system should alert the customer and indicate that the item has reached its purchase limit\n\nScenario: Apply free shipping for orders over 500\n    Given the customer selects the cart items\n    When the total price is over 500\n    Then the customer should receive free shipping\n  A Scenario is a descriptive statement used to outline the behavior of a specific feature. It often uses the \"Given-When-Then\" syntax to detail how software should behave under certain conditions. Scenarios serve as readable and understandable requirement specifications, helping developers and non-technical team members reach a consensus. Suppose we have a shopping website; we could write the following Scenario for the \"add a product to the shopping cart\" feature:   Scenario: 用戶將產品添加到購物車\n  Given 用戶已登錄購物網站\n  And 用戶瀏覽一個產品頁面\n  When 用戶點擊“添加到購物車”按鈕\n  Then 產品應該被添加到購物車中\n  And 購物車內的產品數量應該增加\n  Step 3: Generate the Test Code Framework  Right-click on the feature file and select \"Define Step.\" This action will automatically generate the test code framework for you, with names following the descriptions in the BDD Scenario.    Step 4: After completing the example, directly run the test.    Points to Note  Generating code results in anomalies.  When generating code using \"Define\" in a multi-scenario context, it's common to encounter issues where code for some scenarios isn't generated. The most frequent issue is missing code for certain scenarios. When a description fails to generate code successfully, it will turn purple as shown below.    Possible solutions include:   Rewrite the description. It's recommended to use ChatGPT 4.0 for assistance in generating descriptions. This can be especially helpful for those who are not proficient in English or unfamiliar with using SpecFlow, as it can be challenging to describe scenarios in a way that SpecFlow can recognize.  Right-click on the purple-highlighted sentences and choose \"Define.\" There is a \"Copy\" option to manually paste it. This method is the quickest.  Multi Scenario Object Initialization Using SharedContext Approach  If a Feature has multiple Scenarios and you need to initialize objects, you can use the SharedContext approach. The approach is demonstrated with the provided example scenario below.     // Declare the objects to be used with SharedContext.\n   public     class     SharedContext\n   {\n         public     Action     addToCart  ;\n         public     Cart     Cart   {   get  ;   set  ; }\n         public     CardItem     Erasier   {   get  ;   set  ; }\n         public     CardItem     Pencial   {   get  ;   set  ; }\n         public     CardItem     BluePen   {   get  ;   set  ; }\n         public     CardItem     Ruler   {   get  ;   set  ; }\n         public     CardItem     Notebook   {   get  ;   set  ; }\n         public     CardItem     PencilSharpener   {   get  ;   set  ; }\n   }     // Perform the following injection settings in the test object\n   private     readonly     SharedContext     _sharedContext  ;\n     public     AddItemToCartStepDefinitions  (  SharedContext     sharedContext  )\n   {\n       _sharedContext   =   sharedContext;\n   }\n     [  BeforeScenario  ]\n   public     void     Setup  ()\n   {\n       _sharedContext.Cart   =     new     Cart  ();\n       _sharedContext.Erasier   =     new     CardItem  (  name  :   \"Erasiers\"  ,   unitPrice  :   10  ,   maxPurchaseQty  :   10  );\n       _sharedContext.Pencial   =     new     CardItem  (  name  :   \"Pencial\"  ,   unitPrice  :   20  ,   maxPurchaseQty  :   10  );\n       _sharedContext.BluePen   =     new     CardItem  (  name  :   \"BluePen\"  ,   unitPrice  :   30  ,   maxPurchaseQty  :   10  );\n       _sharedContext.Ruler   =     new     CardItem  (  name  :   \"Ruler\"  ,   unitPrice  :   30  ,   maxPurchaseQty  :   10  );\n       _sharedContext.Notebook   =     new     CardItem  (  name  :   \"Notebook\"  ,   unitPrice  :   50  ,   maxPurchaseQty  :   5  );\n       _sharedContext.PencilSharpener   =     new     CardItem  (  name  :   \"PencilSharpener\"  ,   unitPrice  :   50  ,   maxPurchaseQty  :   5  );\n   }  Then, directly use the _sharedContext object in the test code.  .github-light_github-dark{color:#24292e;background:#fff;}.dark .github-light_github-dark{color:#e1e4e8;background:#24292e;}.ct-086898{color:#6A737D;}.ct-149352{color:#D73A49;}.dark .ct-149352{color:#F97583;}.ct-553616{color:#24292E;}.dark .ct-553616{color:#E1E4E8;}.ct-762058{color:#6F42C1;}.dark .ct-762058{color:#B392F0;}.ct-952708{color:#032F62;}.dark .ct-952708{color:#9ECBFF;}.ct-617022{color:#005CC5;}.dark .ct-617022{color:#79B8FF;}"},{"id":"content:100.lesson-learn:Api Key Credential Management.md","path":"/lesson-learn/api-key-credential-management","dir":"lesson-learn","title":"Api Key Credential Management","description":"","keywords":["情境","疏離"],"body":"  Api Key Credential Management  情境  實務上串接時需串別的部門Notify服務，但中間要過Gateway，需要跟Gatway所屬部門申請認證Key，但這Key是人工核發的，稍微思考一下Best Practice是什麼。  疏離  角色轉換思考  我有一個Gateway服務系統，會去串不同部門的服務。其他系統經過我們Gateway服務系統時需要帶我們給的API certifaied Key做認證識別。目前我們這個API certifaied Key是人工核發. 我覺得設計不太好。因為Key期限要到的時候還要人工去處理。  設計合理性  針對情境去做最適應思考設計，所以沒有一個絕對答案。我覺得最方便的做法是OAuth 2.0 的 Client Credentials Grant Flow 認證。但實作OAuth 2.0 設計會牽扯很多工作項目，如果是API Key認證，相對起來設計比較簡單沒錯。  先來考慮Context情境目前看來設計上會有些考量   公司大多會是內網環境，如果只是內網環境。人工替換Key目前問起來也算常見。但對於ITIL 或是ISO 20000在IT治理管理上會是優化項目。  如果系統對外，最好方法當然是照OAuth Flow設計。如果還是想堅持使用API Key，除了防火牆，在金鑰演算法部分應該都是需要被審核設計。   Google 的 API 金鑰管理最佳實踐   限制 API 金鑰的使用範圍：   API 限制：將金鑰限制為只能存取特定的 API，避免未經授權的使用。  應用程式限制 : 限制哪些應用程式或 IP 位址可以使用該金鑰，確保只有授權的系統能夠存取。  定期輪替和撤銷金鑰   定期更新：定期更新 API 金鑰，減少因金鑰洩漏而帶來的風險。  即時撤銷 : 當發現金鑰可能被濫用時，應立即撤銷並重新發佈新的金鑰。  監控和配額管理   監控使用情況 : 監控追蹤 API 金鑰的使用情況，及時發現異常活動。  設定配額限制 : 為 API 設定使用配額，防止濫用並控制成本。  使用更安全的認證方式   OAuth 2.0 : 對於需要更高安全性的應用，建議使用 OAuth 2.0 等更安全的認證方式，提供細緻的存取控制。  透過遵循上述最佳實踐，可以有效地管理 API 金鑰，確保系統的安全性和穩定性。  API Key 自動化更新   輪替機制（Key Rotation）   使用腳本或服務，定期生成新金鑰並將其更新到需要使用的系統。  確保新金鑰啟用前，舊金鑰依然有效一段時間，避免中斷服務。  設置一個自動更新週期（例如每 30 天）。  Endpoint 配合更新   提供一個安全的 API Endpoint，例如 /refresh-api-key。  使用該 Endpoint 提供\n   新的 API Key：返回新的有效金鑰。  舊金鑰的失效時間：明確舊金鑰將何時失效，給客戶端留足緩衝時間。  雙金鑰模式（Dual-Key Approach）   無縫切換 : 允許新舊金鑰同時有效一段時間，確保應用程式有時間切換到新金鑰。  應用檢測 : *客戶端可定期檢查是否需要切換到新金鑰。  結合安全傳輸與存儲   TLS 保護：確保 API Key 傳輸時的安全性。  密文存儲：在伺服器和客戶端，對 API Key 進行加密存儲。  是否實作自動化更新?  推薦理由:   業務規模大，需要頻繁發佈和管理 API Key，人工管理成本高。  技術基礎強，有能力實作並維護一個穩定、安全的金鑰管理系統。  不推理由:   系統簡單，如只有少量用戶，人工管理可能更直觀。  資源不足，無法提供足夠的開發和安全測試資源。  改善計畫設想  短期 : 建立標準化的申請表單和流程，實作基本的金鑰管理系統與建立即時監控機制\n中期 : 開發自助服務平台， 實作自動化的審核流程與建立金鑰輪換機制\n長期 : 建立完整的金鑰生命週期管理，實作分層式金鑰架構與整合公司的身分識別系統  金鑰生命週期管理參考     Creation（創建）: 金鑰的生成階段，系統會依據特定的加密演算法和資安需求來產生新的加密金鑰。  Backup (備份) : 為了資安措施，金鑰匙產生後須立即進行備份，一般會放在硬體安全模組（HSM）中，雲端會放到類似雲端金鑰管理服務（如 AWS KMS、Google Cloud KMS）。  Deployment（佈署）: 這個階段是將金鑰實際部署到運作環境中。金鑰會被安全地散佈給需要使用的系統或使用者。  Monitoring (監控) : 當金鑰開始使用後，需要持續進行監控。系統會追蹤金鑰的使用狀況，檢查是否有異常存取模式或潛在的資安威脅。  Rotation (更換) : 為了維持資安，金鑰需要定期更換。這就像定期更新 SSL 憑證一樣，可以降低金鑰被破解的風險。  Expiration (過期) : 每個金鑰都有其設定的有效期限。當金鑰達到期限時，系統會將其標記為過期。這類似於 SSL 憑證的到期日，提醒管理員進行更新。  Archival (封存) : 過期的金鑰不會立即被刪除，而是先進行封存。這些金鑰可能還需要用來解密歷史資料。就像我們會保存舊系統的存取金鑰，以備日後需要查看歷史資料。  Destruction (銷毀) : 這是金鑰生命週期的最終階段。當確定金鑰真的不再需要時，會以安全的方式將其永久刪除。這個過程必須徹底，確保金鑰無法被還原，就像是要確實清除硬碟中的敏感資料。  金鑰生命週期管理參考  分層式金鑰架構是一種多層次的金鑰管理策略，目的是簡化金鑰管理並增強系統的安全性。該架構通常包含以下層次：    根金鑰（Root Key）"}]